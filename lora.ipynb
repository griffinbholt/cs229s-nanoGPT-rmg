{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cy75saA_KfAt",
        "outputId": "e79d2111-0f48-4ec4-d988-d8467a6eac5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.16.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Collecting memory-profiler\n",
            "  Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
            "Collecting torcheval\n",
            "  Downloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.39.0-py2.py3-none-any.whl (254 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.0/254.0 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: torcheval, smmap, setproctitle, sentry-sdk, pyarrow-hotfix, memory-profiler, docker-pycreds, dill, tiktoken, multiprocess, gitdb, GitPython, wandb, datasets\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.40 datasets-2.15.0 dill-0.3.7 docker-pycreds-0.4.0 gitdb-4.0.11 memory-profiler-0.61.0 multiprocess-0.70.15 pyarrow-hotfix-0.6 sentry-sdk-1.39.0 setproctitle-1.3.3 smmap-5.0.1 tiktoken-0.5.2 torcheval-0.0.7 wandb-0.16.1\n"
          ]
        }
      ],
      "source": [
        "# install necessary libraries\n",
        "!pip install torch numpy transformers datasets tiktoken wandb tqdm memory-profiler torcheval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEhwPEfR8Bf2",
        "outputId": "7b7b525d-6e91-4527-f8d2-ab5d37e94fc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Suzdrt38VI1",
        "outputId": "d3b9b205-5a71-40c4-e8b1-1f9e51a15c3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CS229S/cs229s-nanoGPT-rmg\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/CS229S/cs229s-nanoGPT-rmg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfOd-9uOiVwv",
        "outputId": "378c50eb-6937-4bd9-bb77-df2d2aa6a72d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 301,966 tokens\n",
            "val has 36,059 tokens\n"
          ]
        }
      ],
      "source": [
        "!python data/shakespeare/prepare.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using shakespeare because it's smaller\n",
        "!python train.py --max_iters=100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_IWaXoOfazB",
        "outputId": "f08fb594-b41e-466b-d33a-361a2d61ef60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: max_iters = 100\n",
            "tokens per iteration will be: 327,680\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "config.json: 100% 665/665 [00:00<00:00, 2.91MB/s]\n",
            "model.safetensors: 100% 548M/548M [00:02<00:00, 244MB/s]\n",
            "generation_config.json: 100% 124/124 [00:00<00:00, 754kB/s]\n",
            "num decayed parameter tensors: 50, with 124,318,464 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1774, val loss 4.0018\n",
            "iter 0: loss 4.1306, time 94571.93ms, mfu -100.00%\n",
            "iter 1: loss 4.2609, time 21141.85ms, mfu -100.00%\n",
            "iter 2: loss 4.1471, time 21753.49ms, mfu -100.00%\n",
            "iter 3: loss 3.9653, time 22253.37ms, mfu -100.00%\n",
            "iter 4: loss 4.1734, time 21902.57ms, mfu -100.00%\n",
            "iter 5: loss 4.2267, time 21817.87ms, mfu 4.12%\n",
            "iter 6: loss 4.0306, time 22037.49ms, mfu 4.11%\n",
            "iter 7: loss 3.9921, time 22000.13ms, mfu 4.11%\n",
            "iter 8: loss 3.9475, time 21987.53ms, mfu 4.11%\n",
            "iter 9: loss 3.9570, time 21978.81ms, mfu 4.10%\n",
            "iter 10: loss 3.6055, time 21957.24ms, mfu 4.10%\n",
            "iter 11: loss 3.8801, time 21986.04ms, mfu 4.10%\n",
            "iter 12: loss 3.6320, time 21962.39ms, mfu 4.10%\n",
            "iter 13: loss 3.7186, time 21957.29ms, mfu 4.10%\n",
            "iter 14: loss 3.7274, time 21928.14ms, mfu 4.10%\n",
            "iter 15: loss 3.6854, time 21948.02ms, mfu 4.10%\n",
            "iter 16: loss 3.7546, time 21933.07ms, mfu 4.10%\n",
            "iter 17: loss 3.6858, time 21962.28ms, mfu 4.10%\n",
            "iter 18: loss 3.7122, time 21932.12ms, mfu 4.10%\n",
            "iter 19: loss 3.5126, time 21986.40ms, mfu 4.10%\n",
            "iter 20: loss 3.6867, time 22016.27ms, mfu 4.09%\n",
            "iter 21: loss 3.5992, time 22030.27ms, mfu 4.09%\n",
            "iter 22: loss 3.6530, time 22052.99ms, mfu 4.09%\n",
            "iter 23: loss 3.6867, time 22035.00ms, mfu 4.09%\n",
            "iter 24: loss 3.8541, time 22027.24ms, mfu 4.09%\n",
            "iter 25: loss 3.6194, time 22000.11ms, mfu 4.09%\n",
            "iter 26: loss 3.7743, time 21983.87ms, mfu 4.09%\n",
            "iter 27: loss 3.7342, time 21996.08ms, mfu 4.09%\n",
            "iter 28: loss 3.7138, time 21997.90ms, mfu 4.09%\n",
            "iter 29: loss 3.7474, time 22001.74ms, mfu 4.09%\n",
            "iter 30: loss 3.6069, time 21946.72ms, mfu 4.09%\n",
            "iter 31: loss 3.5507, time 21907.73ms, mfu 4.09%\n",
            "iter 32: loss 3.5012, time 21926.31ms, mfu 4.09%\n",
            "iter 33: loss 3.5474, time 22026.65ms, mfu 4.09%\n",
            "iter 34: loss 3.6106, time 21999.17ms, mfu 4.09%\n",
            "iter 35: loss 3.5383, time 21958.90ms, mfu 4.09%\n",
            "iter 36: loss 3.4712, time 21967.16ms, mfu 4.09%\n",
            "iter 37: loss 3.4860, time 21960.22ms, mfu 4.09%\n",
            "iter 38: loss 3.5861, time 21981.46ms, mfu 4.09%\n",
            "iter 39: loss 3.7463, time 21979.37ms, mfu 4.09%\n",
            "iter 40: loss 3.5277, time 22032.39ms, mfu 4.09%\n",
            "iter 41: loss 3.6138, time 21987.44ms, mfu 4.09%\n",
            "iter 42: loss 3.7291, time 21974.03ms, mfu 4.09%\n",
            "iter 43: loss 3.4478, time 21930.27ms, mfu 4.09%\n",
            "iter 44: loss 3.6309, time 21912.85ms, mfu 4.09%\n",
            "iter 45: loss 3.7021, time 21907.43ms, mfu 4.09%\n",
            "iter 46: loss 3.5934, time 21934.20ms, mfu 4.09%\n",
            "iter 47: loss 3.5611, time 21942.82ms, mfu 4.09%\n",
            "iter 48: loss 3.4428, time 21964.71ms, mfu 4.09%\n",
            "iter 49: loss 3.6798, time 21993.69ms, mfu 4.09%\n",
            "iter 50: loss 3.5113, time 21979.26ms, mfu 4.09%\n",
            "iter 51: loss 3.4153, time 21996.20ms, mfu 4.09%\n",
            "iter 52: loss 3.4904, time 22006.23ms, mfu 4.09%\n",
            "iter 53: loss 3.5792, time 21981.23ms, mfu 4.09%\n",
            "iter 54: loss 3.4675, time 22002.53ms, mfu 4.09%\n",
            "iter 55: loss 3.6086, time 21995.24ms, mfu 4.09%\n",
            "iter 56: loss 3.6578, time 21973.10ms, mfu 4.09%\n",
            "iter 57: loss 3.4494, time 21996.08ms, mfu 4.09%\n",
            "iter 58: loss 3.2466, time 21973.18ms, mfu 4.09%\n",
            "iter 59: loss 3.1701, time 21898.66ms, mfu 4.09%\n",
            "iter 60: loss 3.5266, time 21954.53ms, mfu 4.09%\n",
            "iter 61: loss 3.4446, time 22024.48ms, mfu 4.09%\n",
            "iter 62: loss 3.5374, time 22021.41ms, mfu 4.09%\n",
            "iter 63: loss 3.3333, time 21997.67ms, mfu 4.09%\n",
            "iter 64: loss 3.3982, time 21989.58ms, mfu 4.09%\n",
            "iter 65: loss 3.3968, time 21981.42ms, mfu 4.09%\n",
            "iter 66: loss 3.3311, time 21969.24ms, mfu 4.09%\n",
            "iter 67: loss 3.5746, time 21985.59ms, mfu 4.09%\n",
            "iter 68: loss 3.3458, time 21972.05ms, mfu 4.09%\n",
            "iter 69: loss 3.5871, time 21974.66ms, mfu 4.09%\n",
            "iter 70: loss 3.3701, time 22003.41ms, mfu 4.09%\n",
            "iter 71: loss 3.4004, time 21966.66ms, mfu 4.09%\n",
            "iter 72: loss 3.3332, time 22005.16ms, mfu 4.09%\n",
            "iter 73: loss 3.2173, time 21978.52ms, mfu 4.09%\n",
            "iter 74: loss 3.1033, time 21956.19ms, mfu 4.09%\n",
            "iter 75: loss 3.3028, time 21944.20ms, mfu 4.09%\n",
            "iter 76: loss 3.4651, time 21926.18ms, mfu 4.09%\n",
            "iter 77: loss 3.5628, time 21942.06ms, mfu 4.09%\n",
            "iter 78: loss 3.2695, time 21980.16ms, mfu 4.09%\n",
            "iter 79: loss 3.3959, time 21970.49ms, mfu 4.09%\n",
            "iter 80: loss 3.4515, time 21984.88ms, mfu 4.09%\n",
            "iter 81: loss 3.3108, time 22028.66ms, mfu 4.09%\n",
            "iter 82: loss 3.3338, time 22019.09ms, mfu 4.09%\n",
            "iter 83: loss 3.4112, time 22031.51ms, mfu 4.09%\n",
            "iter 84: loss 3.3526, time 22016.00ms, mfu 4.08%\n",
            "iter 85: loss 3.3314, time 21960.67ms, mfu 4.09%\n",
            "iter 86: loss 3.3031, time 21982.35ms, mfu 4.09%\n",
            "iter 87: loss 3.3812, time 21967.57ms, mfu 4.09%\n",
            "iter 88: loss 3.2374, time 21966.57ms, mfu 4.09%\n",
            "iter 89: loss 3.2840, time 21947.63ms, mfu 4.09%\n",
            "iter 90: loss 3.2278, time 21935.86ms, mfu 4.09%\n",
            "iter 91: loss 3.2621, time 21936.16ms, mfu 4.09%\n",
            "iter 92: loss 3.2420, time 21946.41ms, mfu 4.09%\n",
            "iter 93: loss 3.3044, time 21900.56ms, mfu 4.09%\n",
            "iter 94: loss 3.1131, time 21916.79ms, mfu 4.09%\n",
            "iter 95: loss 3.2483, time 21951.67ms, mfu 4.09%\n",
            "iter 96: loss 3.2305, time 21921.11ms, mfu 4.09%\n",
            "iter 97: loss 3.4015, time 21966.87ms, mfu 4.09%\n",
            "iter 98: loss 2.9929, time 21955.49ms, mfu 4.09%\n",
            "iter 99: loss 3.1819, time 22014.97ms, mfu 4.09%\n",
            "iter 100: loss 3.1048, time 22019.31ms, mfu 4.09%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run training with LoRA for 20 iterations\n",
        "# it's not very good...\n",
        "!python train.py --max_iters=20 --lora_layers=True --dataset=shakespeare"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTFMR3Q02GOv",
        "outputId": "023631ca-7c2f-4d16-a402-a79125821652"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: max_iters = 20\n",
            "Overriding: lora_layers = True\n",
            "Overriding: dataset = shakespeare\n",
            "tokens per iteration will be: 327,680\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "config.json: 100% 665/665 [00:00<00:00, 3.09MB/s]\n",
            "model.safetensors: 100% 548M/548M [00:02<00:00, 223MB/s]\n",
            "generation_config.json: 100% 124/124 [00:00<00:00, 833kB/s]\n",
            "PRINTING MODULES:\n",
            "transformer.wte.weight\n",
            "False\n",
            "transformer.wpe.weight\n",
            "False\n",
            "transformer.h.0.ln_1.weight\n",
            "False\n",
            "transformer.h.0.ln_1.bias\n",
            "False\n",
            "transformer.h.0.attn.c_attn.pretrained.weight\n",
            "False\n",
            "transformer.h.0.attn.c_attn.pretrained.bias\n",
            "False\n",
            "transformer.h.0.attn.c_attn.lora_A.weight\n",
            "True\n",
            "transformer.h.0.attn.c_attn.lora_B.weight\n",
            "True\n",
            "transformer.h.0.attn.c_proj.pretrained.weight\n",
            "False\n",
            "transformer.h.0.attn.c_proj.pretrained.bias\n",
            "False\n",
            "transformer.h.0.attn.c_proj.lora_A.weight\n",
            "True\n",
            "transformer.h.0.attn.c_proj.lora_B.weight\n",
            "True\n",
            "transformer.h.0.ln_2.weight\n",
            "False\n",
            "transformer.h.0.ln_2.bias\n",
            "False\n",
            "transformer.h.0.mlp.c_fc.pretrained.weight\n",
            "False\n",
            "transformer.h.0.mlp.c_fc.pretrained.bias\n",
            "False\n",
            "transformer.h.0.mlp.c_fc.lora_A.weight\n",
            "True\n",
            "transformer.h.0.mlp.c_fc.lora_B.weight\n",
            "True\n",
            "transformer.h.0.mlp.c_proj.pretrained.weight\n",
            "False\n",
            "transformer.h.0.mlp.c_proj.pretrained.bias\n",
            "False\n",
            "transformer.h.0.mlp.c_proj.lora_A.weight\n",
            "True\n",
            "transformer.h.0.mlp.c_proj.lora_B.weight\n",
            "True\n",
            "transformer.h.1.ln_1.weight\n",
            "False\n",
            "transformer.h.1.ln_1.bias\n",
            "False\n",
            "transformer.h.1.attn.c_attn.pretrained.weight\n",
            "False\n",
            "transformer.h.1.attn.c_attn.pretrained.bias\n",
            "False\n",
            "transformer.h.1.attn.c_attn.lora_A.weight\n",
            "True\n",
            "transformer.h.1.attn.c_attn.lora_B.weight\n",
            "True\n",
            "transformer.h.1.attn.c_proj.pretrained.weight\n",
            "False\n",
            "transformer.h.1.attn.c_proj.pretrained.bias\n",
            "False\n",
            "transformer.h.1.attn.c_proj.lora_A.weight\n",
            "True\n",
            "transformer.h.1.attn.c_proj.lora_B.weight\n",
            "True\n",
            "transformer.h.1.ln_2.weight\n",
            "False\n",
            "transformer.h.1.ln_2.bias\n",
            "False\n",
            "transformer.h.1.mlp.c_fc.pretrained.weight\n",
            "False\n",
            "transformer.h.1.mlp.c_fc.pretrained.bias\n",
            "False\n",
            "transformer.h.1.mlp.c_fc.lora_A.weight\n",
            "True\n",
            "transformer.h.1.mlp.c_fc.lora_B.weight\n",
            "True\n",
            "transformer.h.1.mlp.c_proj.pretrained.weight\n",
            "False\n",
            "transformer.h.1.mlp.c_proj.pretrained.bias\n",
            "False\n",
            "transformer.h.1.mlp.c_proj.lora_A.weight\n",
            "True\n",
            "transformer.h.1.mlp.c_proj.lora_B.weight\n",
            "True\n",
            "transformer.h.2.ln_1.weight\n",
            "False\n",
            "transformer.h.2.ln_1.bias\n",
            "False\n",
            "transformer.h.2.attn.c_attn.pretrained.weight\n",
            "False\n",
            "transformer.h.2.attn.c_attn.pretrained.bias\n",
            "False\n",
            "transformer.h.2.attn.c_attn.lora_A.weight\n",
            "True\n",
            "transformer.h.2.attn.c_attn.lora_B.weight\n",
            "True\n",
            "transformer.h.2.attn.c_proj.pretrained.weight\n",
            "False\n",
            "transformer.h.2.attn.c_proj.pretrained.bias\n",
            "False\n",
            "transformer.h.2.attn.c_proj.lora_A.weight\n",
            "True\n",
            "transformer.h.2.attn.c_proj.lora_B.weight\n",
            "True\n",
            "transformer.h.2.ln_2.weight\n",
            "False\n",
            "transformer.h.2.ln_2.bias\n",
            "False\n",
            "transformer.h.2.mlp.c_fc.pretrained.weight\n",
            "False\n",
            "transformer.h.2.mlp.c_fc.pretrained.bias\n",
            "False\n",
            "transformer.h.2.mlp.c_fc.lora_A.weight\n",
            "True\n",
            "transformer.h.2.mlp.c_fc.lora_B.weight\n",
            "True\n",
            "transformer.h.2.mlp.c_proj.pretrained.weight\n",
            "False\n",
            "transformer.h.2.mlp.c_proj.pretrained.bias\n",
            "False\n",
            "transformer.h.2.mlp.c_proj.lora_A.weight\n",
            "True\n",
            "transformer.h.2.mlp.c_proj.lora_B.weight\n",
            "True\n",
            "transformer.h.3.ln_1.weight\n",
            "False\n",
            "transformer.h.3.ln_1.bias\n",
            "False\n",
            "transformer.h.3.attn.c_attn.pretrained.weight\n",
            "False\n",
            "transformer.h.3.attn.c_attn.pretrained.bias\n",
            "False\n",
            "transformer.h.3.attn.c_attn.lora_A.weight\n",
            "True\n",
            "transformer.h.3.attn.c_attn.lora_B.weight\n",
            "True\n",
            "transformer.h.3.attn.c_proj.pretrained.weight\n",
            "False\n",
            "transformer.h.3.attn.c_proj.pretrained.bias\n",
            "False\n",
            "transformer.h.3.attn.c_proj.lora_A.weight\n",
            "True\n",
            "transformer.h.3.attn.c_proj.lora_B.weight\n",
            "True\n",
            "transformer.h.3.ln_2.weight\n",
            "False\n",
            "transformer.h.3.ln_2.bias\n",
            "False\n",
            "transformer.h.3.mlp.c_fc.pretrained.weight\n",
            "False\n",
            "transformer.h.3.mlp.c_fc.pretrained.bias\n",
            "False\n",
            "transformer.h.3.mlp.c_fc.lora_A.weight\n",
            "True\n",
            "transformer.h.3.mlp.c_fc.lora_B.weight\n",
            "True\n",
            "transformer.h.3.mlp.c_proj.pretrained.weight\n",
            "False\n",
            "transformer.h.3.mlp.c_proj.pretrained.bias\n",
            "False\n",
            "transformer.h.3.mlp.c_proj.lora_A.weight\n",
            "True\n",
            "transformer.h.3.mlp.c_proj.lora_B.weight\n",
            "True\n",
            "transformer.h.4.ln_1.weight\n",
            "False\n",
            "transformer.h.4.ln_1.bias\n",
            "False\n",
            "transformer.h.4.attn.c_attn.pretrained.weight\n",
            "False\n",
            "transformer.h.4.attn.c_attn.pretrained.bias\n",
            "False\n",
            "transformer.h.4.attn.c_attn.lora_A.weight\n",
            "True\n",
            "transformer.h.4.attn.c_attn.lora_B.weight\n",
            "True\n",
            "transformer.h.4.attn.c_proj.pretrained.weight\n",
            "False\n",
            "transformer.h.4.attn.c_proj.pretrained.bias\n",
            "False\n",
            "transformer.h.4.attn.c_proj.lora_A.weight\n",
            "True\n",
            "transformer.h.4.attn.c_proj.lora_B.weight\n",
            "True\n",
            "transformer.h.4.ln_2.weight\n",
            "False\n",
            "transformer.h.4.ln_2.bias\n",
            "False\n",
            "transformer.h.4.mlp.c_fc.pretrained.weight\n",
            "False\n",
            "transformer.h.4.mlp.c_fc.pretrained.bias\n",
            "False\n",
            "transformer.h.4.mlp.c_fc.lora_A.weight\n",
            "True\n",
            "transformer.h.4.mlp.c_fc.lora_B.weight\n",
            "True\n",
            "transformer.h.4.mlp.c_proj.pretrained.weight\n",
            "False\n",
            "transformer.h.4.mlp.c_proj.pretrained.bias\n",
            "False\n",
            "transformer.h.4.mlp.c_proj.lora_A.weight\n",
            "True\n",
            "transformer.h.4.mlp.c_proj.lora_B.weight\n",
            "True\n",
            "transformer.h.5.ln_1.weight\n",
            "False\n",
            "transformer.h.5.ln_1.bias\n",
            "False\n",
            "transformer.h.5.attn.c_attn.pretrained.weight\n",
            "False\n",
            "transformer.h.5.attn.c_attn.pretrained.bias\n",
            "False\n",
            "transformer.h.5.attn.c_attn.lora_A.weight\n",
            "True\n",
            "transformer.h.5.attn.c_attn.lora_B.weight\n",
            "True\n",
            "transformer.h.5.attn.c_proj.pretrained.weight\n",
            "False\n",
            "transformer.h.5.attn.c_proj.pretrained.bias\n",
            "False\n",
            "transformer.h.5.attn.c_proj.lora_A.weight\n",
            "True\n",
            "transformer.h.5.attn.c_proj.lora_B.weight\n",
            "True\n",
            "transformer.h.5.ln_2.weight\n",
            "False\n",
            "transformer.h.5.ln_2.bias\n",
            "False\n",
            "transformer.h.5.mlp.c_fc.pretrained.weight\n",
            "False\n",
            "transformer.h.5.mlp.c_fc.pretrained.bias\n",
            "False\n",
            "transformer.h.5.mlp.c_fc.lora_A.weight\n",
            "True\n",
            "transformer.h.5.mlp.c_fc.lora_B.weight\n",
            "True\n",
            "transformer.h.5.mlp.c_proj.pretrained.weight\n",
            "False\n",
            "transformer.h.5.mlp.c_proj.pretrained.bias\n",
            "False\n",
            "transformer.h.5.mlp.c_proj.lora_A.weight\n",
            "True\n",
            "transformer.h.5.mlp.c_proj.lora_B.weight\n",
            "True\n",
            "transformer.h.6.ln_1.weight\n",
            "False\n",
            "transformer.h.6.ln_1.bias\n",
            "False\n",
            "transformer.h.6.attn.c_attn.pretrained.weight\n",
            "False\n",
            "transformer.h.6.attn.c_attn.pretrained.bias\n",
            "False\n",
            "transformer.h.6.attn.c_attn.lora_A.weight\n",
            "True\n",
            "transformer.h.6.attn.c_attn.lora_B.weight\n",
            "True\n",
            "transformer.h.6.attn.c_proj.pretrained.weight\n",
            "False\n",
            "transformer.h.6.attn.c_proj.pretrained.bias\n",
            "False\n",
            "transformer.h.6.attn.c_proj.lora_A.weight\n",
            "True\n",
            "transformer.h.6.attn.c_proj.lora_B.weight\n",
            "True\n",
            "transformer.h.6.ln_2.weight\n",
            "False\n",
            "transformer.h.6.ln_2.bias\n",
            "False\n",
            "transformer.h.6.mlp.c_fc.pretrained.weight\n",
            "False\n",
            "transformer.h.6.mlp.c_fc.pretrained.bias\n",
            "False\n",
            "transformer.h.6.mlp.c_fc.lora_A.weight\n",
            "True\n",
            "transformer.h.6.mlp.c_fc.lora_B.weight\n",
            "True\n",
            "transformer.h.6.mlp.c_proj.pretrained.weight\n",
            "False\n",
            "transformer.h.6.mlp.c_proj.pretrained.bias\n",
            "False\n",
            "transformer.h.6.mlp.c_proj.lora_A.weight\n",
            "True\n",
            "transformer.h.6.mlp.c_proj.lora_B.weight\n",
            "True\n",
            "transformer.h.7.ln_1.weight\n",
            "False\n",
            "transformer.h.7.ln_1.bias\n",
            "False\n",
            "transformer.h.7.attn.c_attn.pretrained.weight\n",
            "False\n",
            "transformer.h.7.attn.c_attn.pretrained.bias\n",
            "False\n",
            "transformer.h.7.attn.c_attn.lora_A.weight\n",
            "True\n",
            "transformer.h.7.attn.c_attn.lora_B.weight\n",
            "True\n",
            "transformer.h.7.attn.c_proj.pretrained.weight\n",
            "False\n",
            "transformer.h.7.attn.c_proj.pretrained.bias\n",
            "False\n",
            "transformer.h.7.attn.c_proj.lora_A.weight\n",
            "True\n",
            "transformer.h.7.attn.c_proj.lora_B.weight\n",
            "True\n",
            "transformer.h.7.ln_2.weight\n",
            "False\n",
            "transformer.h.7.ln_2.bias\n",
            "False\n",
            "transformer.h.7.mlp.c_fc.pretrained.weight\n",
            "False\n",
            "transformer.h.7.mlp.c_fc.pretrained.bias\n",
            "False\n",
            "transformer.h.7.mlp.c_fc.lora_A.weight\n",
            "True\n",
            "transformer.h.7.mlp.c_fc.lora_B.weight\n",
            "True\n",
            "transformer.h.7.mlp.c_proj.pretrained.weight\n",
            "False\n",
            "transformer.h.7.mlp.c_proj.pretrained.bias\n",
            "False\n",
            "transformer.h.7.mlp.c_proj.lora_A.weight\n",
            "True\n",
            "transformer.h.7.mlp.c_proj.lora_B.weight\n",
            "True\n",
            "transformer.h.8.ln_1.weight\n",
            "False\n",
            "transformer.h.8.ln_1.bias\n",
            "False\n",
            "transformer.h.8.attn.c_attn.pretrained.weight\n",
            "False\n",
            "transformer.h.8.attn.c_attn.pretrained.bias\n",
            "False\n",
            "transformer.h.8.attn.c_attn.lora_A.weight\n",
            "True\n",
            "transformer.h.8.attn.c_attn.lora_B.weight\n",
            "True\n",
            "transformer.h.8.attn.c_proj.pretrained.weight\n",
            "False\n",
            "transformer.h.8.attn.c_proj.pretrained.bias\n",
            "False\n",
            "transformer.h.8.attn.c_proj.lora_A.weight\n",
            "True\n",
            "transformer.h.8.attn.c_proj.lora_B.weight\n",
            "True\n",
            "transformer.h.8.ln_2.weight\n",
            "False\n",
            "transformer.h.8.ln_2.bias\n",
            "False\n",
            "transformer.h.8.mlp.c_fc.pretrained.weight\n",
            "False\n",
            "transformer.h.8.mlp.c_fc.pretrained.bias\n",
            "False\n",
            "transformer.h.8.mlp.c_fc.lora_A.weight\n",
            "True\n",
            "transformer.h.8.mlp.c_fc.lora_B.weight\n",
            "True\n",
            "transformer.h.8.mlp.c_proj.pretrained.weight\n",
            "False\n",
            "transformer.h.8.mlp.c_proj.pretrained.bias\n",
            "False\n",
            "transformer.h.8.mlp.c_proj.lora_A.weight\n",
            "True\n",
            "transformer.h.8.mlp.c_proj.lora_B.weight\n",
            "True\n",
            "transformer.h.9.ln_1.weight\n",
            "False\n",
            "transformer.h.9.ln_1.bias\n",
            "False\n",
            "transformer.h.9.attn.c_attn.pretrained.weight\n",
            "False\n",
            "transformer.h.9.attn.c_attn.pretrained.bias\n",
            "False\n",
            "transformer.h.9.attn.c_attn.lora_A.weight\n",
            "True\n",
            "transformer.h.9.attn.c_attn.lora_B.weight\n",
            "True\n",
            "transformer.h.9.attn.c_proj.pretrained.weight\n",
            "False\n",
            "transformer.h.9.attn.c_proj.pretrained.bias\n",
            "False\n",
            "transformer.h.9.attn.c_proj.lora_A.weight\n",
            "True\n",
            "transformer.h.9.attn.c_proj.lora_B.weight\n",
            "True\n",
            "transformer.h.9.ln_2.weight\n",
            "False\n",
            "transformer.h.9.ln_2.bias\n",
            "False\n",
            "transformer.h.9.mlp.c_fc.pretrained.weight\n",
            "False\n",
            "transformer.h.9.mlp.c_fc.pretrained.bias\n",
            "False\n",
            "transformer.h.9.mlp.c_fc.lora_A.weight\n",
            "True\n",
            "transformer.h.9.mlp.c_fc.lora_B.weight\n",
            "True\n",
            "transformer.h.9.mlp.c_proj.pretrained.weight\n",
            "False\n",
            "transformer.h.9.mlp.c_proj.pretrained.bias\n",
            "False\n",
            "transformer.h.9.mlp.c_proj.lora_A.weight\n",
            "True\n",
            "transformer.h.9.mlp.c_proj.lora_B.weight\n",
            "True\n",
            "transformer.h.10.ln_1.weight\n",
            "False\n",
            "transformer.h.10.ln_1.bias\n",
            "False\n",
            "transformer.h.10.attn.c_attn.pretrained.weight\n",
            "False\n",
            "transformer.h.10.attn.c_attn.pretrained.bias\n",
            "False\n",
            "transformer.h.10.attn.c_attn.lora_A.weight\n",
            "True\n",
            "transformer.h.10.attn.c_attn.lora_B.weight\n",
            "True\n",
            "transformer.h.10.attn.c_proj.pretrained.weight\n",
            "False\n",
            "transformer.h.10.attn.c_proj.pretrained.bias\n",
            "False\n",
            "transformer.h.10.attn.c_proj.lora_A.weight\n",
            "True\n",
            "transformer.h.10.attn.c_proj.lora_B.weight\n",
            "True\n",
            "transformer.h.10.ln_2.weight\n",
            "False\n",
            "transformer.h.10.ln_2.bias\n",
            "False\n",
            "transformer.h.10.mlp.c_fc.pretrained.weight\n",
            "False\n",
            "transformer.h.10.mlp.c_fc.pretrained.bias\n",
            "False\n",
            "transformer.h.10.mlp.c_fc.lora_A.weight\n",
            "True\n",
            "transformer.h.10.mlp.c_fc.lora_B.weight\n",
            "True\n",
            "transformer.h.10.mlp.c_proj.pretrained.weight\n",
            "False\n",
            "transformer.h.10.mlp.c_proj.pretrained.bias\n",
            "False\n",
            "transformer.h.10.mlp.c_proj.lora_A.weight\n",
            "True\n",
            "transformer.h.10.mlp.c_proj.lora_B.weight\n",
            "True\n",
            "transformer.h.11.ln_1.weight\n",
            "False\n",
            "transformer.h.11.ln_1.bias\n",
            "False\n",
            "transformer.h.11.attn.c_attn.pretrained.weight\n",
            "False\n",
            "transformer.h.11.attn.c_attn.pretrained.bias\n",
            "False\n",
            "transformer.h.11.attn.c_attn.lora_A.weight\n",
            "True\n",
            "transformer.h.11.attn.c_attn.lora_B.weight\n",
            "True\n",
            "transformer.h.11.attn.c_proj.pretrained.weight\n",
            "False\n",
            "transformer.h.11.attn.c_proj.pretrained.bias\n",
            "False\n",
            "transformer.h.11.attn.c_proj.lora_A.weight\n",
            "True\n",
            "transformer.h.11.attn.c_proj.lora_B.weight\n",
            "True\n",
            "transformer.h.11.ln_2.weight\n",
            "False\n",
            "transformer.h.11.ln_2.bias\n",
            "False\n",
            "transformer.h.11.mlp.c_fc.pretrained.weight\n",
            "False\n",
            "transformer.h.11.mlp.c_fc.pretrained.bias\n",
            "False\n",
            "transformer.h.11.mlp.c_fc.lora_A.weight\n",
            "True\n",
            "transformer.h.11.mlp.c_fc.lora_B.weight\n",
            "True\n",
            "transformer.h.11.mlp.c_proj.pretrained.weight\n",
            "False\n",
            "transformer.h.11.mlp.c_proj.pretrained.bias\n",
            "False\n",
            "transformer.h.11.mlp.c_proj.lora_A.weight\n",
            "True\n",
            "transformer.h.11.mlp.c_proj.lora_B.weight\n",
            "True\n",
            "transformer.ln_f.weight\n",
            "False\n",
            "transformer.ln_f.bias\n",
            "False\n",
            "lm_head.pretrained.weight\n",
            "False\n",
            "lm_head.pretrained.bias\n",
            "False\n",
            "lm_head.lora_A.weight\n",
            "True\n",
            "lm_head.lora_B.weight\n",
            "True\n",
            "MAX MEM: 660789760\n",
            "num decayed parameter tensors: 98, with 1,587,848 parameters\n",
            "num non-decayed parameter tensors: 0, with 0 parameters\n",
            "using fused AdamW: True\n",
            "100% 200/200 [00:49<00:00,  4.05it/s]\n",
            "100% 200/200 [00:48<00:00,  4.16it/s]\n",
            "step 0: train loss 4.1772, val loss 4.0122\n",
            "iter 0: loss 4.1690, time 117813.24ms, mfu -100.00%\n",
            "iter 1: loss 3.9864, time 20540.96ms, mfu -100.00%\n",
            "iter 2: loss 4.3375, time 21068.86ms, mfu -100.00%\n",
            "iter 3: loss 4.0211, time 21000.60ms, mfu -100.00%\n",
            "iter 4: loss 4.2177, time 20918.39ms, mfu -100.00%\n",
            "iter 6: loss 4.3052, time 21071.02ms, mfu 5.48%\n",
            "iter 7: loss 3.8496, time 21014.18ms, mfu 5.48%\n",
            "iter 8: loss 4.1928, time 20995.17ms, mfu 5.48%\n",
            "iter 9: loss 4.1674, time 21039.49ms, mfu 5.48%\n",
            "iter 10: loss 4.0537, time 21073.48ms, mfu 5.48%\n",
            "iter 11: loss 4.2219, time 21100.88ms, mfu 5.48%\n",
            "iter 12: loss 4.2692, time 21114.62ms, mfu 5.48%\n",
            "iter 13: loss 4.1759, time 21118.32ms, mfu 5.47%\n",
            "iter 14: loss 4.2484, time 21090.67ms, mfu 5.47%\n",
            "iter 15: loss 4.2789, time 21096.95ms, mfu 5.47%\n",
            "iter 16: loss 3.9864, time 21097.51ms, mfu 5.47%\n",
            "iter 17: loss 4.2033, time 21092.22ms, mfu 5.47%\n",
            "iter 18: loss 4.2695, time 21110.52ms, mfu 5.47%\n",
            "iter 19: loss 4.2266, time 21066.19ms, mfu 5.47%\n",
            "iter 20: loss 4.2045, time 21051.43ms, mfu 5.47%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary: LoRA training is a little faster (1 sec/iteration) but it doesn't converge with these hyperparameters."
      ],
      "metadata": {
        "id": "qR45ycsr5IxG"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}