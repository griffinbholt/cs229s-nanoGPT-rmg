{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cy75saA_KfAt",
        "outputId": "546d0b43-f770-4d5f-ffa9-dd88a32696bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.5.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: memory-profiler in /usr/local/lib/python3.10/dist-packages (0.61.0)\n",
            "Requirement already satisfied: torcheval in /usr/local/lib/python3.10/dist-packages (0.0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.40)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.37.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch numpy transformers datasets tiktoken wandb tqdm memory-profiler torcheval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEhwPEfR8Bf2",
        "outputId": "ad792406-3bf5-4df2-c89b-84e0c0caa61e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Suzdrt38VI1",
        "outputId": "252f3f7f-a354-4b32-b5b8-600afa0c7add"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CS229S/cs229s-nanoGPT-rmg\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/CS229S/cs229s-nanoGPT-rmg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfOd-9uOiVwv",
        "outputId": "e62f5446-fbab-46a1-f5a1-f25df9e34bae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 301,966 tokens\n",
            "val has 36,059 tokens\n"
          ]
        }
      ],
      "source": [
        "!python data/shakespeare/prepare.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JQ4B-JnO7zn",
        "outputId": "fa30a6a6-fa68-4ef4-d4b2-548267c94938"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: init_from = gpt2\n",
            "Overriding: start = What is the answer to life, the universe, and everything?\n",
            "Overriding: num_samples = 5\n",
            "Overriding: max_new_tokens = 100\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "PERPLEXITY:  tensor(8244.5469, device='cuda:0')\n",
            "Filename: /content/drive/MyDrive/CS229S/cs229s-nanoGPT-rmg/model.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "   420   2198.7 MiB   2198.7 MiB           1       @torch.no_grad()\n",
            "   421                                             @profile\n",
            "   422                                             def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
            "   423                                                 \"\"\"\n",
            "   424                                                 Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
            "   425                                                 the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
            "   426                                                 Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
            "   427                                                 \"\"\"\n",
            "   428   2731.3 MiB      0.0 MiB         101           for _ in range(max_new_tokens):\n",
            "   429                                                     # if the sequence context is growing too long we must crop it at block_size\n",
            "   430   2731.3 MiB      0.0 MiB         100               idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
            "   431                                                     # forward the model to get the logits for the index in the sequence\n",
            "   432   2731.3 MiB    529.4 MiB         100               logits, _ = self(idx_cond)\n",
            "   433                                                     # pluck the logits at the final step and scale by desired temperature\n",
            "   434   2731.3 MiB      0.0 MiB         100               logits = logits[:, -1, :] / temperature\n",
            "   435                                                     # optionally crop the logits to only the top k options\n",
            "   436   2731.3 MiB      0.0 MiB         100               if top_k is not None:\n",
            "   437   2731.3 MiB      3.3 MiB         100                   v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
            "   438   2731.3 MiB      0.0 MiB         100                   logits[logits < v[:, [-1]]] = -float('Inf')\n",
            "   439                                                     # apply softmax to convert logits to (normalized) probabilities\n",
            "   440   2731.3 MiB      0.0 MiB         100               probs = F.softmax(logits, dim=-1)\n",
            "   441                                                     # sample from the distribution\n",
            "   442   2731.3 MiB      0.0 MiB         100               idx_next = torch.multinomial(probs, num_samples=1)\n",
            "   443                                                     # append sampled index to the running sequence and continue\n",
            "   444   2731.3 MiB      0.0 MiB         100               idx = torch.cat((idx, idx_next), dim=1)\n",
            "   445                                         \n",
            "   446   2734.3 MiB      3.0 MiB           1           ppl = self.calculate_perplexity(idx)\n",
            "   447   2734.3 MiB      0.0 MiB           1           print(\"PERPLEXITY: \", ppl)\n",
            "   448   2734.3 MiB      0.0 MiB           1           return idx\n",
            "\n",
            "\n",
            "What is the answer to life, the universe, and everything?\n",
            "\n",
            "One of the most important questions we need to answer is:\n",
            "\n",
            "What is the answer to life?\n",
            "\n",
            "The answer to life is an answer to life itself, an answer to life itself.\n",
            "\n",
            "It is the answer to life itself, a question that requires us to have a kind of identity.\n",
            "\n",
            "It is the answer to life itself, an answer to life itself. It is the answer to life itself, a question that demands us to experience it on our own,\n",
            "---------------\n",
            "PERPLEXITY:  tensor(11225.3896, device='cuda:0')\n",
            "Filename: /content/drive/MyDrive/CS229S/cs229s-nanoGPT-rmg/model.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "   420   2734.3 MiB   2734.3 MiB           1       @torch.no_grad()\n",
            "   421                                             @profile\n",
            "   422                                             def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
            "   423                                                 \"\"\"\n",
            "   424                                                 Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
            "   425                                                 the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
            "   426                                                 Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
            "   427                                                 \"\"\"\n",
            "   428   2734.3 MiB      0.0 MiB         101           for _ in range(max_new_tokens):\n",
            "   429                                                     # if the sequence context is growing too long we must crop it at block_size\n",
            "   430   2734.3 MiB      0.0 MiB         100               idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
            "   431                                                     # forward the model to get the logits for the index in the sequence\n",
            "   432   2734.3 MiB      0.0 MiB         100               logits, _ = self(idx_cond)\n",
            "   433                                                     # pluck the logits at the final step and scale by desired temperature\n",
            "   434   2734.3 MiB      0.0 MiB         100               logits = logits[:, -1, :] / temperature\n",
            "   435                                                     # optionally crop the logits to only the top k options\n",
            "   436   2734.3 MiB      0.0 MiB         100               if top_k is not None:\n",
            "   437   2734.3 MiB      0.0 MiB         100                   v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
            "   438   2734.3 MiB      0.0 MiB         100                   logits[logits < v[:, [-1]]] = -float('Inf')\n",
            "   439                                                     # apply softmax to convert logits to (normalized) probabilities\n",
            "   440   2734.3 MiB      0.0 MiB         100               probs = F.softmax(logits, dim=-1)\n",
            "   441                                                     # sample from the distribution\n",
            "   442   2734.3 MiB      0.0 MiB         100               idx_next = torch.multinomial(probs, num_samples=1)\n",
            "   443                                                     # append sampled index to the running sequence and continue\n",
            "   444   2734.3 MiB      0.0 MiB         100               idx = torch.cat((idx, idx_next), dim=1)\n",
            "   445                                         \n",
            "   446   2734.3 MiB      0.0 MiB           1           ppl = self.calculate_perplexity(idx)\n",
            "   447   2734.3 MiB      0.0 MiB           1           print(\"PERPLEXITY: \", ppl)\n",
            "   448   2734.3 MiB      0.0 MiB           1           return idx\n",
            "\n",
            "\n",
            "What is the answer to life, the universe, and everything? Probably the answer is none of these things. When you think of everything, always think of everything in a way that's probably not what you know.\n",
            "\n",
            "The answer to life is life itself. A life is the life that you're living on, the life that you're running on, the life that you're running outta. Imagine this life. This is, unfortunately, a life worth death.\n",
            "\n",
            "This life is, unfortunately, a life worth death.\n",
            "\n",
            "The answer to\n",
            "---------------\n",
            "PERPLEXITY:  tensor(16898.7832, device='cuda:0')\n",
            "Filename: /content/drive/MyDrive/CS229S/cs229s-nanoGPT-rmg/model.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "   420   2734.3 MiB   2734.3 MiB           1       @torch.no_grad()\n",
            "   421                                             @profile\n",
            "   422                                             def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
            "   423                                                 \"\"\"\n",
            "   424                                                 Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
            "   425                                                 the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
            "   426                                                 Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
            "   427                                                 \"\"\"\n",
            "   428   2734.3 MiB      0.0 MiB         101           for _ in range(max_new_tokens):\n",
            "   429                                                     # if the sequence context is growing too long we must crop it at block_size\n",
            "   430   2734.3 MiB      0.0 MiB         100               idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
            "   431                                                     # forward the model to get the logits for the index in the sequence\n",
            "   432   2734.3 MiB      0.0 MiB         100               logits, _ = self(idx_cond)\n",
            "   433                                                     # pluck the logits at the final step and scale by desired temperature\n",
            "   434   2734.3 MiB      0.0 MiB         100               logits = logits[:, -1, :] / temperature\n",
            "   435                                                     # optionally crop the logits to only the top k options\n",
            "   436   2734.3 MiB      0.0 MiB         100               if top_k is not None:\n",
            "   437   2734.3 MiB      0.0 MiB         100                   v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
            "   438   2734.3 MiB      0.0 MiB         100                   logits[logits < v[:, [-1]]] = -float('Inf')\n",
            "   439                                                     # apply softmax to convert logits to (normalized) probabilities\n",
            "   440   2734.3 MiB      0.0 MiB         100               probs = F.softmax(logits, dim=-1)\n",
            "   441                                                     # sample from the distribution\n",
            "   442   2734.3 MiB      0.0 MiB         100               idx_next = torch.multinomial(probs, num_samples=1)\n",
            "   443                                                     # append sampled index to the running sequence and continue\n",
            "   444   2734.3 MiB      0.0 MiB         100               idx = torch.cat((idx, idx_next), dim=1)\n",
            "   445                                         \n",
            "   446   2734.3 MiB      0.0 MiB           1           ppl = self.calculate_perplexity(idx)\n",
            "   447   2734.3 MiB      0.0 MiB           1           print(\"PERPLEXITY: \", ppl)\n",
            "   448   2734.3 MiB      0.0 MiB           1           return idx\n",
            "\n",
            "\n",
            "What is the answer to life, the universe, and everything?\n",
            "\n",
            "\n",
            "Can's you answer that question before us?\n",
            "\n",
            "\n",
            "You could.\n",
            "\n",
            "\n",
            "Some answers might be easy, but the few that really matter are those that may not directly inform your decision.\n",
            "\n",
            "\n",
            "If you're a survivor of a time when it was the right thing to do, you don't want to talk about it. If you know what you're gonna do, you're more likely than not to find a solution.\n",
            "\n",
            "\n",
            "And that's why a lot of people of faith\n",
            "---------------\n",
            "PERPLEXITY:  tensor(8805.1309, device='cuda:0')\n",
            "Filename: /content/drive/MyDrive/CS229S/cs229s-nanoGPT-rmg/model.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "   420   2734.3 MiB   2734.3 MiB           1       @torch.no_grad()\n",
            "   421                                             @profile\n",
            "   422                                             def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
            "   423                                                 \"\"\"\n",
            "   424                                                 Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
            "   425                                                 the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
            "   426                                                 Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
            "   427                                                 \"\"\"\n",
            "   428   2734.3 MiB      0.0 MiB         101           for _ in range(max_new_tokens):\n",
            "   429                                                     # if the sequence context is growing too long we must crop it at block_size\n",
            "   430   2734.3 MiB      0.0 MiB         100               idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
            "   431                                                     # forward the model to get the logits for the index in the sequence\n",
            "   432   2734.3 MiB      0.0 MiB         100               logits, _ = self(idx_cond)\n",
            "   433                                                     # pluck the logits at the final step and scale by desired temperature\n",
            "   434   2734.3 MiB      0.0 MiB         100               logits = logits[:, -1, :] / temperature\n",
            "   435                                                     # optionally crop the logits to only the top k options\n",
            "   436   2734.3 MiB      0.0 MiB         100               if top_k is not None:\n",
            "   437   2734.3 MiB      0.0 MiB         100                   v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
            "   438   2734.3 MiB      0.0 MiB         100                   logits[logits < v[:, [-1]]] = -float('Inf')\n",
            "   439                                                     # apply softmax to convert logits to (normalized) probabilities\n",
            "   440   2734.3 MiB      0.0 MiB         100               probs = F.softmax(logits, dim=-1)\n",
            "   441                                                     # sample from the distribution\n",
            "   442   2734.3 MiB      0.0 MiB         100               idx_next = torch.multinomial(probs, num_samples=1)\n",
            "   443                                                     # append sampled index to the running sequence and continue\n",
            "   444   2734.3 MiB      0.0 MiB         100               idx = torch.cat((idx, idx_next), dim=1)\n",
            "   445                                         \n",
            "   446   2734.3 MiB      0.0 MiB           1           ppl = self.calculate_perplexity(idx)\n",
            "   447   2734.3 MiB      0.0 MiB           1           print(\"PERPLEXITY: \", ppl)\n",
            "   448   2734.3 MiB      0.0 MiB           1           return idx\n",
            "\n",
            "\n",
            "What is the answer to life, the universe, and everything? Just like the question \"Why?\" is answered and answered more often than you could ever ask for a chance at understanding what the answer to life is.\n",
            "\n",
            "But yes, here is the answer.\n",
            "\n",
            "Is life itself the answer?\n",
            "\n",
            "\n",
            "It is the answer to life, the answer to life.\n",
            "\n",
            "Life is what it is.\n",
            "\n",
            "\n",
            "It is the answer.\n",
            "\n",
            "\n",
            "It is, after all, the answer to life. Because we are living it.\n",
            "\n",
            "Life is\n",
            "---------------\n",
            "PERPLEXITY:  tensor(11275.0332, device='cuda:0')\n",
            "Filename: /content/drive/MyDrive/CS229S/cs229s-nanoGPT-rmg/model.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "   420   2734.3 MiB   2734.3 MiB           1       @torch.no_grad()\n",
            "   421                                             @profile\n",
            "   422                                             def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
            "   423                                                 \"\"\"\n",
            "   424                                                 Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
            "   425                                                 the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
            "   426                                                 Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
            "   427                                                 \"\"\"\n",
            "   428   2734.3 MiB      0.0 MiB         101           for _ in range(max_new_tokens):\n",
            "   429                                                     # if the sequence context is growing too long we must crop it at block_size\n",
            "   430   2734.3 MiB      0.0 MiB         100               idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
            "   431                                                     # forward the model to get the logits for the index in the sequence\n",
            "   432   2734.3 MiB      0.0 MiB         100               logits, _ = self(idx_cond)\n",
            "   433                                                     # pluck the logits at the final step and scale by desired temperature\n",
            "   434   2734.3 MiB      0.0 MiB         100               logits = logits[:, -1, :] / temperature\n",
            "   435                                                     # optionally crop the logits to only the top k options\n",
            "   436   2734.3 MiB      0.0 MiB         100               if top_k is not None:\n",
            "   437   2734.3 MiB      0.0 MiB         100                   v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
            "   438   2734.3 MiB      0.0 MiB         100                   logits[logits < v[:, [-1]]] = -float('Inf')\n",
            "   439                                                     # apply softmax to convert logits to (normalized) probabilities\n",
            "   440   2734.3 MiB      0.0 MiB         100               probs = F.softmax(logits, dim=-1)\n",
            "   441                                                     # sample from the distribution\n",
            "   442   2734.3 MiB      0.0 MiB         100               idx_next = torch.multinomial(probs, num_samples=1)\n",
            "   443                                                     # append sampled index to the running sequence and continue\n",
            "   444   2734.3 MiB      0.0 MiB         100               idx = torch.cat((idx, idx_next), dim=1)\n",
            "   445                                         \n",
            "   446   2734.3 MiB      0.0 MiB           1           ppl = self.calculate_perplexity(idx)\n",
            "   447   2734.3 MiB      0.0 MiB           1           print(\"PERPLEXITY: \", ppl)\n",
            "   448   2734.3 MiB      0.0 MiB           1           return idx\n",
            "\n",
            "\n",
            "What is the answer to life, the universe, and everything? This question has been explored in many of the ancient cultures of ancient Greece, Peru, and India. Some ancient texts, such as the Book of the Dead, in which the goddess has played a major role, are still under investigation.\n",
            "\n",
            "A look at the history of religion\n",
            "\n",
            "People who worshipped gods in ancient times have known about them; they have often told stories about them. They have also written histories of the gods.\n",
            "\n",
            "It is strange, indeed, that people who worship them\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "!python sample.py \\\n",
        "    --init_from=gpt2 \\\n",
        "    --start=\"What is the answer to life, the universe, and everything?\" \\\n",
        "    --num_samples=5 --max_new_tokens=100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash eval_quantized.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAzfCNSnUPcA",
        "outputId": "5dc2bdfe-860b-45f1-8275-7bfc4c5a42e4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: init_from = gpt2\n",
            "Overriding: start = To be or not to be, that is the question.\n",
            "Overriding: max_new_tokens = 128\n",
            "Overriding: num_samples = 3\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "PERPLEXITY:  tensor(9440.5195, device='cuda:0')\n",
            "Filename: /content/drive/MyDrive/CS229S/cs229s-nanoGPT-rmg/model.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "   420   2200.8 MiB   2200.8 MiB           1       @torch.no_grad()\n",
            "   421                                             @profile\n",
            "   422                                             def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
            "   423                                                 \"\"\"\n",
            "   424                                                 Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
            "   425                                                 the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
            "   426                                                 Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
            "   427                                                 \"\"\"\n",
            "   428   2739.0 MiB      0.0 MiB         129           for _ in range(max_new_tokens):\n",
            "   429                                                     # if the sequence context is growing too long we must crop it at block_size\n",
            "   430   2739.0 MiB      0.0 MiB         128               idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
            "   431                                                     # forward the model to get the logits for the index in the sequence\n",
            "   432   2739.0 MiB    534.9 MiB         128               logits, _ = self(idx_cond)\n",
            "   433                                                     # pluck the logits at the final step and scale by desired temperature\n",
            "   434   2739.0 MiB      0.0 MiB         128               logits = logits[:, -1, :] / temperature\n",
            "   435                                                     # optionally crop the logits to only the top k options\n",
            "   436   2739.0 MiB      0.0 MiB         128               if top_k is not None:\n",
            "   437   2739.0 MiB      0.0 MiB         128                   v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
            "   438   2739.0 MiB      0.0 MiB         128                   logits[logits < v[:, [-1]]] = -float('Inf')\n",
            "   439                                                     # apply softmax to convert logits to (normalized) probabilities\n",
            "   440   2739.0 MiB      0.0 MiB         128               probs = F.softmax(logits, dim=-1)\n",
            "   441                                                     # sample from the distribution\n",
            "   442   2739.0 MiB      0.0 MiB         128               idx_next = torch.multinomial(probs, num_samples=1)\n",
            "   443                                                     # append sampled index to the running sequence and continue\n",
            "   444   2739.0 MiB      3.2 MiB         128               idx = torch.cat((idx, idx_next), dim=1)\n",
            "   445                                         \n",
            "   446   2739.0 MiB      0.0 MiB           1           ppl = self.calculate_perplexity(idx)\n",
            "   447   2739.0 MiB      0.0 MiB           1           print(\"PERPLEXITY: \", ppl)\n",
            "   448   2739.0 MiB      0.0 MiB           1           return idx\n",
            "\n",
            "\n",
            "To be or not to be, that is the question. Have any of the four parties, in terms of their ability to do that, and if so, should the Speaker of the House decide that they should be disbarred?\"\n",
            "\n",
            "Mr. Speaker, this question was brought out of the House and then he admitted that it was a question that he would answer at a certain moment in the future.\n",
            "\n",
            "Mr. Speaker, I'll say this again. The House, I hope you are able to present this up in a right order. Many of the views in your budget and I know there's a lot of people who think that. But I'm happy to sit this one out\n",
            "---------------\n",
            "PERPLEXITY:  tensor(12118.5605, device='cuda:0')\n",
            "Filename: /content/drive/MyDrive/CS229S/cs229s-nanoGPT-rmg/model.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "   420   2739.0 MiB   2739.0 MiB           1       @torch.no_grad()\n",
            "   421                                             @profile\n",
            "   422                                             def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
            "   423                                                 \"\"\"\n",
            "   424                                                 Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
            "   425                                                 the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
            "   426                                                 Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
            "   427                                                 \"\"\"\n",
            "   428   2739.0 MiB      0.0 MiB         129           for _ in range(max_new_tokens):\n",
            "   429                                                     # if the sequence context is growing too long we must crop it at block_size\n",
            "   430   2739.0 MiB      0.0 MiB         128               idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
            "   431                                                     # forward the model to get the logits for the index in the sequence\n",
            "   432   2739.0 MiB      0.0 MiB         128               logits, _ = self(idx_cond)\n",
            "   433                                                     # pluck the logits at the final step and scale by desired temperature\n",
            "   434   2739.0 MiB      0.0 MiB         128               logits = logits[:, -1, :] / temperature\n",
            "   435                                                     # optionally crop the logits to only the top k options\n",
            "   436   2739.0 MiB      0.0 MiB         128               if top_k is not None:\n",
            "   437   2739.0 MiB      0.0 MiB         128                   v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
            "   438   2739.0 MiB      0.0 MiB         128                   logits[logits < v[:, [-1]]] = -float('Inf')\n",
            "   439                                                     # apply softmax to convert logits to (normalized) probabilities\n",
            "   440   2739.0 MiB      0.0 MiB         128               probs = F.softmax(logits, dim=-1)\n",
            "   441                                                     # sample from the distribution\n",
            "   442   2739.0 MiB      0.0 MiB         128               idx_next = torch.multinomial(probs, num_samples=1)\n",
            "   443                                                     # append sampled index to the running sequence and continue\n",
            "   444   2739.0 MiB      0.0 MiB         128               idx = torch.cat((idx, idx_next), dim=1)\n",
            "   445                                         \n",
            "   446   2739.0 MiB      0.0 MiB           1           ppl = self.calculate_perplexity(idx)\n",
            "   447   2739.0 MiB      0.0 MiB           1           print(\"PERPLEXITY: \", ppl)\n",
            "   448   2739.0 MiB      0.0 MiB           1           return idx\n",
            "\n",
            "\n",
            "To be or not to be, that is the question.\n",
            "\n",
            "The new law is supposed to help us all face the problem of the people who control our rights as citizens. It's a system that's rigged to allow the elite to take control of the people.\n",
            "\n",
            "Today, it's up to us to get rid of this system and we will. With the help of our allies and elected officials, we can stop it's insidious, cruel, cruel and cruel.\n",
            "\n",
            "We need to stop this system so as to save our democracy.<|endoftext|>President Obama for one, even as the Democrats refused to accept the number-one candidate in their race for the White House, the party's\n",
            "---------------\n",
            "PERPLEXITY:  tensor(7294.9287, device='cuda:0')\n",
            "Filename: /content/drive/MyDrive/CS229S/cs229s-nanoGPT-rmg/model.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "   420   2739.0 MiB   2739.0 MiB           1       @torch.no_grad()\n",
            "   421                                             @profile\n",
            "   422                                             def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
            "   423                                                 \"\"\"\n",
            "   424                                                 Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
            "   425                                                 the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
            "   426                                                 Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
            "   427                                                 \"\"\"\n",
            "   428   2739.0 MiB      0.0 MiB         129           for _ in range(max_new_tokens):\n",
            "   429                                                     # if the sequence context is growing too long we must crop it at block_size\n",
            "   430   2739.0 MiB      0.0 MiB         128               idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
            "   431                                                     # forward the model to get the logits for the index in the sequence\n",
            "   432   2739.0 MiB      0.0 MiB         128               logits, _ = self(idx_cond)\n",
            "   433                                                     # pluck the logits at the final step and scale by desired temperature\n",
            "   434   2739.0 MiB      0.0 MiB         128               logits = logits[:, -1, :] / temperature\n",
            "   435                                                     # optionally crop the logits to only the top k options\n",
            "   436   2739.0 MiB      0.0 MiB         128               if top_k is not None:\n",
            "   437   2739.0 MiB      0.0 MiB         128                   v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
            "   438   2739.0 MiB      0.0 MiB         128                   logits[logits < v[:, [-1]]] = -float('Inf')\n",
            "   439                                                     # apply softmax to convert logits to (normalized) probabilities\n",
            "   440   2739.0 MiB      0.0 MiB         128               probs = F.softmax(logits, dim=-1)\n",
            "   441                                                     # sample from the distribution\n",
            "   442   2739.0 MiB      0.0 MiB         128               idx_next = torch.multinomial(probs, num_samples=1)\n",
            "   443                                                     # append sampled index to the running sequence and continue\n",
            "   444   2739.0 MiB      0.0 MiB         128               idx = torch.cat((idx, idx_next), dim=1)\n",
            "   445                                         \n",
            "   446   2739.0 MiB      0.0 MiB           1           ppl = self.calculate_perplexity(idx)\n",
            "   447   2739.0 MiB      0.0 MiB           1           print(\"PERPLEXITY: \", ppl)\n",
            "   448   2739.0 MiB      0.0 MiB           1           return idx\n",
            "\n",
            "\n",
            "To be or not to be, that is the question.\n",
            "\n",
            "How did we know that?\n",
            "\n",
            "That is the question, because the science of the earth's climate hasn't even been scientifically validated, though the scientific literature has been validated through a long process of direct measurement of the Earth's rainfall levels, which means that there is no solid proof that the Earth is warming.\n",
            "\n",
            "We tried to prove that by using a new model. We made it very simple.\n",
            "\n",
            "We literally tested that model.\n",
            "\n",
            "\n",
            "So, the answer is in the case where water is the variable but that is what we were told when we invented the model.\n",
            "\n",
            "It is, in order to\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Manually went in and set QUANTIZE = False\n",
        "!bash eval_quantized.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5IBJIWufTwb",
        "outputId": "ac1fbd70-ccf7-4ea9-d70e-cb6894bc1370"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: init_from = gpt2\n",
            "Overriding: start = To be or not to be, that is the question.\n",
            "Overriding: max_new_tokens = 128\n",
            "Overriding: num_samples = 3\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "PERPLEXITY:  tensor(9771.9482, device='cuda:0')\n",
            "Filename: /content/drive/MyDrive/CS229S/cs229s-nanoGPT-rmg/model.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "   420   2141.6 MiB   2141.6 MiB           1       @torch.no_grad()\n",
            "   421                                             @profile\n",
            "   422                                             def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
            "   423                                                 \"\"\"\n",
            "   424                                                 Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
            "   425                                                 the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
            "   426                                                 Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
            "   427                                                 \"\"\"\n",
            "   428   2730.9 MiB      0.0 MiB         129           for _ in range(max_new_tokens):\n",
            "   429                                                     # if the sequence context is growing too long we must crop it at block_size\n",
            "   430   2730.9 MiB      0.0 MiB         128               idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
            "   431                                                     # forward the model to get the logits for the index in the sequence\n",
            "   432   2730.9 MiB    586.3 MiB         128               logits, _ = self(idx_cond)\n",
            "   433                                                     # pluck the logits at the final step and scale by desired temperature\n",
            "   434   2730.9 MiB      0.0 MiB         128               logits = logits[:, -1, :] / temperature\n",
            "   435                                                     # optionally crop the logits to only the top k options\n",
            "   436   2730.9 MiB      0.0 MiB         128               if top_k is not None:\n",
            "   437   2730.9 MiB      3.0 MiB         128                   v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
            "   438   2730.9 MiB      0.0 MiB         128                   logits[logits < v[:, [-1]]] = -float('Inf')\n",
            "   439                                                     # apply softmax to convert logits to (normalized) probabilities\n",
            "   440   2730.9 MiB      0.0 MiB         128               probs = F.softmax(logits, dim=-1)\n",
            "   441                                                     # sample from the distribution\n",
            "   442   2730.9 MiB      0.0 MiB         128               idx_next = torch.multinomial(probs, num_samples=1)\n",
            "   443                                                     # append sampled index to the running sequence and continue\n",
            "   444   2730.9 MiB      0.0 MiB         128               idx = torch.cat((idx, idx_next), dim=1)\n",
            "   445                                         \n",
            "   446   2730.9 MiB      0.0 MiB           1           ppl = self.calculate_perplexity(idx)\n",
            "   447   2730.9 MiB      0.0 MiB           1           print(\"PERPLEXITY: \", ppl)\n",
            "   448   2730.9 MiB      0.0 MiB           1           return idx\n",
            "\n",
            "\n",
            "To be or not to be, that is the question. Any time you think about it, you think about the fact that all of that stuff is going to be a part of your life. Who knows? Maybe you'll eventually tell yourself you love writing, but that's for another time.\n",
            "\n",
            "That is, if you see me posting stories that I would like to meet online, or text, or email to someone who is interested. And I don't always do this. On the contrary, I try to do my best to keep myself from being an asshole to other people.\n",
            "\n",
            "The fact that I am not involved in more than one story about writing is an insult to anyone who\n",
            "---------------\n",
            "PERPLEXITY:  tensor(12382.4697, device='cuda:0')\n",
            "Filename: /content/drive/MyDrive/CS229S/cs229s-nanoGPT-rmg/model.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "   420   2730.9 MiB   2730.9 MiB           1       @torch.no_grad()\n",
            "   421                                             @profile\n",
            "   422                                             def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
            "   423                                                 \"\"\"\n",
            "   424                                                 Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
            "   425                                                 the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
            "   426                                                 Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
            "   427                                                 \"\"\"\n",
            "   428   2730.9 MiB      0.0 MiB         129           for _ in range(max_new_tokens):\n",
            "   429                                                     # if the sequence context is growing too long we must crop it at block_size\n",
            "   430   2730.9 MiB      0.0 MiB         128               idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
            "   431                                                     # forward the model to get the logits for the index in the sequence\n",
            "   432   2730.9 MiB      0.0 MiB         128               logits, _ = self(idx_cond)\n",
            "   433                                                     # pluck the logits at the final step and scale by desired temperature\n",
            "   434   2730.9 MiB      0.0 MiB         128               logits = logits[:, -1, :] / temperature\n",
            "   435                                                     # optionally crop the logits to only the top k options\n",
            "   436   2730.9 MiB      0.0 MiB         128               if top_k is not None:\n",
            "   437   2730.9 MiB      0.0 MiB         128                   v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
            "   438   2730.9 MiB      0.0 MiB         128                   logits[logits < v[:, [-1]]] = -float('Inf')\n",
            "   439                                                     # apply softmax to convert logits to (normalized) probabilities\n",
            "   440   2730.9 MiB      0.0 MiB         128               probs = F.softmax(logits, dim=-1)\n",
            "   441                                                     # sample from the distribution\n",
            "   442   2730.9 MiB      0.0 MiB         128               idx_next = torch.multinomial(probs, num_samples=1)\n",
            "   443                                                     # append sampled index to the running sequence and continue\n",
            "   444   2730.9 MiB      0.0 MiB         128               idx = torch.cat((idx, idx_next), dim=1)\n",
            "   445                                         \n",
            "   446   2730.9 MiB      0.0 MiB           1           ppl = self.calculate_perplexity(idx)\n",
            "   447   2730.9 MiB      0.0 MiB           1           print(\"PERPLEXITY: \", ppl)\n",
            "   448   2730.9 MiB      0.0 MiB           1           return idx\n",
            "\n",
            "\n",
            "To be or not to be, that is the question.\n",
            "\n",
            "What is your background and how do you get into the game?\n",
            "\n",
            "I have always wanted to play a game, and at the time I was working as a programmer at one of the producers who is a friend of mine. I was thinking about getting into a game, and I started thinking about my background, and I started thinking about my design skills. And I think I'm a good programmer and a great designer.\n",
            "\n",
            "That's really exciting.\n",
            "\n",
            "Oh, I feel kind of like you say, \"It's a puzzle game; don't go there.\"\n",
            "\n",
            "Yeah, that's right. Actually I\n",
            "---------------\n",
            "PERPLEXITY:  tensor(13372.1553, device='cuda:0')\n",
            "Filename: /content/drive/MyDrive/CS229S/cs229s-nanoGPT-rmg/model.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "   420   2730.9 MiB   2730.9 MiB           1       @torch.no_grad()\n",
            "   421                                             @profile\n",
            "   422                                             def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
            "   423                                                 \"\"\"\n",
            "   424                                                 Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
            "   425                                                 the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
            "   426                                                 Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
            "   427                                                 \"\"\"\n",
            "   428   2730.9 MiB      0.0 MiB         129           for _ in range(max_new_tokens):\n",
            "   429                                                     # if the sequence context is growing too long we must crop it at block_size\n",
            "   430   2730.9 MiB      0.0 MiB         128               idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
            "   431                                                     # forward the model to get the logits for the index in the sequence\n",
            "   432   2730.9 MiB      0.0 MiB         128               logits, _ = self(idx_cond)\n",
            "   433                                                     # pluck the logits at the final step and scale by desired temperature\n",
            "   434   2730.9 MiB      0.0 MiB         128               logits = logits[:, -1, :] / temperature\n",
            "   435                                                     # optionally crop the logits to only the top k options\n",
            "   436   2730.9 MiB      0.0 MiB         128               if top_k is not None:\n",
            "   437   2730.9 MiB      0.0 MiB         128                   v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
            "   438   2730.9 MiB      0.0 MiB         128                   logits[logits < v[:, [-1]]] = -float('Inf')\n",
            "   439                                                     # apply softmax to convert logits to (normalized) probabilities\n",
            "   440   2730.9 MiB      0.0 MiB         128               probs = F.softmax(logits, dim=-1)\n",
            "   441                                                     # sample from the distribution\n",
            "   442   2730.9 MiB      0.0 MiB         128               idx_next = torch.multinomial(probs, num_samples=1)\n",
            "   443                                                     # append sampled index to the running sequence and continue\n",
            "   444   2730.9 MiB      0.0 MiB         128               idx = torch.cat((idx, idx_next), dim=1)\n",
            "   445                                         \n",
            "   446   2730.9 MiB      0.0 MiB           1           ppl = self.calculate_perplexity(idx)\n",
            "   447   2730.9 MiB      0.0 MiB           1           print(\"PERPLEXITY: \", ppl)\n",
            "   448   2730.9 MiB      0.0 MiB           1           return idx\n",
            "\n",
            "\n",
            "To be or not to be, that is the question.\n",
            "\n",
            "I think it is clear that very nearly every single person in this country who happens to be an immigrant has never been able to get a visa to live in the United States. It is clearly a major problem for the working class, and all they can expect and expect is the extreme poverty of the American middle class. I do not think that there is anything that is required of any human being to be able to come to the United States.\n",
            "\n",
            "But what they will not get is any kind of immigration policy that makes it possible for somebody to become a lawfully present American in the first place. I do not mean, of course\n",
            "---------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}