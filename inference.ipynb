{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cy75saA_KfAt",
        "outputId": "8bfebb02-b6da-40e6-a9cd-5349d4555be0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.16.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Collecting memory-profiler\n",
            "  Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
            "Collecting torcheval\n",
            "  Downloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.38.0-py2.py3-none-any.whl (252 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: torcheval, smmap, setproctitle, sentry-sdk, pyarrow-hotfix, memory-profiler, docker-pycreds, dill, tiktoken, multiprocess, gitdb, GitPython, wandb, datasets\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.40 datasets-2.15.0 dill-0.3.7 docker-pycreds-0.4.0 gitdb-4.0.11 memory-profiler-0.61.0 multiprocess-0.70.15 pyarrow-hotfix-0.6 sentry-sdk-1.38.0 setproctitle-1.3.3 smmap-5.0.1 tiktoken-0.5.2 torcheval-0.0.7 wandb-0.16.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch numpy transformers datasets tiktoken wandb tqdm memory-profiler torcheval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEhwPEfR8Bf2",
        "outputId": "40dfd199-8c9b-485d-ecaf-e45fa4fdc09a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Suzdrt38VI1",
        "outputId": "b41697b8-2757-476a-d69f-b4d15580b5a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CS229S/cs229s-nanoGPT-rmg\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/CS229S/cs229s-nanoGPT-rmg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfOd-9uOiVwv",
        "outputId": "58c4ae9e-8f47-497c-f141-d58aad1ffeee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 301,966 tokens\n",
            "val has 36,059 tokens\n"
          ]
        }
      ],
      "source": [
        "!python data/shakespeare/prepare.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JQ4B-JnO7zn",
        "outputId": "336d16e6-454d-4210-9333-200a8d0c2e48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: init_from = gpt2\n",
            "Overriding: start = A weed is but an unloved flower.\n",
            "Overriding: num_samples = 1\n",
            "Overriding: max_new_tokens = 100\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "PERPLEXITY:  tensor(10993.2061, device='cuda:0')\n",
            "A weed is but an unloved flower. It sometimes requires the insertion of a needle or knife for the right to grow it.\n",
            "\n",
            "A weed grows better from a leaf. A weed leaves an individual leaf. It follows an orderly path through leaves. Its leaves are not always very delicate and remain smooth when planted in an open field. It devour any single leaf or branch, and the flower it is growing is sometimes weak or flat. It leaves a few leaves a day with a lot of fruit.\n",
            "\n",
            "A weed does not grow\n",
            "MAX MEM:  572878336\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "!python sample.py \\\n",
        "    --init_from=gpt2 \\\n",
        "    --start=\"A weed is but an unloved flower.\" \\\n",
        "    --num_samples=1 --max_new_tokens=100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash eval_quantized.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAzfCNSnUPcA",
        "outputId": "4245a5c7-8d18-47f1-9324-669bede0ddf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: init_from = gpt2\n",
            "Overriding: start = To be or not to be, that is the question.\n",
            "Overriding: max_new_tokens = 128\n",
            "Overriding: num_samples = 3\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "PERPLEXITY:  tensor(9440.5195, device='cuda:0')\n",
            "To be or not to be, that is the question. Have any of the four parties, in terms of their ability to do that, and if so, should the Speaker of the House decide that they should be disbarred?\"\n",
            "\n",
            "Mr. Speaker, this question was brought out of the House and then he admitted that it was a question that he would answer at a certain moment in the future.\n",
            "\n",
            "Mr. Speaker, I'll say this again. The House, I hope you are able to present this up in a right order. Many of the views in your budget and I know there's a lot of people who think that. But I'm happy to sit this one out\n",
            "MAX MEM:  576281600\n",
            "---------------\n",
            "PERPLEXITY:  tensor(12118.5605, device='cuda:0')\n",
            "To be or not to be, that is the question.\n",
            "\n",
            "The new law is supposed to help us all face the problem of the people who control our rights as citizens. It's a system that's rigged to allow the elite to take control of the people.\n",
            "\n",
            "Today, it's up to us to get rid of this system and we will. With the help of our allies and elected officials, we can stop it's insidious, cruel, cruel and cruel.\n",
            "\n",
            "We need to stop this system so as to save our democracy.<|endoftext|>President Obama for one, even as the Democrats refused to accept the number-one candidate in their race for the White House, the party's\n",
            "MAX MEM:  576283136\n",
            "---------------\n",
            "PERPLEXITY:  tensor(7294.9287, device='cuda:0')\n",
            "To be or not to be, that is the question.\n",
            "\n",
            "How did we know that?\n",
            "\n",
            "That is the question, because the science of the earth's climate hasn't even been scientifically validated, though the scientific literature has been validated through a long process of direct measurement of the Earth's rainfall levels, which means that there is no solid proof that the Earth is warming.\n",
            "\n",
            "We tried to prove that by using a new model. We made it very simple.\n",
            "\n",
            "We literally tested that model.\n",
            "\n",
            "\n",
            "So, the answer is in the case where water is the variable but that is what we were told when we invented the model.\n",
            "\n",
            "It is, in order to\n",
            "MAX MEM:  576283136\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Manually went in and set QUANTIZE = False\n",
        "!bash eval_quantized.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5IBJIWufTwb",
        "outputId": "124edee3-e328-432f-838b-1ab790473694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: init_from = gpt2\n",
            "Overriding: start = To be or not to be, that is the question.\n",
            "Overriding: max_new_tokens = 128\n",
            "Overriding: num_samples = 3\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "PERPLEXITY:  tensor(9771.9482, device='cuda:0')\n",
            "To be or not to be, that is the question. Any time you think about it, you think about the fact that all of that stuff is going to be a part of your life. Who knows? Maybe you'll eventually tell yourself you love writing, but that's for another time.\n",
            "\n",
            "That is, if you see me posting stories that I would like to meet online, or text, or email to someone who is interested. And I don't always do this. On the contrary, I try to do my best to keep myself from being an asshole to other people.\n",
            "\n",
            "The fact that I am not involved in more than one story about writing is an insult to anyone who\n",
            "MAX MEM:  827707904\n",
            "---------------\n",
            "PERPLEXITY:  tensor(12382.4697, device='cuda:0')\n",
            "To be or not to be, that is the question.\n",
            "\n",
            "What is your background and how do you get into the game?\n",
            "\n",
            "I have always wanted to play a game, and at the time I was working as a programmer at one of the producers who is a friend of mine. I was thinking about getting into a game, and I started thinking about my background, and I started thinking about my design skills. And I think I'm a good programmer and a great designer.\n",
            "\n",
            "That's really exciting.\n",
            "\n",
            "Oh, I feel kind of like you say, \"It's a puzzle game; don't go there.\"\n",
            "\n",
            "Yeah, that's right. Actually I\n",
            "MAX MEM:  827709440\n",
            "---------------\n",
            "PERPLEXITY:  tensor(13372.1553, device='cuda:0')\n",
            "To be or not to be, that is the question.\n",
            "\n",
            "I think it is clear that very nearly every single person in this country who happens to be an immigrant has never been able to get a visa to live in the United States. It is clearly a major problem for the working class, and all they can expect and expect is the extreme poverty of the American middle class. I do not think that there is anything that is required of any human being to be able to come to the United States.\n",
            "\n",
            "But what they will not get is any kind of immigration policy that makes it possible for somebody to become a lawfully present American in the first place. I do not mean, of course\n",
            "MAX MEM:  827709440\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --batch_size=4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-HeoSQgnRm4",
        "outputId": "010cb2fa-42be-4b66-a7ed-16971b495ad8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: batch_size = 4\n",
            "tokens per iteration will be: 163,840\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "num decayed parameter tensors: 0, with 0 parameters\n",
            "num non-decayed parameter tensors: 0, with 0 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1722, val loss 3.9851\n",
            "MAX MEM:  2229316608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --batch_size=12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxI4rwlOJsqz",
        "outputId": "1bb0263e-8089-47f4-df12-2ba2e6331c30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: batch_size = 12\n",
            "tokens per iteration will be: 491,520\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "num decayed parameter tensors: 0, with 0 parameters\n",
            "num non-decayed parameter tensors: 0, with 0 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1772, val loss 3.9716\n",
            "MAX MEM:  6390000640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# manually set QUANTIZE = False\n",
        "!python train.py --batch_size=4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8A7FM0rUIWIl",
        "outputId": "0358ba09-79ad-4e07-90c9-351b05f30d2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: batch_size = 4\n",
            "tokens per iteration will be: 163,840\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "num decayed parameter tensors: 50, with 124,318,464 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1752, val loss 4.0122\n",
            "MAX MEM:  2852541952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# shakespeare perplexities quantized\n",
        "import numpy as np\n",
        "print(np.exp(4.1722))\n",
        "print(np.exp(3.9851))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZOzLpZnH1Qt",
        "outputId": "3226a256-e780-4afc-cb41-2141a54266f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64.85798282834499\n",
            "53.790668275772944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# shakespeare perplexities unquantized\n",
        "import numpy as np\n",
        "print(np.exp(4.1752))\n",
        "print(np.exp(4.0122))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-NagJiWIpHF",
        "outputId": "72f9fbfc-1be2-4fd4-e84b-2ecbc84a782b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65.0528489298327\n",
            "55.26832723205136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample.py \\\n",
        "    --init_from=gpt2 \\\n",
        "    --start=\"For whom does the bell toll?\" \\\n",
        "    --num_samples=1 --max_new_tokens=100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TB3HNk1WJTBM",
        "outputId": "8da793f9-3a57-491a-d100-c201d493f7ff"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: init_from = gpt2\n",
            "Overriding: start = For whom does the bell toll?\n",
            "Overriding: num_samples = 1\n",
            "Overriding: max_new_tokens = 100\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "PERPLEXITY:  tensor(8622.4395, device='cuda:0')\n",
            "For whom does the bell toll?\n",
            "\n",
            "One of the most important questions we asked for our survey was which of three studies (A, B, and C) conducted the data linking the individual voices. None had an answer for this question.\n",
            "\n",
            "We found that voices of white conservative voters did not differ substantially from those of black liberals and white liberals.\n",
            "\n",
            "If voices of black liberals and black conservatives differed significantly from voices of white conservatives, we found a significant relationship between the two voices.\n",
            "\n",
            "But if voices of black\n",
            "MAX MEM:  572660736\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample.py \\\n",
        "    --init_from=gpt2-quantized \\\n",
        "    --start=\"For whom does the bell toll?\" \\\n",
        "    --num_samples=1 --max_new_tokens=100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HistRyS3J4Nw",
        "outputId": "1f2bfe1f-fd36-49ec-f037-4962b284d500"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: init_from = gpt2-quantized\n",
            "Overriding: start = For whom does the bell toll?\n",
            "Overriding: num_samples = 1\n",
            "Overriding: max_new_tokens = 100\n",
            "loading quantized weights from pretrained gpt: gpt2-quantized\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "PERPLEXITY:  tensor(8622.4395, device='cuda:0')\n",
            "For whom does the bell toll?\n",
            "\n",
            "One of the most important questions we asked for our survey was which of three studies (A, B, and C) conducted the data linking the individual voices. None had an answer for this question.\n",
            "\n",
            "We found that voices of white conservative voters did not differ substantially from those of black liberals and white liberals.\n",
            "\n",
            "If voices of black liberals and black conservatives differed significantly from voices of white conservatives, we found a significant relationship between the two voices.\n",
            "\n",
            "But if voices of black\n",
            "MAX MEM:  572660736\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample_speculative.py \\\n",
        "    --init_from=gpt2 \\\n",
        "    --start=\"For whom does the bell toll?\" \\\n",
        "    --num_samples=1 --max_new_tokens=100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIIn7fV6cw7Y",
        "outputId": "8dd796b2-83ae-4197-fb61-b98b6db48f56"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: init_from = gpt2\n",
            "Overriding: start = For whom does the bell toll?\n",
            "Overriding: num_samples = 1\n",
            "Overriding: max_new_tokens = 100\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "loading quantized weights from pretrained gpt: gpt2-quantized\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "PERPLEXITY:  tensor(3043.5291, device='cuda:0')\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/CS229S/cs229s-nanoGPT-rmg/sample_speculative.py\", line 103, in <module>\n",
            "    y = model.generate_speculative(x, max_new_tokens, draft_model, temperature=temperature, top_k=top_k, num_speculative=4)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/CS229S/cs229s-nanoGPT-rmg/model.py\", line 531, in generate_speculative\n",
            "    logits = all_logits[:, i, :] / temperature\n",
            "IndexError: index 6 is out of bounds for dimension 1 with size 1\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}