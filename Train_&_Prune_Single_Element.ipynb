{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqtKpwZz5hkI",
        "outputId": "d79d3ee5-7c64-48f0-8926-1d5c39d71699"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/University (M.S., Stanford)/CS 229S - Systems for ML/cs229s-nanoGPT-rmg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aNC46vF6V4m",
        "outputId": "f273f36f-72aa-440c-f04e-b0916eb95ad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/University (M.S., Stanford)/CS 229S - Systems for ML/cs229s-nanoGPT-rmg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch numpy transformers datasets tiktoken wandb tqdm memory-profiler torcheval"
      ],
      "metadata": {
        "id": "4i9iFCgX8nzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python data/wikitext/prepare.py"
      ],
      "metadata": {
        "id": "LwC1_3PL8Apw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCjw_CAr0Jh3",
        "outputId": "d3b3bb63-7118-43f1-c26d-2b873f994016"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens per iteration will be: 163,840\n",
            "Resuming training from out\n",
            "number of parameters: 353.77M\n",
            "num decayed parameter tensors: 98, with 354,501,632 parameters\n",
            "num non-decayed parameter tensors: 194, with 321,536 parameters\n",
            "using fused AdamW: True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from model import GPTConfig, GPT\n",
        "from pruning import convert_to_prunable, compress_layers, PrunableLinear\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
        "# I/O\n",
        "out_dir = 'out'\n",
        "eval_interval = 100\n",
        "log_interval = 1\n",
        "eval_iters = 200\n",
        "eval_only = False # if True, script exits right after the first eval\n",
        "always_save_checkpoint = False # if True, always save a checkpoint after each eval\n",
        "init_from = 'resume' # 'gpt2-medium' # 'scratch' or 'resume' or 'gpt2*'\n",
        "# wandb logging\n",
        "wandb_log = False # disabled by default\n",
        "wandb_project = 'cs229s'\n",
        "wandb_run_name = 'gpt2' # 'run' + str(time.time())\n",
        "# data\n",
        "dataset = 'wikitext'\n",
        "gradient_accumulation_steps = 5 * 8 # used to simulate larger batch sizes\n",
        "batch_size = 4 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
        "block_size = 1024\n",
        "# model\n",
        "n_layer = 12\n",
        "n_head = 12\n",
        "n_embd = 768\n",
        "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
        "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
        "# adamw optimizer\n",
        "learning_rate = 6e-4 # max learning rate\n",
        "max_iters = 100 # total number of training iterations\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
        "# learning rate decay settings\n",
        "decay_lr = True # whether to decay the learning rate\n",
        "warmup_iters = 2000 # how many steps to warm up for\n",
        "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
        "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
        "# DDP settings\n",
        "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
        "# system\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "# -----------------------------------------------------------------------------\n",
        "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
        "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# if not ddp, we are running on a single gpu, and one process\n",
        "seed_offset = 0\n",
        "tokens_per_iter = gradient_accumulation_steps * batch_size * block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "# poor man's data loader\n",
        "data_dir = os.path.join('data', dataset)\n",
        "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "\n",
        "# attempt to derive vocab_size from the dataset\n",
        "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
        "meta_vocab_size = None\n",
        "if os.path.exists(meta_path):\n",
        "    with open(meta_path, 'rb') as f:\n",
        "        meta = pickle.load(f)\n",
        "    meta_vocab_size = meta['vocab_size']\n",
        "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
        "\n",
        "# model init\n",
        "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
        "                  bias=bias, vocab_size=None, dropout=dropout) # start with model_args from command line\n",
        "if init_from == 'scratch':\n",
        "    # init a new model from scratch\n",
        "    print(\"Initializing a new model from scratch\")\n",
        "    # determine the vocab size we'll use for from-scratch training\n",
        "    if meta_vocab_size is None:\n",
        "        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
        "    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)\n",
        "elif init_from == 'resume':\n",
        "    print(f\"Resuming training from {out_dir}\")\n",
        "    # resume training from a checkpoint.\n",
        "    ckpt_path = os.path.join(out_dir, 'ckpt_500_pruned_0.42.pt')\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    checkpoint_model_args = checkpoint['model_args']\n",
        "    # force these config attributes to be equal otherwise we can't even resume training\n",
        "    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
        "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
        "        model_args[k] = checkpoint_model_args[k]\n",
        "    # create the model\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)\n",
        "    convert_to_prunable(model, device=device_type)\n",
        "    state_dict = checkpoint['model']\n",
        "    # fix the keys of the state dictionary :(\n",
        "    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
        "    unwanted_prefix = '_orig_mod.'\n",
        "    for k,v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "    model.load_state_dict(state_dict)\n",
        "    iter_num = checkpoint['iter_num']\n",
        "    best_val_loss = checkpoint['best_val_loss']\n",
        "elif init_from.startswith('gpt2'):\n",
        "    print(f\"Initializing from OpenAI GPT-2 weights: {init_from}\")\n",
        "    # initialize from OpenAI GPT-2 weights\n",
        "    override_args = dict(dropout=dropout)\n",
        "    model = GPT.from_pretrained(init_from, override_args)\n",
        "    # read off the created config params, so we can store them into checkpoint correctly\n",
        "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
        "        model_args[k] = getattr(model.config, k)\n",
        "# crop down the model block size if desired, using model surgery\n",
        "if block_size < model.config.block_size:\n",
        "    model.crop_block_size(block_size)\n",
        "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
        "model.to(device)\n",
        "\n",
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "\n",
        "# optimizer\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "if init_from == 'resume':\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "checkpoint = None # free up memory\n",
        "\n",
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "# logging\n",
        "if wandb_log:\n",
        "    import wandb\n",
        "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n",
        "\n",
        "del state_dict, checkpoint\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert_to_prunable(model, device=device_type)"
      ],
      "metadata": {
        "id": "AWQUn1fOBfxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def finetune(model, iter_num, best_val_loss, eval_only=False):\n",
        "  # training loop\n",
        "  X, Y = get_batch('train') # fetch the very first batch\n",
        "  t0 = time.time()\n",
        "  running_mfu = -1.0\n",
        "  for local_iter_num in range(max_iters):\n",
        "\n",
        "      # determine and set the learning rate for this iteration\n",
        "      lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "      for param_group in optimizer.param_groups:\n",
        "          param_group['lr'] = lr\n",
        "\n",
        "      # evaluate the loss on train/val sets and write checkpoints\n",
        "      if iter_num % eval_interval == 0:\n",
        "          losses = estimate_loss()\n",
        "          print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "          if wandb_log:\n",
        "              wandb.log({\n",
        "                  \"iter\": iter_num,\n",
        "                  \"train/loss\": losses['train'],\n",
        "                  \"val/loss\": losses['val'],\n",
        "                  \"lr\": lr,\n",
        "                  \"mfu\": running_mfu*100, # convert to percentage\n",
        "              })\n",
        "          if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "              best_val_loss = losses['val']\n",
        "              if iter_num > 0:\n",
        "                  checkpoint = {\n",
        "                      'model': model.state_dict(),\n",
        "                      'optimizer': optimizer.state_dict(),\n",
        "                      'model_args': model_args,\n",
        "                      'iter_num': iter_num,\n",
        "                      'best_val_loss': best_val_loss,\n",
        "                      'config': config,\n",
        "                  }\n",
        "                  print(f\"saving checkpoint to {out_dir}\")\n",
        "                  torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "      if local_iter_num == 0 and eval_only:\n",
        "          break\n",
        "\n",
        "      # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "      # and using the GradScaler if data type is float16\n",
        "      for micro_step in range(gradient_accumulation_steps):\n",
        "          with ctx:\n",
        "              logits, loss = model(X, Y)\n",
        "              loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "          # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "          X, Y = get_batch('train')\n",
        "          # backward pass, with gradient scaling if training in fp16\n",
        "          scaler.scale(loss).backward()\n",
        "      # clip the gradient\n",
        "      if grad_clip != 0.0:\n",
        "          scaler.unscale_(optimizer)\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "      # step the optimizer and scaler if training in fp16\n",
        "      scaler.step(optimizer)\n",
        "      scaler.update()\n",
        "      # flush the gradients as soon as we can, no need for this memory anymore\n",
        "      optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "      # timing and logging\n",
        "      t1 = time.time()\n",
        "      dt = t1 - t0\n",
        "      t0 = t1\n",
        "      if iter_num % log_interval == 0:\n",
        "          # get loss as float. note: this is a CPU-GPU sync point\n",
        "          # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
        "          lossf = loss.item() * gradient_accumulation_steps\n",
        "          if local_iter_num >= 5: # let the training loop settle a bit\n",
        "              mfu = model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
        "              running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "          print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "\n",
        "      iter_num += 1\n",
        "  return iter_num, best_val_loss"
      ],
      "metadata": {
        "id": "OBVpMckTHJJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWr3jMFw0ZKN",
        "outputId": "29c30e5a-0fa2-4d2a-8745-f77f0e002fda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 3.7232, val loss 3.7326\n",
            "iter 0: loss 3.8476, time 138304.75ms, mfu -100.00%\n",
            "iter 1: loss 3.5523, time 31620.78ms, mfu -100.00%\n",
            "iter 2: loss 3.8951, time 31535.70ms, mfu -100.00%\n",
            "iter 3: loss 3.6432, time 31759.12ms, mfu -100.00%\n",
            "iter 4: loss 3.6798, time 31835.23ms, mfu -100.00%\n",
            "iter 5: loss 3.7793, time 31738.59ms, mfu 4.01%\n",
            "iter 6: loss 3.6765, time 31699.78ms, mfu 4.01%\n",
            "iter 7: loss 3.6175, time 31734.08ms, mfu 4.01%\n",
            "iter 8: loss 3.5956, time 31774.57ms, mfu 4.01%\n",
            "iter 9: loss 3.7610, time 31749.35ms, mfu 4.01%\n",
            "iter 10: loss 3.7035, time 31711.85ms, mfu 4.01%\n",
            "iter 11: loss 3.5853, time 31625.33ms, mfu 4.01%\n",
            "iter 12: loss 3.5244, time 31618.93ms, mfu 4.01%\n",
            "iter 13: loss 3.5588, time 31592.44ms, mfu 4.02%\n",
            "iter 14: loss 3.5339, time 31542.19ms, mfu 4.02%\n",
            "iter 15: loss 3.6042, time 31568.39ms, mfu 4.02%\n",
            "iter 16: loss 3.4638, time 31736.23ms, mfu 4.02%\n",
            "iter 17: loss 3.4432, time 31840.00ms, mfu 4.02%\n",
            "iter 18: loss 3.5116, time 31844.93ms, mfu 4.02%\n",
            "iter 19: loss 3.4349, time 31811.03ms, mfu 4.01%\n",
            "iter 20: loss 3.3000, time 31824.93ms, mfu 4.01%\n",
            "iter 21: loss 3.4941, time 31785.84ms, mfu 4.01%\n",
            "iter 22: loss 3.4543, time 31710.05ms, mfu 4.01%\n",
            "iter 23: loss 3.2301, time 31727.64ms, mfu 4.01%\n",
            "iter 24: loss 3.4035, time 31721.06ms, mfu 4.01%\n",
            "iter 25: loss 3.3403, time 31554.87ms, mfu 4.01%\n",
            "iter 26: loss 3.2395, time 31763.58ms, mfu 4.01%\n",
            "iter 27: loss 3.2169, time 31775.10ms, mfu 4.01%\n",
            "iter 28: loss 3.4475, time 31641.39ms, mfu 4.01%\n",
            "iter 29: loss 3.1738, time 31604.83ms, mfu 4.02%\n",
            "iter 30: loss 3.4122, time 31626.64ms, mfu 4.02%\n",
            "iter 31: loss 3.1282, time 31609.14ms, mfu 4.02%\n",
            "iter 32: loss 3.1820, time 31633.10ms, mfu 4.02%\n",
            "iter 33: loss 3.1628, time 31633.92ms, mfu 4.02%\n",
            "iter 34: loss 3.2325, time 31605.20ms, mfu 4.02%\n",
            "iter 35: loss 3.1175, time 31516.11ms, mfu 4.02%\n",
            "iter 36: loss 3.2081, time 31784.17ms, mfu 4.02%\n",
            "iter 37: loss 3.1680, time 31842.98ms, mfu 4.02%\n",
            "iter 38: loss 3.0822, time 31718.54ms, mfu 4.02%\n",
            "iter 39: loss 3.1180, time 31585.94ms, mfu 4.02%\n",
            "iter 40: loss 3.0479, time 31621.10ms, mfu 4.02%\n",
            "iter 41: loss 3.1208, time 31607.78ms, mfu 4.02%\n",
            "iter 42: loss 3.1571, time 31573.10ms, mfu 4.02%\n",
            "iter 43: loss 3.0869, time 31643.16ms, mfu 4.02%\n",
            "iter 44: loss 3.3685, time 31601.31ms, mfu 4.02%\n",
            "iter 45: loss 3.1092, time 31858.94ms, mfu 4.02%\n",
            "iter 46: loss 3.3292, time 31799.65ms, mfu 4.02%\n",
            "iter 47: loss 3.1233, time 31641.98ms, mfu 4.02%\n",
            "iter 48: loss 3.2363, time 31636.13ms, mfu 4.02%\n",
            "iter 49: loss 3.2049, time 31608.33ms, mfu 4.02%\n",
            "iter 50: loss 3.1725, time 31751.05ms, mfu 4.02%\n",
            "iter 51: loss 3.0872, time 31817.47ms, mfu 4.02%\n",
            "iter 52: loss 3.1518, time 31688.86ms, mfu 4.02%\n",
            "iter 53: loss 3.2431, time 31550.00ms, mfu 4.02%\n",
            "iter 54: loss 3.0024, time 31594.54ms, mfu 4.02%\n",
            "iter 55: loss 3.1252, time 31628.49ms, mfu 4.02%\n",
            "iter 56: loss 2.9412, time 31694.78ms, mfu 4.02%\n",
            "iter 57: loss 3.2762, time 31774.72ms, mfu 4.02%\n",
            "iter 58: loss 3.1127, time 31827.42ms, mfu 4.02%\n",
            "iter 59: loss 3.1000, time 31802.93ms, mfu 4.02%\n",
            "iter 60: loss 2.9952, time 31736.80ms, mfu 4.02%\n",
            "iter 61: loss 2.9477, time 31818.03ms, mfu 4.01%\n",
            "iter 62: loss 3.0035, time 31826.11ms, mfu 4.01%\n",
            "iter 63: loss 3.0878, time 31718.56ms, mfu 4.01%\n",
            "iter 64: loss 3.0713, time 31785.20ms, mfu 4.01%\n",
            "iter 65: loss 3.1025, time 31818.21ms, mfu 4.01%\n",
            "iter 66: loss 3.0213, time 31672.79ms, mfu 4.01%\n",
            "iter 67: loss 3.0777, time 31633.07ms, mfu 4.01%\n",
            "iter 68: loss 2.9883, time 31623.52ms, mfu 4.01%\n",
            "iter 69: loss 3.0485, time 31546.25ms, mfu 4.02%\n",
            "iter 70: loss 2.9809, time 31623.46ms, mfu 4.02%\n",
            "iter 71: loss 2.9744, time 31718.17ms, mfu 4.02%\n",
            "iter 72: loss 2.9271, time 31834.45ms, mfu 4.02%\n",
            "iter 73: loss 2.8855, time 31870.99ms, mfu 4.01%\n",
            "iter 74: loss 2.9897, time 31837.31ms, mfu 4.01%\n",
            "iter 75: loss 2.8242, time 31710.17ms, mfu 4.01%\n",
            "iter 76: loss 2.8594, time 31820.49ms, mfu 4.01%\n",
            "iter 77: loss 3.0080, time 31841.34ms, mfu 4.01%\n",
            "iter 78: loss 2.9570, time 31727.12ms, mfu 4.01%\n",
            "iter 79: loss 3.0005, time 31711.35ms, mfu 4.01%\n",
            "iter 80: loss 2.9068, time 31730.41ms, mfu 4.01%\n",
            "iter 81: loss 3.0260, time 31700.43ms, mfu 4.01%\n",
            "iter 82: loss 2.9559, time 31702.11ms, mfu 4.01%\n",
            "iter 83: loss 2.8301, time 31665.56ms, mfu 4.01%\n",
            "iter 84: loss 3.2671, time 31701.67ms, mfu 4.01%\n",
            "iter 85: loss 3.0839, time 31727.53ms, mfu 4.01%\n",
            "iter 86: loss 2.8721, time 31812.16ms, mfu 4.01%\n",
            "iter 87: loss 2.8994, time 31771.12ms, mfu 4.01%\n",
            "iter 88: loss 2.9675, time 31723.34ms, mfu 4.01%\n",
            "iter 89: loss 2.7085, time 31729.67ms, mfu 4.01%\n",
            "iter 90: loss 3.0200, time 31647.70ms, mfu 4.01%\n",
            "iter 91: loss 2.9229, time 31529.94ms, mfu 4.02%\n",
            "iter 92: loss 3.0344, time 31671.28ms, mfu 4.02%\n",
            "iter 93: loss 2.8267, time 31820.84ms, mfu 4.01%\n",
            "iter 94: loss 2.9279, time 31844.93ms, mfu 4.01%\n",
            "iter 95: loss 2.9031, time 31788.22ms, mfu 4.01%\n",
            "iter 96: loss 3.1034, time 31708.43ms, mfu 4.01%\n",
            "iter 97: loss 3.0125, time 31715.15ms, mfu 4.01%\n",
            "iter 98: loss 2.9442, time 31682.14ms, mfu 4.01%\n",
            "iter 99: loss 3.0288, time 31621.98ms, mfu 4.01%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(model.state_dict(), \"./out/backup_100_it.pt\")"
      ],
      "metadata": {
        "id": "2jWhNJ8xanUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPCvTmTjbCQl",
        "outputId": "29834e0d-09bd-4962-9c8b-30783440ce3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 100: train loss 2.9703, val loss 3.0115\n",
            "saving checkpoint to out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pcnt_pruned = model.prune(m=0.015)\n",
        "print(pcnt_pruned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6E5_txkue-Zv",
        "outputId": "81761977-e0d0-4972-f1c8-6b08a987441e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.12141100003021132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# for name, module in model.named_modules():\n",
        "#   if isinstance(module, PrunableLinear):\n",
        "#     print(name, module.n_pruned)\n",
        "# # model.transformer['h'][2].attn.c_attn.n_pruned"
      ],
      "metadata": {
        "id": "kokBylr5djIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint = {\n",
        "#     'model': model.state_dict(),\n",
        "#     'optimizer': optimizer.state_dict(),\n",
        "#     'model_args': model_args,\n",
        "#     'iter_num': iter_num,\n",
        "#     'best_val_loss': best_val_loss,\n",
        "#     'config': config,\n",
        "# }"
      ],
      "metadata": {
        "id": "0oY9T4RBadm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pcnt_pruned = model.prune(p=0.1)\n",
        "# losses = estimate_loss()\n",
        "# print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")"
      ],
      "metadata": {
        "id": "Tu11H7Fa4U-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34lVnoiFhcbx",
        "outputId": "deebbb8e-ace1-4cfe-d7a9-04cf584bcee1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 100: train loss 2.9889, val loss 3.0161\n",
            "iter 100: loss 3.0003, time 138688.28ms, mfu -100.00%\n",
            "iter 101: loss 2.9146, time 31454.99ms, mfu -100.00%\n",
            "iter 102: loss 2.9878, time 31515.09ms, mfu -100.00%\n",
            "iter 103: loss 2.8782, time 31510.14ms, mfu -100.00%\n",
            "iter 104: loss 2.9141, time 31503.95ms, mfu -100.00%\n",
            "iter 105: loss 2.9519, time 31454.09ms, mfu 4.05%\n",
            "iter 106: loss 3.0229, time 31744.26ms, mfu 4.04%\n",
            "iter 107: loss 2.9633, time 31592.72ms, mfu 4.04%\n",
            "iter 108: loss 2.9199, time 31489.54ms, mfu 4.04%\n",
            "iter 109: loss 3.0572, time 31549.54ms, mfu 4.04%\n",
            "iter 110: loss 2.9676, time 31664.88ms, mfu 4.04%\n",
            "iter 111: loss 2.9564, time 31685.03ms, mfu 4.04%\n",
            "iter 112: loss 3.0131, time 31741.12ms, mfu 4.04%\n",
            "iter 113: loss 3.0980, time 31670.20ms, mfu 4.03%\n",
            "iter 114: loss 3.0155, time 31741.99ms, mfu 4.03%\n",
            "iter 115: loss 3.0021, time 31737.56ms, mfu 4.03%\n",
            "iter 116: loss 2.9743, time 31589.28ms, mfu 4.03%\n",
            "iter 117: loss 2.9903, time 31544.30ms, mfu 4.03%\n",
            "iter 118: loss 2.9818, time 31551.33ms, mfu 4.03%\n",
            "iter 119: loss 2.8343, time 31523.43ms, mfu 4.03%\n",
            "iter 120: loss 2.9723, time 31504.52ms, mfu 4.03%\n",
            "iter 121: loss 3.1217, time 31494.75ms, mfu 4.03%\n",
            "iter 122: loss 3.0063, time 31494.14ms, mfu 4.03%\n",
            "iter 123: loss 3.0620, time 31469.85ms, mfu 4.04%\n",
            "iter 124: loss 2.9514, time 31455.11ms, mfu 4.04%\n",
            "iter 125: loss 2.8041, time 31480.29ms, mfu 4.04%\n",
            "iter 126: loss 2.9298, time 31477.25ms, mfu 4.04%\n",
            "iter 127: loss 3.1146, time 31514.73ms, mfu 4.04%\n",
            "iter 128: loss 2.9582, time 31455.65ms, mfu 4.04%\n",
            "iter 129: loss 3.0304, time 31544.26ms, mfu 4.04%\n",
            "iter 130: loss 2.7856, time 31749.68ms, mfu 4.04%\n",
            "iter 131: loss 2.9968, time 31652.88ms, mfu 4.03%\n",
            "iter 132: loss 2.9081, time 31621.51ms, mfu 4.03%\n",
            "iter 133: loss 3.0479, time 31584.45ms, mfu 4.03%\n",
            "iter 134: loss 2.9935, time 31569.23ms, mfu 4.03%\n",
            "iter 135: loss 2.9318, time 31552.65ms, mfu 4.03%\n",
            "iter 136: loss 2.9216, time 31552.42ms, mfu 4.03%\n",
            "iter 137: loss 2.9228, time 31544.48ms, mfu 4.03%\n",
            "iter 138: loss 3.0178, time 31495.51ms, mfu 4.04%\n",
            "iter 139: loss 2.9318, time 31502.02ms, mfu 4.04%\n",
            "iter 140: loss 2.9677, time 31549.67ms, mfu 4.04%\n",
            "iter 141: loss 2.8690, time 31681.32ms, mfu 4.03%\n",
            "iter 142: loss 3.1395, time 31633.60ms, mfu 4.03%\n",
            "iter 143: loss 2.9157, time 31555.44ms, mfu 4.03%\n",
            "iter 144: loss 2.9698, time 31573.72ms, mfu 4.03%\n",
            "iter 145: loss 2.9085, time 31516.74ms, mfu 4.03%\n",
            "iter 146: loss 2.7935, time 31460.82ms, mfu 4.04%\n",
            "iter 147: loss 2.8709, time 31456.71ms, mfu 4.04%\n",
            "iter 148: loss 2.8795, time 31512.99ms, mfu 4.04%\n",
            "iter 149: loss 3.0083, time 31600.14ms, mfu 4.04%\n",
            "iter 150: loss 2.8978, time 31665.80ms, mfu 4.03%\n",
            "iter 151: loss 2.8537, time 31787.77ms, mfu 4.03%\n",
            "iter 152: loss 2.9291, time 31628.42ms, mfu 4.03%\n",
            "iter 153: loss 2.9719, time 31495.90ms, mfu 4.03%\n",
            "iter 154: loss 3.0537, time 31452.75ms, mfu 4.03%\n",
            "iter 155: loss 2.9103, time 31442.87ms, mfu 4.04%\n",
            "iter 156: loss 2.8836, time 31478.45ms, mfu 4.04%\n",
            "iter 157: loss 3.0548, time 31476.08ms, mfu 4.04%\n",
            "iter 158: loss 3.0365, time 31456.40ms, mfu 4.04%\n",
            "iter 159: loss 2.7725, time 31466.03ms, mfu 4.04%\n",
            "iter 160: loss 2.8845, time 31620.48ms, mfu 4.04%\n",
            "iter 161: loss 2.9489, time 31750.15ms, mfu 4.04%\n",
            "iter 162: loss 2.8963, time 31740.42ms, mfu 4.03%\n",
            "iter 163: loss 2.9093, time 31591.30ms, mfu 4.03%\n",
            "iter 164: loss 2.9542, time 31531.91ms, mfu 4.03%\n",
            "iter 165: loss 2.9829, time 31519.12ms, mfu 4.03%\n",
            "iter 166: loss 2.9186, time 31476.00ms, mfu 4.03%\n",
            "iter 167: loss 2.9806, time 31475.73ms, mfu 4.04%\n",
            "iter 168: loss 2.9706, time 31439.84ms, mfu 4.04%\n",
            "iter 169: loss 2.9730, time 31433.98ms, mfu 4.04%\n",
            "iter 170: loss 2.9420, time 31543.42ms, mfu 4.04%\n",
            "iter 171: loss 2.9498, time 31654.50ms, mfu 4.04%\n",
            "iter 172: loss 2.9209, time 31669.02ms, mfu 4.04%\n",
            "iter 173: loss 2.8861, time 31744.13ms, mfu 4.03%\n",
            "iter 174: loss 2.8765, time 31728.41ms, mfu 4.03%\n",
            "iter 175: loss 2.8628, time 31576.57ms, mfu 4.03%\n",
            "iter 176: loss 2.8183, time 31474.80ms, mfu 4.03%\n",
            "iter 177: loss 2.9530, time 31429.01ms, mfu 4.03%\n",
            "iter 178: loss 3.0265, time 31448.77ms, mfu 4.04%\n",
            "iter 179: loss 3.0233, time 31487.73ms, mfu 4.04%\n",
            "iter 180: loss 2.9550, time 31618.92ms, mfu 4.04%\n",
            "iter 181: loss 2.9008, time 31624.79ms, mfu 4.03%\n",
            "iter 182: loss 2.8408, time 31578.09ms, mfu 4.03%\n",
            "iter 183: loss 2.9999, time 31536.75ms, mfu 4.03%\n",
            "iter 184: loss 2.9918, time 31511.04ms, mfu 4.04%\n",
            "iter 185: loss 2.8977, time 31526.86ms, mfu 4.04%\n",
            "iter 186: loss 2.7852, time 31513.09ms, mfu 4.04%\n",
            "iter 187: loss 2.9581, time 31503.52ms, mfu 4.04%\n",
            "iter 188: loss 2.8946, time 31472.64ms, mfu 4.04%\n",
            "iter 189: loss 2.9412, time 31459.04ms, mfu 4.04%\n",
            "iter 190: loss 3.0391, time 31447.84ms, mfu 4.04%\n",
            "iter 191: loss 2.8368, time 31395.16ms, mfu 4.04%\n",
            "iter 192: loss 2.9983, time 31574.04ms, mfu 4.04%\n",
            "iter 193: loss 2.8701, time 31678.77ms, mfu 4.04%\n",
            "iter 194: loss 2.7903, time 31649.23ms, mfu 4.04%\n",
            "iter 195: loss 2.8566, time 31562.16ms, mfu 4.04%\n",
            "iter 196: loss 2.8765, time 31529.92ms, mfu 4.04%\n",
            "iter 197: loss 2.7544, time 31478.61ms, mfu 4.04%\n",
            "iter 198: loss 2.7981, time 31578.28ms, mfu 4.04%\n",
            "iter 199: loss 3.0597, time 31671.08ms, mfu 4.04%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(iter_num, best_val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wsTrpCevcIr",
        "outputId": "b045ca02-9553-4e75-8ac4-b395b6d36764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200 tensor(3.0115, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "av_D7tbLhf27",
        "outputId": "9a8a656d-de63-4da1-de1a-66440a247e0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: train loss 2.8916, val loss 2.9354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(iter_num, best_val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NL8D6xuxwuwM",
        "outputId": "d553e63d-5ca4-4133-ddf1-d320dca9510e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200 tensor(2.9343)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f\"saving checkpoint to {out_dir}\")\n",
        "# torch.save({\n",
        "#     'model': model.state_dict(),\n",
        "#     'optimizer': optimizer.state_dict(),\n",
        "#     'model_args': model_args,\n",
        "#     'iter_num': iter_num,\n",
        "#     'best_val_loss': best_val_loss,\n",
        "#     'config': config,\n",
        "# }, os.path.join(out_dir, 'ckpt_200_pruned_0.12.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKCn_Dv-yKyi",
        "outputId": "98e9a790-c485-4e77-8a86-1e9e9da9df3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving checkpoint to out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for module in model.modules():\n",
        "    if isinstance(module, PrunableLinear):\n",
        "      module.weight *= module.mask"
      ],
      "metadata": {
        "id": "Aunkz4x05AuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.where(~model.transformer['h'][2].attn.c_proj.mask[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dMGYTKT46L4",
        "outputId": "2b5da8c3-fa95-408b-98a8-e89ae141dd47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([   8,   11,   13,   14,   24,   26,   27,   30,   31,   34,   36,   40,\n",
              "           42,   44,   47,   49,   52,   55,   57,   62,   66,   71,   81,   85,\n",
              "           90,   92,   96,  106,  107,  109,  111,  113,  114,  115,  118,  124,\n",
              "          125,  126,  129,  131,  132,  135,  138,  144,  151,  163,  172,  178,\n",
              "          193,  206,  211,  215,  216,  241,  242,  247,  248,  254,  257,  260,\n",
              "          263,  281,  288,  289,  297,  299,  300,  318,  322,  327,  329,  361,\n",
              "          368,  394,  397,  398,  403,  405,  408,  410,  411,  413,  460,  461,\n",
              "          468,  479,  484,  490,  491,  493,  494,  495,  499,  518,  519,  530,\n",
              "          534,  537,  538,  548,  549,  554,  555,  567,  570,  575,  579,  580,\n",
              "          581,  593,  596,  600,  601,  605,  606,  607,  610,  614,  634,  641,\n",
              "          646,  647,  650,  667,  672,  675,  680,  700,  704,  707,  708,  711,\n",
              "          716,  719,  721,  727,  728,  731,  733,  734,  737,  740,  744,  745,\n",
              "          747,  753,  758,  762,  766,  767,  768,  770,  771,  778,  790,  812,\n",
              "          820,  828,  833,  835,  838,  842,  843,  846,  851,  858,  859,  860,\n",
              "          864,  866,  874,  882,  884,  891,  896,  907,  908,  910,  912,  913,\n",
              "          917,  920,  922,  923,  925,  932,  942,  960,  980,  995, 1003, 1005,\n",
              "         1013, 1019, 1022, 1023], device='cuda:0'),)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.prune(m=0.025)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5HOV6soychZ",
        "outputId": "d74013a0-dc07-4e77-ed2a-fe5cee255bcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.20027845300998892"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for module in model.modules():\n",
        "    if isinstance(module, PrunableLinear):\n",
        "      module.weight *= module.mask"
      ],
      "metadata": {
        "id": "t_hYYzjzypPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.where(~model.transformer['h'][2].attn.c_proj.mask[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dk42P5J50Oe6",
        "outputId": "2bc3de30-0030-4235-d93b-58ec8e3f37fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([   1,    2,    8,    9,   11,   13,   14,   23,   24,   26,   27,   29,\n",
              "           30,   31,   34,   36,   40,   41,   42,   43,   44,   45,   47,   49,\n",
              "           52,   55,   57,   59,   62,   65,   66,   69,   71,   72,   74,   75,\n",
              "           76,   78,   80,   81,   83,   85,   87,   89,   90,   92,   96,  104,\n",
              "          105,  106,  107,  109,  110,  111,  113,  114,  115,  117,  118,  120,\n",
              "          122,  123,  124,  125,  126,  129,  131,  132,  133,  135,  136,  138,\n",
              "          144,  148,  149,  151,  163,  172,  175,  178,  182,  186,  193,  195,\n",
              "          197,  202,  204,  205,  206,  210,  211,  214,  215,  216,  241,  242,\n",
              "          246,  247,  248,  252,  254,  257,  260,  263,  266,  267,  271,  274,\n",
              "          281,  283,  288,  289,  297,  299,  300,  302,  303,  312,  318,  320,\n",
              "          322,  327,  329,  333,  334,  336,  351,  361,  363,  368,  369,  383,\n",
              "          391,  393,  394,  395,  397,  398,  403,  405,  406,  408,  410,  411,\n",
              "          412,  413,  417,  424,  429,  434,  439,  444,  451,  460,  461,  462,\n",
              "          464,  467,  468,  470,  473,  478,  479,  480,  482,  484,  486,  490,\n",
              "          491,  493,  494,  495,  497,  499,  503,  504,  505,  507,  515,  518,\n",
              "          519,  521,  530,  534,  537,  538,  547,  548,  549,  554,  555,  563,\n",
              "          567,  570,  575,  579,  580,  581,  583,  587,  592,  593,  596,  600,\n",
              "          601,  605,  606,  607,  610,  612,  614,  615,  624,  630,  632,  633,\n",
              "          634,  637,  641,  646,  647,  650,  653,  667,  668,  672,  675,  677,\n",
              "          680,  700,  703,  704,  706,  707,  708,  709,  711,  716,  718,  719,\n",
              "          720,  721,  727,  728,  729,  731,  732,  733,  734,  737,  738,  740,\n",
              "          744,  745,  746,  747,  749,  752,  753,  758,  761,  762,  766,  767,\n",
              "          768,  770,  771,  773,  778,  779,  790,  797,  808,  810,  812,  817,\n",
              "          820,  822,  828,  833,  835,  838,  839,  842,  843,  844,  846,  851,\n",
              "          858,  859,  860,  861,  864,  865,  866,  874,  876,  882,  884,  885,\n",
              "          886,  891,  896,  898,  899,  906,  907,  908,  910,  912,  913,  917,\n",
              "          920,  922,  923,  925,  931,  932,  942,  949,  952,  957,  960,  967,\n",
              "          980,  995, 1003, 1005, 1009, 1013, 1019, 1022, 1023], device='cuda:0'),)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqSML8pa3KGU",
        "outputId": "d8ad5083-36d0-4a37-91b5-083159ca3cf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: train loss 2.9388, val loss 2.9758\n",
            "iter 200: loss 2.9429, time 137213.41ms, mfu -100.00%\n",
            "iter 201: loss 2.8565, time 31290.64ms, mfu -100.00%\n",
            "iter 202: loss 2.9032, time 31250.92ms, mfu -100.00%\n",
            "iter 203: loss 2.8096, time 31192.49ms, mfu -100.00%\n",
            "iter 204: loss 2.8390, time 31329.26ms, mfu -100.00%\n",
            "iter 205: loss 2.8935, time 31586.36ms, mfu 4.03%\n",
            "iter 206: loss 2.9585, time 31219.90ms, mfu 4.04%\n",
            "iter 207: loss 2.8916, time 31300.63ms, mfu 4.04%\n",
            "iter 208: loss 2.8385, time 31321.18ms, mfu 4.04%\n",
            "iter 209: loss 2.9934, time 31317.19ms, mfu 4.04%\n",
            "iter 210: loss 2.8834, time 31414.56ms, mfu 4.04%\n",
            "iter 211: loss 2.8969, time 31447.02ms, mfu 4.05%\n",
            "iter 212: loss 2.9224, time 31478.04ms, mfu 4.05%\n",
            "iter 213: loss 3.0015, time 31521.42ms, mfu 4.04%\n",
            "iter 214: loss 2.9259, time 31520.53ms, mfu 4.04%\n",
            "iter 215: loss 2.9143, time 31496.70ms, mfu 4.04%\n",
            "iter 216: loss 2.8812, time 31443.80ms, mfu 4.04%\n",
            "iter 217: loss 2.8985, time 31420.98ms, mfu 4.05%\n",
            "iter 218: loss 2.9008, time 31309.13ms, mfu 4.05%\n",
            "iter 219: loss 2.7346, time 31257.33ms, mfu 4.05%\n",
            "iter 220: loss 2.8737, time 31311.94ms, mfu 4.05%\n",
            "iter 221: loss 3.0404, time 31316.90ms, mfu 4.05%\n",
            "iter 222: loss 2.9300, time 31371.50ms, mfu 4.05%\n",
            "iter 223: loss 2.9888, time 31461.75ms, mfu 4.05%\n",
            "iter 224: loss 2.8806, time 31493.45ms, mfu 4.05%\n",
            "iter 225: loss 2.6941, time 31508.62ms, mfu 4.05%\n",
            "iter 226: loss 2.8435, time 31501.59ms, mfu 4.05%\n",
            "iter 227: loss 3.0278, time 31455.62ms, mfu 4.05%\n",
            "iter 228: loss 2.8784, time 31387.43ms, mfu 4.05%\n",
            "iter 229: loss 2.9515, time 31373.57ms, mfu 4.05%\n",
            "iter 230: loss 2.7204, time 31368.94ms, mfu 4.05%\n",
            "iter 231: loss 2.9079, time 31294.96ms, mfu 4.05%\n",
            "iter 232: loss 2.8242, time 31284.74ms, mfu 4.06%\n",
            "iter 233: loss 2.9564, time 31416.47ms, mfu 4.06%\n",
            "iter 234: loss 2.9257, time 31515.41ms, mfu 4.05%\n",
            "iter 235: loss 2.8651, time 31571.02ms, mfu 4.05%\n",
            "iter 236: loss 2.8515, time 31413.96ms, mfu 4.05%\n",
            "iter 237: loss 2.8421, time 31288.56ms, mfu 4.05%\n",
            "iter 238: loss 2.9278, time 31319.50ms, mfu 4.05%\n",
            "iter 239: loss 2.8525, time 31341.01ms, mfu 4.06%\n",
            "iter 240: loss 2.9004, time 31328.89ms, mfu 4.06%\n",
            "iter 241: loss 2.8058, time 31236.96ms, mfu 4.06%\n",
            "iter 242: loss 3.0678, time 31412.82ms, mfu 4.06%\n",
            "iter 243: loss 2.8487, time 31546.94ms, mfu 4.06%\n",
            "iter 244: loss 2.8853, time 31569.14ms, mfu 4.05%\n",
            "iter 245: loss 2.8442, time 31421.30ms, mfu 4.05%\n",
            "iter 246: loss 2.6844, time 31346.90ms, mfu 4.05%\n",
            "iter 247: loss 2.8143, time 31385.40ms, mfu 4.05%\n",
            "iter 248: loss 2.8053, time 31393.10ms, mfu 4.05%\n",
            "iter 249: loss 2.9300, time 31374.62ms, mfu 4.05%\n",
            "iter 250: loss 2.8233, time 31354.43ms, mfu 4.06%\n",
            "iter 251: loss 2.7856, time 31332.48ms, mfu 4.06%\n",
            "iter 252: loss 2.8559, time 31266.67ms, mfu 4.06%\n",
            "iter 253: loss 2.9195, time 31288.65ms, mfu 4.06%\n",
            "iter 254: loss 2.9688, time 31299.11ms, mfu 4.06%\n",
            "iter 255: loss 2.8264, time 31348.80ms, mfu 4.06%\n",
            "iter 256: loss 2.7888, time 31374.54ms, mfu 4.06%\n",
            "iter 257: loss 2.9918, time 31350.52ms, mfu 4.06%\n",
            "iter 258: loss 2.9569, time 31379.24ms, mfu 4.06%\n",
            "iter 259: loss 2.6978, time 31376.22ms, mfu 4.06%\n",
            "iter 260: loss 2.8096, time 31392.38ms, mfu 4.06%\n",
            "iter 261: loss 2.8862, time 31300.32ms, mfu 4.06%\n",
            "iter 262: loss 2.8240, time 31401.55ms, mfu 4.06%\n",
            "iter 263: loss 2.8480, time 31491.71ms, mfu 4.06%\n",
            "iter 264: loss 2.8769, time 31485.10ms, mfu 4.06%\n",
            "iter 265: loss 2.9010, time 31394.55ms, mfu 4.06%\n",
            "iter 266: loss 2.8444, time 31384.81ms, mfu 4.06%\n",
            "iter 267: loss 2.8828, time 31414.55ms, mfu 4.06%\n",
            "iter 268: loss 2.9127, time 31402.55ms, mfu 4.06%\n",
            "iter 269: loss 2.8887, time 31376.82ms, mfu 4.06%\n",
            "iter 270: loss 2.8682, time 31389.66ms, mfu 4.06%\n",
            "iter 271: loss 2.8843, time 31357.94ms, mfu 4.06%\n",
            "iter 272: loss 2.8505, time 31364.90ms, mfu 4.06%\n",
            "iter 273: loss 2.8130, time 31323.12ms, mfu 4.06%\n",
            "iter 274: loss 2.7847, time 31307.96ms, mfu 4.06%\n",
            "iter 275: loss 2.8037, time 31247.50ms, mfu 4.06%\n",
            "iter 276: loss 2.7595, time 31391.58ms, mfu 4.06%\n",
            "iter 277: loss 2.8843, time 31579.53ms, mfu 4.06%\n",
            "iter 278: loss 2.9585, time 31462.74ms, mfu 4.06%\n",
            "iter 279: loss 2.9550, time 31381.28ms, mfu 4.06%\n",
            "iter 280: loss 2.8938, time 31344.77ms, mfu 4.06%\n",
            "iter 281: loss 2.8309, time 31331.82ms, mfu 4.06%\n",
            "iter 282: loss 2.7417, time 31280.12ms, mfu 4.06%\n",
            "iter 283: loss 2.9240, time 31321.17ms, mfu 4.06%\n",
            "iter 284: loss 2.9197, time 31313.07ms, mfu 4.06%\n",
            "iter 285: loss 2.8428, time 31239.24ms, mfu 4.06%\n",
            "iter 286: loss 2.7260, time 31229.00ms, mfu 4.06%\n",
            "iter 287: loss 2.8784, time 31381.61ms, mfu 4.06%\n",
            "iter 288: loss 2.8258, time 31456.01ms, mfu 4.06%\n",
            "iter 289: loss 2.8741, time 31526.85ms, mfu 4.06%\n",
            "iter 290: loss 2.9805, time 31483.53ms, mfu 4.06%\n",
            "iter 291: loss 2.7796, time 31432.08ms, mfu 4.06%\n",
            "iter 292: loss 2.9314, time 31418.84ms, mfu 4.06%\n",
            "iter 293: loss 2.7798, time 31415.31ms, mfu 4.06%\n",
            "iter 294: loss 2.7210, time 31417.67ms, mfu 4.06%\n",
            "iter 295: loss 2.7900, time 31329.83ms, mfu 4.06%\n",
            "iter 296: loss 2.8054, time 31347.98ms, mfu 4.06%\n",
            "iter 297: loss 2.6752, time 31330.93ms, mfu 4.06%\n",
            "iter 298: loss 2.7135, time 31316.29ms, mfu 4.06%\n",
            "iter 299: loss 2.9801, time 31386.52ms, mfu 4.06%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"saving checkpoint to {out_dir}\")\n",
        "torch.save({\n",
        "    'model': model.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "    'model_args': model_args,\n",
        "    'iter_num': iter_num,\n",
        "    'best_val_loss': best_val_loss,\n",
        "    'config': config,\n",
        "}, os.path.join(out_dir, 'ckpt_300_pruned_0.20.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhTlp0pB3ZUq",
        "outputId": "d146b361-9ba0-4c83-9203-838963fc21df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving checkpoint to out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.where(~model.transformer['h'][2].attn.c_proj.mask[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2mUzwKgNlO6",
        "outputId": "d729c5a6-6491-4d61-e032-fc6c8ccab951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([   1,    2,    8,    9,   11,   13,   14,   23,   24,   26,   27,   29,\n",
              "           30,   31,   34,   36,   40,   41,   42,   43,   44,   45,   47,   49,\n",
              "           52,   55,   57,   59,   62,   65,   66,   69,   71,   72,   74,   75,\n",
              "           76,   78,   80,   81,   83,   85,   87,   89,   90,   92,   96,  104,\n",
              "          105,  106,  107,  109,  110,  111,  113,  114,  115,  117,  118,  120,\n",
              "          122,  123,  124,  125,  126,  129,  131,  132,  133,  135,  136,  138,\n",
              "          144,  148,  149,  151,  163,  172,  175,  178,  182,  186,  193,  195,\n",
              "          197,  202,  204,  205,  206,  210,  211,  214,  215,  216,  241,  242,\n",
              "          246,  247,  248,  252,  254,  257,  260,  263,  266,  267,  271,  274,\n",
              "          281,  283,  288,  289,  297,  299,  300,  302,  303,  312,  318,  320,\n",
              "          322,  327,  329,  333,  334,  336,  351,  361,  363,  368,  369,  383,\n",
              "          391,  393,  394,  395,  397,  398,  403,  405,  406,  408,  410,  411,\n",
              "          412,  413,  417,  424,  429,  434,  439,  444,  451,  460,  461,  462,\n",
              "          464,  467,  468,  470,  473,  478,  479,  480,  482,  484,  486,  490,\n",
              "          491,  493,  494,  495,  497,  499,  503,  504,  505,  507,  515,  518,\n",
              "          519,  521,  530,  534,  537,  538,  547,  548,  549,  554,  555,  563,\n",
              "          567,  570,  575,  579,  580,  581,  583,  587,  592,  593,  596,  600,\n",
              "          601,  605,  606,  607,  610,  612,  614,  615,  624,  630,  632,  633,\n",
              "          634,  637,  641,  646,  647,  650,  653,  667,  668,  672,  675,  677,\n",
              "          680,  700,  703,  704,  706,  707,  708,  709,  711,  716,  718,  719,\n",
              "          720,  721,  727,  728,  729,  731,  732,  733,  734,  737,  738,  740,\n",
              "          744,  745,  746,  747,  749,  752,  753,  758,  761,  762,  766,  767,\n",
              "          768,  770,  771,  773,  778,  779,  790,  797,  808,  810,  812,  817,\n",
              "          820,  822,  828,  833,  835,  838,  839,  842,  843,  844,  846,  851,\n",
              "          858,  859,  860,  861,  864,  865,  866,  874,  876,  882,  884,  885,\n",
              "          886,  891,  896,  898,  899,  906,  907,  908,  910,  912,  913,  917,\n",
              "          920,  922,  923,  925,  931,  932,  942,  949,  952,  957,  960,  967,\n",
              "          980,  995, 1003, 1005, 1009, 1013, 1019, 1022, 1023], device='cuda:0'),)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AL_VgG8PTWnS",
        "outputId": "82b9af17-edbb-45d9-ccd0-f8bd5693a51f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 300: train loss 2.8775, val loss 2.9221\n",
            "saving checkpoint to out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.prune(m=0.0385)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RO84FDrUNto",
        "outputId": "f4435581-961a-4ff4-d88c-485a3974f927"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3025793780012331"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.where(~model.transformer['h'][2].attn.c_proj.mask[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cW7c_gYlVGZH",
        "outputId": "e2ff4d53-341f-443e-f38a-29a3e6516dcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([   0,    1,    2,    6,    8,    9,   11,   12,   13,   14,   17,   23,\n",
              "           24,   25,   26,   27,   29,   30,   31,   33,   34,   36,   40,   41,\n",
              "           42,   43,   44,   45,   47,   49,   50,   52,   53,   54,   55,   56,\n",
              "           57,   59,   62,   63,   64,   65,   66,   69,   70,   71,   72,   74,\n",
              "           75,   76,   78,   80,   81,   83,   85,   86,   87,   89,   90,   91,\n",
              "           92,   95,   96,   97,  100,  101,  103,  104,  105,  106,  107,  109,\n",
              "          110,  111,  113,  114,  115,  117,  118,  120,  122,  123,  124,  125,\n",
              "          126,  129,  131,  132,  133,  134,  135,  136,  138,  143,  144,  146,\n",
              "          148,  149,  151,  163,  166,  170,  171,  172,  175,  178,  180,  182,\n",
              "          184,  186,  187,  191,  193,  195,  197,  198,  200,  201,  202,  204,\n",
              "          205,  206,  210,  211,  213,  214,  215,  216,  217,  229,  235,  236,\n",
              "          238,  241,  242,  243,  246,  247,  248,  252,  253,  254,  257,  258,\n",
              "          260,  263,  264,  266,  267,  268,  271,  274,  277,  281,  283,  286,\n",
              "          288,  289,  292,  297,  299,  300,  302,  303,  304,  312,  315,  318,\n",
              "          320,  322,  326,  327,  329,  333,  334,  336,  340,  347,  348,  351,\n",
              "          355,  358,  359,  361,  362,  363,  365,  368,  369,  370,  376,  377,\n",
              "          381,  383,  387,  391,  393,  394,  395,  397,  398,  403,  404,  405,\n",
              "          406,  407,  408,  410,  411,  412,  413,  416,  417,  420,  424,  429,\n",
              "          433,  434,  436,  439,  444,  448,  451,  454,  460,  461,  462,  464,\n",
              "          467,  468,  470,  473,  477,  478,  479,  480,  482,  484,  485,  486,\n",
              "          490,  491,  493,  494,  495,  496,  497,  498,  499,  500,  503,  504,\n",
              "          505,  507,  509,  515,  518,  519,  520,  521,  530,  534,  535,  537,\n",
              "          538,  540,  542,  544,  547,  548,  549,  552,  553,  554,  555,  560,\n",
              "          562,  563,  567,  570,  571,  573,  574,  575,  576,  579,  580,  581,\n",
              "          583,  587,  591,  592,  593,  596,  600,  601,  605,  606,  607,  610,\n",
              "          612,  614,  615,  617,  624,  630,  631,  632,  633,  634,  636,  637,\n",
              "          638,  641,  646,  647,  649,  650,  653,  658,  661,  667,  668,  672,\n",
              "          673,  675,  677,  679,  680,  690,  697,  700,  703,  704,  706,  707,\n",
              "          708,  709,  711,  712,  713,  714,  715,  716,  718,  719,  720,  721,\n",
              "          723,  724,  727,  728,  729,  731,  732,  733,  734,  737,  738,  739,\n",
              "          740,  741,  743,  744,  745,  746,  747,  749,  752,  753,  758,  759,\n",
              "          761,  762,  763,  765,  766,  767,  768,  769,  770,  771,  773,  778,\n",
              "          779,  782,  790,  797,  798,  802,  808,  809,  810,  812,  817,  820,\n",
              "          822,  825,  828,  831,  832,  833,  835,  838,  839,  840,  841,  842,\n",
              "          843,  844,  846,  850,  851,  852,  853,  857,  858,  859,  860,  861,\n",
              "          863,  864,  865,  866,  874,  876,  877,  881,  882,  883,  884,  885,\n",
              "          886,  891,  893,  896,  897,  898,  899,  901,  903,  904,  906,  907,\n",
              "          908,  910,  912,  913,  914,  916,  917,  919,  920,  921,  922,  923,\n",
              "          924,  925,  930,  931,  932,  939,  942,  943,  944,  949,  952,  953,\n",
              "          957,  958,  960,  965,  967,  977,  979,  980,  990,  991,  993,  995,\n",
              "         1003, 1005, 1007, 1009, 1013, 1015, 1019, 1021, 1022, 1023],\n",
              "        device='cuda:0'),)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcJkLP0pVKke",
        "outputId": "665f5606-480c-417b-f33e-6d7477d889eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 300: train loss 2.9700, val loss 3.0600\n",
            "iter 300: loss 3.0705, time 136722.15ms, mfu -100.00%\n",
            "iter 301: loss 2.8066, time 31074.61ms, mfu -100.00%\n",
            "iter 302: loss 2.9218, time 31145.96ms, mfu -100.00%\n",
            "iter 303: loss 3.0061, time 31397.18ms, mfu -100.00%\n",
            "iter 304: loss 2.9983, time 31248.36ms, mfu -100.00%\n",
            "iter 305: loss 2.9643, time 31098.34ms, mfu 4.09%\n",
            "iter 306: loss 2.8019, time 31065.81ms, mfu 4.09%\n",
            "iter 307: loss 2.6629, time 31057.56ms, mfu 4.10%\n",
            "iter 308: loss 2.8768, time 31195.51ms, mfu 4.09%\n",
            "iter 309: loss 2.8451, time 31230.92ms, mfu 4.09%\n",
            "iter 310: loss 2.9171, time 31225.30ms, mfu 4.09%\n",
            "iter 311: loss 2.8218, time 31206.47ms, mfu 4.09%\n",
            "iter 312: loss 2.7488, time 31154.20ms, mfu 4.09%\n",
            "iter 313: loss 2.9174, time 31145.44ms, mfu 4.09%\n",
            "iter 314: loss 2.8661, time 31177.37ms, mfu 4.09%\n",
            "iter 315: loss 2.7546, time 31179.94ms, mfu 4.09%\n",
            "iter 316: loss 2.8442, time 31100.56ms, mfu 4.09%\n",
            "iter 317: loss 2.7941, time 31260.97ms, mfu 4.09%\n",
            "iter 318: loss 2.9990, time 31173.99ms, mfu 4.09%\n",
            "iter 319: loss 2.8042, time 31251.40ms, mfu 4.09%\n",
            "iter 320: loss 2.7239, time 31361.80ms, mfu 4.08%\n",
            "iter 321: loss 2.8460, time 31092.56ms, mfu 4.08%\n",
            "iter 322: loss 2.7754, time 31166.87ms, mfu 4.08%\n",
            "iter 323: loss 2.7329, time 31307.54ms, mfu 4.08%\n",
            "iter 324: loss 3.0165, time 31365.91ms, mfu 4.08%\n",
            "iter 325: loss 2.8793, time 31398.74ms, mfu 4.08%\n",
            "iter 326: loss 2.8550, time 31330.02ms, mfu 4.08%\n",
            "iter 327: loss 2.9310, time 31229.60ms, mfu 4.08%\n",
            "iter 328: loss 2.6039, time 31187.24ms, mfu 4.08%\n",
            "iter 329: loss 2.7049, time 31189.13ms, mfu 4.08%\n",
            "iter 330: loss 2.7953, time 31215.79ms, mfu 4.08%\n",
            "iter 331: loss 3.0017, time 31142.13ms, mfu 4.08%\n",
            "iter 332: loss 2.8046, time 31221.05ms, mfu 4.08%\n",
            "iter 333: loss 2.8114, time 31401.86ms, mfu 4.08%\n",
            "iter 334: loss 2.9713, time 31306.16ms, mfu 4.08%\n",
            "iter 335: loss 2.7461, time 31200.19ms, mfu 4.08%\n",
            "iter 336: loss 2.8085, time 31117.10ms, mfu 4.08%\n",
            "iter 337: loss 2.7986, time 31128.84ms, mfu 4.08%\n",
            "iter 338: loss 2.7205, time 31267.81ms, mfu 4.08%\n",
            "iter 339: loss 2.8452, time 31327.29ms, mfu 4.08%\n",
            "iter 340: loss 2.7066, time 31367.15ms, mfu 4.07%\n",
            "iter 341: loss 2.9357, time 31315.52ms, mfu 4.07%\n",
            "iter 342: loss 2.6484, time 31164.61ms, mfu 4.08%\n",
            "iter 343: loss 2.8665, time 31123.35ms, mfu 4.08%\n",
            "iter 344: loss 2.7835, time 31332.49ms, mfu 4.08%\n",
            "iter 345: loss 2.7754, time 31287.60ms, mfu 4.07%\n",
            "iter 346: loss 2.7938, time 31171.16ms, mfu 4.08%\n",
            "iter 347: loss 2.8021, time 31169.41ms, mfu 4.08%\n",
            "iter 348: loss 2.8019, time 31097.74ms, mfu 4.08%\n",
            "iter 349: loss 2.7641, time 31111.73ms, mfu 4.08%\n",
            "iter 350: loss 2.7409, time 31223.11ms, mfu 4.08%\n",
            "iter 351: loss 2.9068, time 31234.47ms, mfu 4.08%\n",
            "iter 352: loss 2.6878, time 31301.04ms, mfu 4.08%\n",
            "iter 353: loss 2.8682, time 31342.98ms, mfu 4.08%\n",
            "iter 354: loss 2.8619, time 31369.72ms, mfu 4.07%\n",
            "iter 355: loss 2.8144, time 31320.42ms, mfu 4.07%\n",
            "iter 356: loss 2.5997, time 31069.91ms, mfu 4.08%\n",
            "iter 357: loss 2.6407, time 31090.27ms, mfu 4.08%\n",
            "iter 358: loss 2.8685, time 31145.35ms, mfu 4.08%\n",
            "iter 359: loss 2.7469, time 31297.79ms, mfu 4.08%\n",
            "iter 360: loss 2.8292, time 31322.57ms, mfu 4.08%\n",
            "iter 361: loss 2.8469, time 31324.18ms, mfu 4.08%\n",
            "iter 362: loss 2.8051, time 31407.01ms, mfu 4.07%\n",
            "iter 363: loss 2.7635, time 31382.44ms, mfu 4.07%\n",
            "iter 364: loss 2.7583, time 31270.86ms, mfu 4.07%\n",
            "iter 365: loss 2.8031, time 31218.99ms, mfu 4.07%\n",
            "iter 366: loss 2.8019, time 31162.96ms, mfu 4.07%\n",
            "iter 367: loss 2.7872, time 31147.62ms, mfu 4.08%\n",
            "iter 368: loss 2.7591, time 31135.61ms, mfu 4.08%\n",
            "iter 369: loss 2.6723, time 31159.16ms, mfu 4.08%\n",
            "iter 370: loss 2.8470, time 31144.76ms, mfu 4.08%\n",
            "iter 371: loss 2.8270, time 31129.49ms, mfu 4.08%\n",
            "iter 372: loss 2.7580, time 31143.81ms, mfu 4.08%\n",
            "iter 373: loss 2.7699, time 31203.45ms, mfu 4.08%\n",
            "iter 374: loss 2.8078, time 31285.00ms, mfu 4.08%\n",
            "iter 375: loss 2.9147, time 31317.66ms, mfu 4.08%\n",
            "iter 376: loss 2.8467, time 31400.27ms, mfu 4.08%\n",
            "iter 377: loss 2.7943, time 31302.00ms, mfu 4.07%\n",
            "iter 378: loss 2.7309, time 31171.58ms, mfu 4.08%\n",
            "iter 379: loss 2.8739, time 31121.29ms, mfu 4.08%\n",
            "iter 380: loss 2.8027, time 31112.58ms, mfu 4.08%\n",
            "iter 381: loss 2.7788, time 31106.32ms, mfu 4.08%\n",
            "iter 382: loss 2.9111, time 31164.59ms, mfu 4.08%\n",
            "iter 383: loss 2.7468, time 31233.41ms, mfu 4.08%\n",
            "iter 384: loss 2.7235, time 31316.46ms, mfu 4.08%\n",
            "iter 385: loss 2.7629, time 31377.17ms, mfu 4.08%\n",
            "iter 386: loss 2.8689, time 31306.19ms, mfu 4.08%\n",
            "iter 387: loss 2.7507, time 31278.48ms, mfu 4.08%\n",
            "iter 388: loss 2.8766, time 31227.11ms, mfu 4.08%\n",
            "iter 389: loss 2.7985, time 31202.64ms, mfu 4.08%\n",
            "iter 390: loss 2.7796, time 31151.03ms, mfu 4.08%\n",
            "iter 391: loss 2.8503, time 31193.85ms, mfu 4.08%\n",
            "iter 392: loss 2.7783, time 31156.18ms, mfu 4.08%\n",
            "iter 393: loss 3.0146, time 31278.55ms, mfu 4.08%\n",
            "iter 394: loss 2.8133, time 31313.44ms, mfu 4.08%\n",
            "iter 395: loss 3.0070, time 31308.11ms, mfu 4.08%\n",
            "iter 396: loss 2.9459, time 31348.95ms, mfu 4.07%\n",
            "iter 397: loss 2.7717, time 31322.04ms, mfu 4.07%\n",
            "iter 398: loss 2.8463, time 31258.43ms, mfu 4.07%\n",
            "iter 399: loss 2.9195, time 31281.37ms, mfu 4.07%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for module in model.modules():\n",
        "    if isinstance(module, PrunableLinear):\n",
        "      module.weight *= module.mask"
      ],
      "metadata": {
        "id": "wj9BL_UpVSG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"saving checkpoint to {out_dir}\")\n",
        "torch.save({\n",
        "    'model': model.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "    'model_args': model_args,\n",
        "    'iter_num': iter_num,\n",
        "    'best_val_loss': best_val_loss,\n",
        "    'config': config,\n",
        "}, os.path.join(out_dir, 'ckpt_400_pruned_0.30.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4vg-8eHXzw9",
        "outputId": "f9b77473-9a08-4737-e6e3-a7de58b43898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving checkpoint to out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.where(model.transformer['h'][2].attn.c_proj.weight[1] == 0.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlMZm28Kh4Qf",
        "outputId": "0800633d-e425-4a30-b6f1-d78d54fb7ec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([   0,    1,    2,    6,    8,    9,   11,   12,   13,   14,   17,   23,\n",
              "           24,   25,   26,   27,   29,   30,   31,   33,   34,   36,   40,   41,\n",
              "           42,   43,   44,   45,   47,   49,   50,   52,   53,   54,   55,   56,\n",
              "           57,   59,   62,   63,   64,   65,   66,   69,   70,   71,   72,   74,\n",
              "           75,   76,   78,   80,   81,   83,   85,   86,   87,   89,   90,   91,\n",
              "           92,   95,   96,   97,  100,  101,  103,  104,  105,  106,  107,  109,\n",
              "          110,  111,  113,  114,  115,  117,  118,  120,  122,  123,  124,  125,\n",
              "          126,  129,  131,  132,  133,  134,  135,  136,  138,  143,  144,  146,\n",
              "          148,  149,  151,  163,  166,  170,  171,  172,  175,  178,  180,  182,\n",
              "          184,  186,  187,  191,  193,  195,  197,  198,  200,  201,  202,  204,\n",
              "          205,  206,  210,  211,  213,  214,  215,  216,  217,  229,  235,  236,\n",
              "          238,  241,  242,  243,  246,  247,  248,  252,  253,  254,  257,  258,\n",
              "          260,  263,  264,  266,  267,  268,  271,  274,  277,  281,  283,  286,\n",
              "          288,  289,  292,  297,  299,  300,  302,  303,  304,  312,  315,  318,\n",
              "          320,  322,  326,  327,  329,  333,  334,  336,  340,  347,  348,  351,\n",
              "          355,  358,  359,  361,  362,  363,  365,  368,  369,  370,  376,  377,\n",
              "          381,  383,  387,  391,  393,  394,  395,  397,  398,  403,  404,  405,\n",
              "          406,  407,  408,  410,  411,  412,  413,  416,  417,  420,  424,  429,\n",
              "          433,  434,  436,  439,  444,  448,  451,  454,  460,  461,  462,  464,\n",
              "          467,  468,  470,  473,  477,  478,  479,  480,  482,  484,  485,  486,\n",
              "          490,  491,  493,  494,  495,  496,  497,  498,  499,  500,  503,  504,\n",
              "          505,  507,  509,  515,  518,  519,  520,  521,  530,  534,  535,  537,\n",
              "          538,  540,  542,  544,  547,  548,  549,  552,  553,  554,  555,  560,\n",
              "          562,  563,  567,  570,  571,  573,  574,  575,  576,  579,  580,  581,\n",
              "          583,  587,  591,  592,  593,  596,  600,  601,  605,  606,  607,  610,\n",
              "          612,  614,  615,  617,  624,  630,  631,  632,  633,  634,  636,  637,\n",
              "          638,  641,  646,  647,  649,  650,  653,  658,  661,  667,  668,  672,\n",
              "          673,  675,  677,  679,  680,  690,  697,  700,  703,  704,  706,  707,\n",
              "          708,  709,  711,  712,  713,  714,  715,  716,  718,  719,  720,  721,\n",
              "          723,  724,  727,  728,  729,  731,  732,  733,  734,  737,  738,  739,\n",
              "          740,  741,  743,  744,  745,  746,  747,  749,  752,  753,  758,  759,\n",
              "          761,  762,  763,  765,  766,  767,  768,  769,  770,  771,  773,  778,\n",
              "          779,  782,  790,  797,  798,  802,  808,  809,  810,  812,  817,  820,\n",
              "          822,  825,  828,  831,  832,  833,  835,  838,  839,  840,  841,  842,\n",
              "          843,  844,  846,  850,  851,  852,  853,  857,  858,  859,  860,  861,\n",
              "          863,  864,  865,  866,  874,  876,  877,  881,  882,  883,  884,  885,\n",
              "          886,  891,  893,  896,  897,  898,  899,  901,  903,  904,  906,  907,\n",
              "          908,  910,  912,  913,  914,  916,  917,  919,  920,  921,  922,  923,\n",
              "          924,  925,  930,  931,  932,  939,  942,  943,  944,  949,  952,  953,\n",
              "          957,  958,  960,  965,  967,  977,  979,  980,  990,  991,  993,  995,\n",
              "         1003, 1005, 1007, 1009, 1013, 1015, 1019, 1021, 1022, 1023],\n",
              "        device='cuda:0'),)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3V8Xsm6X95u",
        "outputId": "576d02a5-423d-4cc6-f3b2-75bf72654497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: train loss 2.8797, val loss 2.9228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.prune(m=0.055)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwitFWG3iBrc",
        "outputId": "acebbd05-594e-4c24-8dde-c9bdde02c34e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.41900721067045993"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for module in model.modules():\n",
        "    if isinstance(module, PrunableLinear):\n",
        "      module.weight *= module.mask"
      ],
      "metadata": {
        "id": "wuvKa6kwirik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ex3TF5uMi-L9",
        "outputId": "d78f740b-95bc-499b-8d38-9255b0c6d165"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: train loss 3.1741, val loss 3.2333\n",
            "iter 400: loss 3.1238, time 135262.17ms, mfu -100.00%\n",
            "iter 401: loss 3.1533, time 30914.75ms, mfu -100.00%\n",
            "iter 402: loss 3.1931, time 30965.67ms, mfu -100.00%\n",
            "iter 403: loss 3.0026, time 31072.06ms, mfu -100.00%\n",
            "iter 404: loss 3.0646, time 31042.03ms, mfu -100.00%\n",
            "iter 405: loss 2.9705, time 31105.72ms, mfu 4.09%\n",
            "iter 406: loss 2.9675, time 31059.01ms, mfu 4.09%\n",
            "iter 407: loss 3.1748, time 31050.29ms, mfu 4.09%\n",
            "iter 408: loss 2.8497, time 30968.47ms, mfu 4.10%\n",
            "iter 409: loss 2.9522, time 31007.30ms, mfu 4.10%\n",
            "iter 410: loss 3.0652, time 31036.07ms, mfu 4.10%\n",
            "iter 411: loss 3.0838, time 31105.06ms, mfu 4.10%\n",
            "iter 412: loss 3.0813, time 31143.65ms, mfu 4.10%\n",
            "iter 413: loss 3.1053, time 31089.94ms, mfu 4.10%\n",
            "iter 414: loss 3.0127, time 31063.78ms, mfu 4.10%\n",
            "iter 415: loss 3.1330, time 30994.39ms, mfu 4.10%\n",
            "iter 416: loss 2.9300, time 30954.18ms, mfu 4.10%\n",
            "iter 417: loss 2.8523, time 30911.01ms, mfu 4.10%\n",
            "iter 418: loss 3.0828, time 30956.65ms, mfu 4.10%\n",
            "iter 419: loss 3.0309, time 31035.21ms, mfu 4.10%\n",
            "iter 420: loss 3.0117, time 31087.54ms, mfu 4.10%\n",
            "iter 421: loss 2.8222, time 31067.98ms, mfu 4.10%\n",
            "iter 422: loss 2.8622, time 31113.77ms, mfu 4.10%\n",
            "iter 423: loss 2.9751, time 31054.94ms, mfu 4.10%\n",
            "iter 424: loss 3.0730, time 31037.07ms, mfu 4.10%\n",
            "iter 425: loss 2.9761, time 30995.08ms, mfu 4.10%\n",
            "iter 426: loss 2.9461, time 30952.64ms, mfu 4.10%\n",
            "iter 427: loss 2.9956, time 31014.36ms, mfu 4.10%\n",
            "iter 428: loss 3.0557, time 31039.78ms, mfu 4.10%\n",
            "iter 429: loss 2.8709, time 31091.17ms, mfu 4.10%\n",
            "iter 430: loss 3.0657, time 31035.53ms, mfu 4.10%\n",
            "iter 431: loss 2.9274, time 31082.05ms, mfu 4.10%\n",
            "iter 432: loss 2.8575, time 31086.92ms, mfu 4.10%\n",
            "iter 433: loss 2.9083, time 31066.65ms, mfu 4.10%\n",
            "iter 434: loss 3.0533, time 31032.99ms, mfu 4.10%\n",
            "iter 435: loss 3.0015, time 30971.11ms, mfu 4.10%\n",
            "iter 436: loss 2.9520, time 30925.78ms, mfu 4.10%\n",
            "iter 437: loss 2.8721, time 30926.00ms, mfu 4.10%\n",
            "iter 438: loss 2.8240, time 30946.32ms, mfu 4.11%\n",
            "iter 439: loss 3.0869, time 30944.04ms, mfu 4.11%\n",
            "iter 440: loss 2.8355, time 30936.52ms, mfu 4.11%\n",
            "iter 441: loss 2.9122, time 30873.30ms, mfu 4.11%\n",
            "iter 442: loss 3.0193, time 30881.96ms, mfu 4.11%\n",
            "iter 443: loss 2.8246, time 31009.67ms, mfu 4.11%\n",
            "iter 444: loss 2.9284, time 31079.89ms, mfu 4.11%\n",
            "iter 445: loss 2.9918, time 31054.15ms, mfu 4.11%\n",
            "iter 446: loss 2.9411, time 31089.84ms, mfu 4.11%\n",
            "iter 447: loss 2.8294, time 31068.33ms, mfu 4.11%\n",
            "iter 448: loss 2.9610, time 30986.53ms, mfu 4.11%\n",
            "iter 449: loss 2.7947, time 30969.59ms, mfu 4.11%\n",
            "iter 450: loss 3.0604, time 30940.71ms, mfu 4.11%\n",
            "iter 451: loss 2.9318, time 30876.53ms, mfu 4.11%\n",
            "iter 452: loss 3.0304, time 30992.75ms, mfu 4.11%\n",
            "iter 453: loss 2.9940, time 31087.32ms, mfu 4.11%\n",
            "iter 454: loss 2.7989, time 31058.21ms, mfu 4.11%\n",
            "iter 455: loss 2.9945, time 30967.51ms, mfu 4.11%\n",
            "iter 456: loss 2.8989, time 30866.80ms, mfu 4.11%\n",
            "iter 457: loss 2.9921, time 30919.37ms, mfu 4.11%\n",
            "iter 458: loss 3.0533, time 31158.18ms, mfu 4.11%\n",
            "iter 459: loss 2.8861, time 31020.96ms, mfu 4.11%\n",
            "iter 460: loss 2.9107, time 30881.91ms, mfu 4.11%\n",
            "iter 461: loss 2.8608, time 31040.59ms, mfu 4.11%\n",
            "iter 462: loss 2.8810, time 31113.67ms, mfu 4.11%\n",
            "iter 463: loss 3.0716, time 31063.21ms, mfu 4.11%\n",
            "iter 464: loss 2.9725, time 31141.71ms, mfu 4.10%\n",
            "iter 465: loss 2.8774, time 31090.92ms, mfu 4.10%\n",
            "iter 466: loss 2.9888, time 31014.55ms, mfu 4.10%\n",
            "iter 467: loss 2.9222, time 30955.10ms, mfu 4.10%\n",
            "iter 468: loss 2.9230, time 30983.54ms, mfu 4.10%\n",
            "iter 469: loss 3.0772, time 31038.92ms, mfu 4.10%\n",
            "iter 470: loss 2.9829, time 31081.44ms, mfu 4.10%\n",
            "iter 471: loss 2.7620, time 31049.47ms, mfu 4.10%\n",
            "iter 472: loss 2.9391, time 31069.88ms, mfu 4.10%\n",
            "iter 473: loss 2.9947, time 31056.26ms, mfu 4.10%\n",
            "iter 474: loss 2.8188, time 31063.94ms, mfu 4.10%\n",
            "iter 475: loss 2.7476, time 31059.58ms, mfu 4.10%\n",
            "iter 476: loss 2.9159, time 31065.83ms, mfu 4.10%\n",
            "iter 477: loss 2.8640, time 30932.12ms, mfu 4.10%\n",
            "iter 478: loss 2.9411, time 30912.41ms, mfu 4.10%\n",
            "iter 479: loss 2.9513, time 30852.09ms, mfu 4.11%\n",
            "iter 480: loss 2.9102, time 30977.63ms, mfu 4.11%\n",
            "iter 481: loss 2.8359, time 31126.53ms, mfu 4.11%\n",
            "iter 482: loss 2.9017, time 31053.42ms, mfu 4.11%\n",
            "iter 483: loss 2.8816, time 30991.85ms, mfu 4.11%\n",
            "iter 484: loss 2.7548, time 30965.83ms, mfu 4.11%\n",
            "iter 485: loss 2.8541, time 30853.66ms, mfu 4.11%\n",
            "iter 486: loss 2.8852, time 31009.21ms, mfu 4.11%\n",
            "iter 487: loss 3.0162, time 31035.02ms, mfu 4.11%\n",
            "iter 488: loss 2.9134, time 31050.55ms, mfu 4.11%\n",
            "iter 489: loss 2.8925, time 31065.54ms, mfu 4.11%\n",
            "iter 490: loss 2.8689, time 31044.92ms, mfu 4.11%\n",
            "iter 491: loss 3.1039, time 31057.45ms, mfu 4.10%\n",
            "iter 492: loss 2.7702, time 31064.41ms, mfu 4.10%\n",
            "iter 493: loss 2.8109, time 31063.02ms, mfu 4.10%\n",
            "iter 494: loss 2.7476, time 31086.95ms, mfu 4.10%\n",
            "iter 495: loss 3.0382, time 31056.76ms, mfu 4.10%\n",
            "iter 496: loss 2.9780, time 31072.64ms, mfu 4.10%\n",
            "iter 497: loss 2.8297, time 31049.81ms, mfu 4.10%\n",
            "iter 498: loss 2.9712, time 31076.85ms, mfu 4.10%\n",
            "iter 499: loss 2.9996, time 31057.81ms, mfu 4.10%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.where(torch.abs(model.transformer['h'][2].attn.c_proj.weight[1]) < 1e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Me2fTQXdjAqf",
        "outputId": "8cbc7a1b-f2eb-4a9f-c178-0dfa999c8dfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([   0,    1,    2,    4,    6,    8,    9,   10,   11,   12,   13,   14,\n",
              "           15,   16,   17,   19,   20,   21,   22,   23,   24,   25,   26,   27,\n",
              "           28,   29,   30,   31,   32,   33,   34,   35,   36,   37,   38,   39,\n",
              "           40,   41,   42,   43,   44,   45,   46,   47,   49,   50,   51,   52,\n",
              "           53,   54,   55,   56,   57,   59,   62,   63,   64,   65,   66,   67,\n",
              "           69,   70,   71,   72,   74,   75,   76,   78,   80,   81,   83,   84,\n",
              "           85,   86,   87,   89,   90,   91,   92,   93,   95,   96,   97,   98,\n",
              "          100,  101,  103,  104,  105,  106,  107,  108,  109,  110,  111,  113,\n",
              "          114,  115,  116,  117,  118,  120,  121,  122,  123,  124,  125,  126,\n",
              "          129,  131,  132,  133,  134,  135,  136,  137,  138,  143,  144,  146,\n",
              "          148,  149,  151,  154,  158,  159,  163,  164,  165,  166,  169,  170,\n",
              "          171,  172,  175,  176,  178,  180,  182,  184,  186,  187,  188,  191,\n",
              "          193,  195,  197,  198,  199,  200,  201,  202,  204,  205,  206,  210,\n",
              "          211,  213,  214,  215,  216,  217,  229,  231,  232,  235,  236,  238,\n",
              "          240,  241,  242,  243,  245,  246,  247,  248,  250,  252,  253,  254,\n",
              "          257,  258,  260,  263,  264,  266,  267,  268,  271,  272,  274,  276,\n",
              "          277,  281,  283,  286,  287,  288,  289,  291,  292,  296,  297,  299,\n",
              "          300,  302,  303,  304,  306,  312,  313,  315,  316,  317,  318,  319,\n",
              "          320,  322,  323,  326,  327,  329,  333,  334,  336,  340,  346,  347,\n",
              "          348,  351,  354,  355,  358,  359,  361,  362,  363,  365,  368,  369,\n",
              "          370,  376,  377,  378,  381,  383,  385,  387,  391,  393,  394,  395,\n",
              "          397,  398,  401,  403,  404,  405,  406,  407,  408,  410,  411,  412,\n",
              "          413,  415,  416,  417,  420,  422,  424,  425,  429,  431,  433,  434,\n",
              "          436,  439,  444,  446,  448,  451,  452,  454,  456,  457,  458,  460,\n",
              "          461,  462,  463,  464,  465,  466,  467,  468,  469,  470,  473,  474,\n",
              "          475,  477,  478,  479,  480,  481,  482,  484,  485,  486,  489,  490,\n",
              "          491,  493,  494,  495,  496,  497,  498,  499,  500,  501,  503,  504,\n",
              "          505,  507,  509,  511,  514,  515,  518,  519,  520,  521,  524,  529,\n",
              "          530,  533,  534,  535,  537,  538,  539,  540,  542,  544,  547,  548,\n",
              "          549,  552,  553,  554,  555,  559,  560,  562,  563,  567,  570,  571,\n",
              "          573,  574,  575,  576,  579,  580,  581,  583,  584,  587,  591,  592,\n",
              "          593,  596,  597,  600,  601,  602,  605,  606,  607,  608,  609,  610,\n",
              "          612,  614,  615,  617,  624,  625,  628,  629,  630,  631,  632,  633,\n",
              "          634,  636,  637,  638,  640,  641,  644,  646,  647,  649,  650,  653,\n",
              "          658,  661,  662,  667,  668,  670,  672,  673,  675,  677,  679,  680,\n",
              "          683,  684,  688,  690,  697,  700,  701,  703,  704,  705,  706,  707,\n",
              "          708,  709,  711,  712,  713,  714,  715,  716,  717,  718,  719,  720,\n",
              "          721,  722,  723,  724,  727,  728,  729,  730,  731,  732,  733,  734,\n",
              "          737,  738,  739,  740,  741,  743,  744,  745,  746,  747,  749,  751,\n",
              "          752,  753,  754,  757,  758,  759,  760,  761,  762,  763,  765,  766,\n",
              "          767,  768,  769,  770,  771,  773,  774,  778,  779,  780,  782,  784,\n",
              "          789,  790,  796,  797,  798,  802,  808,  809,  810,  812,  816,  817,\n",
              "          818,  820,  822,  825,  828,  829,  831,  832,  833,  835,  838,  839,\n",
              "          840,  841,  842,  843,  844,  846,  847,  850,  851,  852,  853,  855,\n",
              "          857,  858,  859,  860,  861,  863,  864,  865,  866,  867,  868,  870,\n",
              "          872,  873,  874,  875,  876,  877,  878,  879,  881,  882,  883,  884,\n",
              "          885,  886,  887,  890,  891,  893,  894,  896,  897,  898,  899,  901,\n",
              "          903,  904,  906,  907,  908,  910,  911,  912,  913,  914,  916,  917,\n",
              "          918,  919,  920,  921,  922,  923,  924,  925,  927,  928,  929,  930,\n",
              "          931,  932,  935,  938,  939,  942,  943,  944,  947,  949,  950,  952,\n",
              "          953,  957,  958,  960,  962,  963,  965,  966,  967,  973,  977,  978,\n",
              "          979,  980,  983,  985,  986,  989,  990,  991,  993,  994,  995,  996,\n",
              "          998, 1001, 1003, 1005, 1006, 1007, 1008, 1009, 1011, 1012, 1013, 1015,\n",
              "         1019, 1021, 1022, 1023], device='cuda:0'),)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsdNi35-pH9v",
        "outputId": "78141427-7bfd-45ee-a6b0-567331fca0d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 500: train loss 2.9011, val loss 2.9731\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for module in model.modules():\n",
        "    if isinstance(module, PrunableLinear):\n",
        "      module.weight *= module.mask"
      ],
      "metadata": {
        "id": "7SLcWkoKuvMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03i174a0vwuy",
        "outputId": "b84c796b-3ceb-4bb8-cc2e-33c552dbf8a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 500: train loss 2.9045, val loss 2.9726\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"saving checkpoint to {out_dir}\")\n",
        "torch.save({\n",
        "    'model': model.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "    'model_args': model_args,\n",
        "    'iter_num': iter_num,\n",
        "    'best_val_loss': best_val_loss,\n",
        "    'config': config,\n",
        "}, os.path.join(out_dir, 'ckpt_500_pruned_0.42.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUrY4JFvvxtd",
        "outputId": "c4a47d89-d872-49eb-9274-86f92fa29231"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving checkpoint to out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.prune(m=0.068)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYU_r7HUv5AI",
        "outputId": "72c5ac69-19b9-46eb-e70e-9c7a3bafa5c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5028553322450019"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for module in model.modules():\n",
        "    if isinstance(module, PrunableLinear):\n",
        "      module.weight *= module.mask"
      ],
      "metadata": {
        "id": "OZCVChkuymOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kr-OSOaSwo_i",
        "outputId": "a0004f1e-1b8d-4591-9f96-32babb0c54d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 500: train loss 3.2638, val loss 3.3162\n",
            "iter 500: loss 3.2477, time 132647.88ms, mfu -100.00%\n",
            "iter 501: loss 3.1438, time 30853.46ms, mfu -100.00%\n",
            "iter 502: loss 3.1076, time 30571.51ms, mfu -100.00%\n",
            "iter 503: loss 3.0438, time 30600.49ms, mfu -100.00%\n",
            "iter 504: loss 3.0423, time 30792.53ms, mfu -100.00%\n",
            "iter 505: loss 3.0970, time 30822.93ms, mfu 4.13%\n",
            "iter 506: loss 3.1580, time 30886.38ms, mfu 4.13%\n",
            "iter 507: loss 3.0724, time 30856.76ms, mfu 4.13%\n",
            "iter 508: loss 3.0667, time 30817.67ms, mfu 4.13%\n",
            "iter 509: loss 3.1683, time 30746.05ms, mfu 4.13%\n",
            "iter 510: loss 3.0291, time 30764.77ms, mfu 4.13%\n",
            "iter 511: loss 3.0810, time 30697.59ms, mfu 4.13%\n",
            "iter 512: loss 3.0712, time 30701.52ms, mfu 4.13%\n",
            "iter 513: loss 3.1551, time 30681.75ms, mfu 4.14%\n",
            "iter 514: loss 3.0471, time 30693.77ms, mfu 4.14%\n",
            "iter 515: loss 3.0497, time 30658.60ms, mfu 4.14%\n",
            "iter 516: loss 2.9981, time 30815.83ms, mfu 4.14%\n",
            "iter 517: loss 3.0083, time 30804.96ms, mfu 4.14%\n",
            "iter 518: loss 3.0388, time 30867.43ms, mfu 4.14%\n",
            "iter 519: loss 2.8280, time 30757.97ms, mfu 4.14%\n",
            "iter 520: loss 2.9639, time 30791.16ms, mfu 4.14%\n",
            "iter 521: loss 3.1530, time 30767.83ms, mfu 4.14%\n",
            "iter 522: loss 3.0551, time 30878.48ms, mfu 4.14%\n",
            "iter 523: loss 3.0624, time 30796.39ms, mfu 4.14%\n",
            "iter 524: loss 2.9401, time 30867.43ms, mfu 4.13%\n",
            "iter 525: loss 2.7177, time 30783.77ms, mfu 4.13%\n",
            "iter 526: loss 2.8990, time 30794.62ms, mfu 4.13%\n",
            "iter 527: loss 3.1241, time 30706.63ms, mfu 4.14%\n",
            "iter 528: loss 2.9685, time 30631.36ms, mfu 4.14%\n",
            "iter 529: loss 3.0508, time 30775.48ms, mfu 4.14%\n",
            "iter 530: loss 2.8046, time 30910.19ms, mfu 4.14%\n",
            "iter 531: loss 3.0101, time 30625.10ms, mfu 4.14%\n",
            "iter 532: loss 2.9043, time 30670.37ms, mfu 4.14%\n",
            "iter 533: loss 3.0166, time 30772.73ms, mfu 4.14%\n",
            "iter 534: loss 3.0085, time 30893.66ms, mfu 4.14%\n",
            "iter 535: loss 2.9583, time 30841.37ms, mfu 4.14%\n",
            "iter 536: loss 2.8887, time 30867.57ms, mfu 4.14%\n",
            "iter 537: loss 2.9040, time 30849.63ms, mfu 4.13%\n",
            "iter 538: loss 2.9815, time 30873.50ms, mfu 4.13%\n",
            "iter 539: loss 2.9434, time 30730.59ms, mfu 4.13%\n",
            "iter 540: loss 2.9585, time 30705.43ms, mfu 4.14%\n",
            "iter 541: loss 2.8656, time 30750.13ms, mfu 4.14%\n",
            "iter 542: loss 3.1231, time 30806.54ms, mfu 4.14%\n",
            "iter 543: loss 2.8962, time 30750.90ms, mfu 4.14%\n",
            "iter 544: loss 2.9398, time 30832.58ms, mfu 4.14%\n",
            "iter 545: loss 2.8986, time 30828.66ms, mfu 4.14%\n",
            "iter 546: loss 2.7141, time 30916.06ms, mfu 4.13%\n",
            "iter 547: loss 2.8764, time 30914.07ms, mfu 4.13%\n",
            "iter 548: loss 2.8571, time 30983.11ms, mfu 4.13%\n",
            "iter 549: loss 3.0048, time 30889.31ms, mfu 4.13%\n",
            "iter 550: loss 2.8674, time 30813.64ms, mfu 4.13%\n",
            "iter 551: loss 2.8386, time 30709.56ms, mfu 4.13%\n",
            "iter 552: loss 2.8917, time 30734.50ms, mfu 4.13%\n",
            "iter 553: loss 2.9977, time 30721.27ms, mfu 4.13%\n",
            "iter 554: loss 3.0230, time 30745.72ms, mfu 4.13%\n",
            "iter 555: loss 2.8693, time 30680.67ms, mfu 4.14%\n",
            "iter 556: loss 2.8359, time 30770.63ms, mfu 4.14%\n",
            "iter 557: loss 3.0337, time 30688.43ms, mfu 4.14%\n",
            "iter 558: loss 2.9922, time 30727.69ms, mfu 4.14%\n",
            "iter 559: loss 2.7203, time 30736.98ms, mfu 4.14%\n",
            "iter 560: loss 2.8277, time 30784.36ms, mfu 4.14%\n",
            "iter 561: loss 2.9593, time 30821.33ms, mfu 4.14%\n",
            "iter 562: loss 2.8890, time 30888.67ms, mfu 4.14%\n",
            "iter 563: loss 2.8969, time 30820.94ms, mfu 4.14%\n",
            "iter 564: loss 2.9186, time 30828.80ms, mfu 4.13%\n",
            "iter 565: loss 2.9513, time 30792.98ms, mfu 4.13%\n",
            "iter 566: loss 2.8800, time 30705.14ms, mfu 4.14%\n",
            "iter 567: loss 2.8992, time 30718.15ms, mfu 4.14%\n",
            "iter 568: loss 2.9986, time 30848.87ms, mfu 4.14%\n",
            "iter 569: loss 2.9191, time 30820.37ms, mfu 4.14%\n",
            "iter 570: loss 2.9066, time 30888.35ms, mfu 4.13%\n",
            "iter 571: loss 2.9555, time 30934.25ms, mfu 4.13%\n",
            "iter 572: loss 2.8823, time 30959.11ms, mfu 4.13%\n",
            "iter 573: loss 2.8359, time 30825.16ms, mfu 4.13%\n",
            "iter 574: loss 2.8207, time 30747.40ms, mfu 4.13%\n",
            "iter 575: loss 2.8609, time 30673.51ms, mfu 4.13%\n",
            "iter 576: loss 2.7814, time 30697.99ms, mfu 4.13%\n",
            "iter 577: loss 2.9186, time 30666.22ms, mfu 4.14%\n",
            "iter 578: loss 3.0089, time 30707.61ms, mfu 4.14%\n",
            "iter 579: loss 2.9676, time 30756.46ms, mfu 4.14%\n",
            "iter 580: loss 2.8992, time 30901.42ms, mfu 4.14%\n",
            "iter 581: loss 2.8296, time 30842.74ms, mfu 4.14%\n",
            "iter 582: loss 2.7260, time 30895.88ms, mfu 4.13%\n",
            "iter 583: loss 2.9588, time 30823.83ms, mfu 4.13%\n",
            "iter 584: loss 2.9718, time 30786.82ms, mfu 4.13%\n",
            "iter 585: loss 2.8668, time 30707.15ms, mfu 4.13%\n",
            "iter 586: loss 2.7356, time 30705.57ms, mfu 4.14%\n",
            "iter 587: loss 2.9334, time 30652.72ms, mfu 4.14%\n",
            "iter 588: loss 2.8435, time 30710.24ms, mfu 4.14%\n",
            "iter 589: loss 2.9279, time 30763.82ms, mfu 4.14%\n",
            "iter 590: loss 3.0254, time 30721.90ms, mfu 4.14%\n",
            "iter 591: loss 2.7748, time 30804.66ms, mfu 4.14%\n",
            "iter 592: loss 2.9653, time 30848.61ms, mfu 4.14%\n",
            "iter 593: loss 2.7741, time 30884.11ms, mfu 4.14%\n",
            "iter 594: loss 2.7309, time 30932.53ms, mfu 4.13%\n",
            "iter 595: loss 2.7878, time 30809.30ms, mfu 4.13%\n",
            "iter 596: loss 2.8371, time 30742.60ms, mfu 4.13%\n",
            "iter 597: loss 2.6804, time 30672.01ms, mfu 4.14%\n",
            "iter 598: loss 2.7529, time 30683.02ms, mfu 4.14%\n",
            "iter 599: loss 2.9490, time 30594.65ms, mfu 4.14%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rslMo7KxNlg",
        "outputId": "417df954-6592-4caa-e114-c6a59025ff92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 600: train loss 2.9010, val loss 3.0341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for module in model.modules():\n",
        "    if isinstance(module, PrunableLinear):\n",
        "      module.weight *= module.mask"
      ],
      "metadata": {
        "id": "Tet5LLtN9MFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3__osm1m9xeo",
        "outputId": "d8786095-8494-445b-dc13-43717703590b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 600: train loss 2.9593, val loss 3.0223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"saving checkpoint to {out_dir}\")\n",
        "torch.save({\n",
        "    'model': model.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "    'model_args': model_args,\n",
        "    'iter_num': iter_num,\n",
        "    'best_val_loss': best_val_loss,\n",
        "    'config': config,\n",
        "}, os.path.join(out_dir, 'ckpt_600_pruned_0.50.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COSG5cLz9yWH",
        "outputId": "fa9fefdb-f996-473e-eb22-0bf564e42a58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving checkpoint to out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.prune(m=0.085)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaynOrwo-Gip",
        "outputId": "3704de32-babd-4e57-997a-0b0853123a71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6007125011397088"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for module in model.modules():\n",
        "    if isinstance(module, PrunableLinear):\n",
        "      module.weight *= module.mask"
      ],
      "metadata": {
        "id": "LOZlPLDu_wfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkMjoXHwAany",
        "outputId": "14bae95b-45d5-4ef0-b518-f8b86f26b503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 600: train loss 4.2862, val loss 4.3404\n",
            "iter 600: loss 4.1979, time 132184.71ms, mfu -100.00%\n",
            "iter 601: loss 4.0288, time 30408.08ms, mfu -100.00%\n",
            "iter 602: loss 3.9112, time 30343.78ms, mfu -100.00%\n",
            "iter 603: loss 3.6945, time 30407.14ms, mfu -100.00%\n",
            "iter 604: loss 3.6111, time 30394.88ms, mfu -100.00%\n",
            "iter 605: loss 3.4950, time 30443.56ms, mfu 4.18%\n",
            "iter 606: loss 3.3665, time 30463.75ms, mfu 4.18%\n",
            "iter 607: loss 3.5673, time 30534.09ms, mfu 4.18%\n",
            "iter 608: loss 3.2651, time 30558.14ms, mfu 4.18%\n",
            "iter 609: loss 3.3153, time 30650.13ms, mfu 4.18%\n",
            "iter 610: loss 3.4048, time 30542.66ms, mfu 4.18%\n",
            "iter 611: loss 3.4150, time 30517.91ms, mfu 4.18%\n",
            "iter 612: loss 3.4047, time 30363.45ms, mfu 4.18%\n",
            "iter 613: loss 3.4133, time 30456.97ms, mfu 4.18%\n",
            "iter 614: loss 3.3169, time 30475.37ms, mfu 4.18%\n",
            "iter 615: loss 3.4294, time 30528.85ms, mfu 4.18%\n",
            "iter 616: loss 3.2098, time 30426.96ms, mfu 4.18%\n",
            "iter 617: loss 3.1102, time 30382.02ms, mfu 4.18%\n",
            "iter 618: loss 3.3461, time 30485.32ms, mfu 4.18%\n",
            "iter 619: loss 3.2867, time 30582.07ms, mfu 4.18%\n",
            "iter 620: loss 3.2780, time 30559.24ms, mfu 4.18%\n",
            "iter 621: loss 3.0530, time 30559.93ms, mfu 4.18%\n",
            "iter 622: loss 3.1342, time 30576.23ms, mfu 4.17%\n",
            "iter 623: loss 3.2019, time 30561.28ms, mfu 4.17%\n",
            "iter 624: loss 3.3316, time 30563.29ms, mfu 4.17%\n",
            "iter 625: loss 3.1927, time 30621.66ms, mfu 4.17%\n",
            "iter 626: loss 3.1856, time 30499.83ms, mfu 4.17%\n",
            "iter 627: loss 3.2470, time 30422.35ms, mfu 4.17%\n",
            "iter 628: loss 3.2839, time 30344.01ms, mfu 4.18%\n",
            "iter 629: loss 3.0485, time 30393.03ms, mfu 4.18%\n",
            "iter 630: loss 3.2688, time 30446.74ms, mfu 4.18%\n",
            "iter 631: loss 3.1465, time 30532.70ms, mfu 4.18%\n",
            "iter 632: loss 3.0734, time 30473.97ms, mfu 4.18%\n",
            "iter 633: loss 3.1132, time 30411.83ms, mfu 4.18%\n",
            "iter 634: loss 3.2601, time 30378.43ms, mfu 4.18%\n",
            "iter 635: loss 3.1994, time 30450.25ms, mfu 4.18%\n",
            "iter 636: loss 3.1579, time 30405.07ms, mfu 4.18%\n",
            "iter 637: loss 3.0917, time 30381.52ms, mfu 4.18%\n",
            "iter 638: loss 3.0618, time 30409.83ms, mfu 4.18%\n",
            "iter 639: loss 3.3039, time 30513.57ms, mfu 4.18%\n",
            "iter 640: loss 3.0304, time 30594.63ms, mfu 4.18%\n",
            "iter 641: loss 3.1336, time 30623.60ms, mfu 4.18%\n",
            "iter 642: loss 3.1951, time 30521.97ms, mfu 4.18%\n",
            "iter 643: loss 3.0089, time 30523.21ms, mfu 4.18%\n",
            "iter 644: loss 3.1182, time 30473.15ms, mfu 4.18%\n",
            "iter 645: loss 3.2061, time 30445.92ms, mfu 4.18%\n",
            "iter 646: loss 3.1467, time 30410.21ms, mfu 4.18%\n",
            "iter 647: loss 3.0022, time 30400.07ms, mfu 4.18%\n",
            "iter 648: loss 3.1427, time 30383.57ms, mfu 4.18%\n",
            "iter 649: loss 2.9995, time 30391.07ms, mfu 4.18%\n",
            "iter 650: loss 3.2520, time 30364.33ms, mfu 4.18%\n",
            "iter 651: loss 3.1065, time 30474.61ms, mfu 4.18%\n",
            "iter 652: loss 3.1879, time 30551.37ms, mfu 4.18%\n",
            "iter 653: loss 3.2045, time 30587.98ms, mfu 4.18%\n",
            "iter 654: loss 2.9902, time 30641.73ms, mfu 4.18%\n",
            "iter 655: loss 3.1523, time 30703.73ms, mfu 4.17%\n",
            "iter 656: loss 3.0541, time 30566.12ms, mfu 4.17%\n",
            "iter 657: loss 3.1449, time 30510.95ms, mfu 4.17%\n",
            "iter 658: loss 3.2226, time 30422.31ms, mfu 4.17%\n",
            "iter 659: loss 3.0346, time 30428.71ms, mfu 4.17%\n",
            "iter 660: loss 3.0759, time 30403.68ms, mfu 4.18%\n",
            "iter 661: loss 3.0155, time 30418.04ms, mfu 4.18%\n",
            "iter 662: loss 3.0337, time 30489.74ms, mfu 4.18%\n",
            "iter 663: loss 3.2424, time 30575.40ms, mfu 4.18%\n",
            "iter 664: loss 3.1691, time 30635.31ms, mfu 4.17%\n",
            "iter 665: loss 3.0439, time 30655.16ms, mfu 4.17%\n",
            "iter 666: loss 3.1695, time 30546.82ms, mfu 4.17%\n",
            "iter 667: loss 3.0665, time 30530.61ms, mfu 4.17%\n",
            "iter 668: loss 3.0888, time 30317.47ms, mfu 4.17%\n",
            "iter 669: loss 3.2625, time 30676.19ms, mfu 4.17%\n",
            "iter 670: loss 3.1119, time 30599.76ms, mfu 4.17%\n",
            "iter 671: loss 2.9277, time 30341.62ms, mfu 4.17%\n",
            "iter 672: loss 3.0842, time 30460.11ms, mfu 4.17%\n",
            "iter 673: loss 3.1548, time 30611.24ms, mfu 4.17%\n",
            "iter 674: loss 2.9551, time 30565.56ms, mfu 4.17%\n",
            "iter 675: loss 2.8815, time 30653.29ms, mfu 4.17%\n",
            "iter 676: loss 3.0431, time 30612.57ms, mfu 4.17%\n",
            "iter 677: loss 3.0224, time 30614.24ms, mfu 4.17%\n",
            "iter 678: loss 3.1078, time 30427.83ms, mfu 4.17%\n",
            "iter 679: loss 3.0830, time 30522.53ms, mfu 4.17%\n",
            "iter 680: loss 3.0399, time 30546.65ms, mfu 4.17%\n",
            "iter 681: loss 3.0296, time 30559.10ms, mfu 4.17%\n",
            "iter 682: loss 3.0839, time 30551.96ms, mfu 4.17%\n",
            "iter 683: loss 3.0016, time 30612.31ms, mfu 4.17%\n",
            "iter 684: loss 2.8640, time 30585.97ms, mfu 4.17%\n",
            "iter 685: loss 3.0175, time 30570.26ms, mfu 4.17%\n",
            "iter 686: loss 3.0045, time 30565.20ms, mfu 4.17%\n",
            "iter 687: loss 3.1808, time 30636.20ms, mfu 4.17%\n",
            "iter 688: loss 3.0449, time 30623.30ms, mfu 4.17%\n",
            "iter 689: loss 3.0499, time 30625.74ms, mfu 4.16%\n",
            "iter 690: loss 3.0207, time 30593.34ms, mfu 4.16%\n",
            "iter 691: loss 3.2621, time 30595.30ms, mfu 4.16%\n",
            "iter 692: loss 2.9031, time 30556.31ms, mfu 4.16%\n",
            "iter 693: loss 2.9744, time 30563.33ms, mfu 4.16%\n",
            "iter 694: loss 2.8838, time 30492.14ms, mfu 4.17%\n",
            "iter 695: loss 3.1771, time 30511.07ms, mfu 4.17%\n",
            "iter 696: loss 3.1099, time 30542.37ms, mfu 4.17%\n",
            "iter 697: loss 2.9248, time 30629.16ms, mfu 4.17%\n",
            "iter 698: loss 3.0978, time 30603.14ms, mfu 4.17%\n",
            "iter 699: loss 3.1067, time 30469.32ms, mfu 4.17%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fso1KcABAg4I",
        "outputId": "1cf63571-99af-41b2-8035-4159d1024266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 700: train loss 3.0812, val loss 3.1637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for module in model.modules():\n",
        "    if isinstance(module, PrunableLinear):\n",
        "      module.weight *= module.mask"
      ],
      "metadata": {
        "id": "A1_E1hLfKh_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZDYIs6kKkdX",
        "outputId": "095550a6-0493-4418-daf6-0854f3519d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 700: train loss 3.0911, val loss 3.1691\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"saving checkpoint to {out_dir}\")\n",
        "torch.save({\n",
        "    'model': model.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "    'model_args': model_args,\n",
        "    'iter_num': iter_num,\n",
        "    'best_val_loss': best_val_loss,\n",
        "    'config': config,\n",
        "}, os.path.join(out_dir, 'ckpt_700_pruned_0.60.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnEUol3IKnSS",
        "outputId": "d99debf3-5246-435c-bc08-bea77cd86840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving checkpoint to out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.prune(m=0.11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUKgk3EyKzN1",
        "outputId": "beb6aa04-d9b3-4cea-983a-f312ad0303a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7190681658676042"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for module in model.modules():\n",
        "    if isinstance(module, PrunableLinear):\n",
        "      module.weight *= module.mask"
      ],
      "metadata": {
        "id": "A5LRFwEFNbS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TP_aDGGWN_8r",
        "outputId": "8c6eba67-5afd-4f26-8ebe-509e999dea0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "700"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a263zbjN7-r",
        "outputId": "304735b1-4d81-4e0e-fbe9-a00651c9ead9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 700: train loss 8.1779, val loss 8.1862\n",
            "iter 700: loss 8.1901, time 129157.44ms, mfu -100.00%\n",
            "iter 701: loss 7.7622, time 29898.45ms, mfu -100.00%\n",
            "iter 702: loss 7.3745, time 30075.06ms, mfu -100.00%\n",
            "iter 703: loss 6.9212, time 30213.90ms, mfu -100.00%\n",
            "iter 704: loss 6.5888, time 30212.24ms, mfu -100.00%\n",
            "iter 705: loss 6.1762, time 30242.17ms, mfu 4.21%\n",
            "iter 706: loss 5.6560, time 30225.20ms, mfu 4.21%\n",
            "iter 707: loss 5.5904, time 30214.18ms, mfu 4.21%\n",
            "iter 708: loss 5.3330, time 30116.91ms, mfu 4.21%\n",
            "iter 709: loss 5.1531, time 30044.39ms, mfu 4.22%\n",
            "iter 710: loss 5.0524, time 30178.26ms, mfu 4.22%\n",
            "iter 711: loss 4.9326, time 30309.63ms, mfu 4.21%\n",
            "iter 712: loss 4.9083, time 29992.96ms, mfu 4.22%\n",
            "iter 713: loss 4.6380, time 30146.25ms, mfu 4.22%\n",
            "iter 714: loss 4.7053, time 30237.18ms, mfu 4.22%\n",
            "iter 715: loss 4.6639, time 30244.66ms, mfu 4.22%\n",
            "iter 716: loss 4.5953, time 30259.72ms, mfu 4.22%\n",
            "iter 717: loss 4.5339, time 30259.80ms, mfu 4.21%\n",
            "iter 718: loss 4.3792, time 30218.27ms, mfu 4.21%\n",
            "iter 719: loss 4.2552, time 30196.56ms, mfu 4.21%\n",
            "iter 720: loss 4.3172, time 30211.57ms, mfu 4.21%\n",
            "iter 721: loss 4.1309, time 30238.48ms, mfu 4.21%\n",
            "iter 722: loss 4.2133, time 30207.96ms, mfu 4.21%\n",
            "iter 723: loss 4.1275, time 30231.07ms, mfu 4.21%\n",
            "iter 724: loss 4.1606, time 30223.70ms, mfu 4.21%\n",
            "iter 725: loss 4.1226, time 30179.33ms, mfu 4.21%\n",
            "iter 726: loss 4.0207, time 30151.31ms, mfu 4.22%\n",
            "iter 727: loss 4.0311, time 30217.35ms, mfu 4.22%\n",
            "iter 728: loss 3.9793, time 30206.28ms, mfu 4.22%\n",
            "iter 729: loss 3.9608, time 30201.00ms, mfu 4.22%\n",
            "iter 730: loss 4.0730, time 30202.87ms, mfu 4.22%\n",
            "iter 731: loss 3.8127, time 30153.40ms, mfu 4.22%\n",
            "iter 732: loss 3.7288, time 30180.41ms, mfu 4.22%\n",
            "iter 733: loss 3.9066, time 30224.59ms, mfu 4.22%\n",
            "iter 734: loss 3.7585, time 30233.00ms, mfu 4.22%\n",
            "iter 735: loss 3.7612, time 30250.07ms, mfu 4.21%\n",
            "iter 736: loss 3.7527, time 30145.73ms, mfu 4.22%\n",
            "iter 737: loss 3.8240, time 30061.95ms, mfu 4.22%\n",
            "iter 738: loss 3.8172, time 30137.31ms, mfu 4.22%\n",
            "iter 739: loss 3.9045, time 30193.67ms, mfu 4.22%\n",
            "iter 740: loss 3.8810, time 30085.37ms, mfu 4.22%\n",
            "iter 741: loss 3.8866, time 30121.44ms, mfu 4.22%\n",
            "iter 742: loss 3.4765, time 30138.51ms, mfu 4.22%\n",
            "iter 743: loss 3.6579, time 30218.12ms, mfu 4.22%\n",
            "iter 744: loss 3.7379, time 30242.57ms, mfu 4.22%\n",
            "iter 745: loss 3.6695, time 30121.63ms, mfu 4.22%\n",
            "iter 746: loss 3.6332, time 30171.05ms, mfu 4.22%\n",
            "iter 747: loss 3.5540, time 30229.82ms, mfu 4.22%\n",
            "iter 748: loss 3.7264, time 30235.38ms, mfu 4.22%\n",
            "iter 749: loss 3.7175, time 30274.17ms, mfu 4.22%\n",
            "iter 750: loss 3.7834, time 30230.37ms, mfu 4.22%\n",
            "iter 751: loss 3.7688, time 30277.38ms, mfu 4.22%\n",
            "iter 752: loss 3.7655, time 30230.26ms, mfu 4.22%\n",
            "iter 753: loss 3.7208, time 30205.83ms, mfu 4.22%\n",
            "iter 754: loss 3.6227, time 30116.67ms, mfu 4.22%\n",
            "iter 755: loss 3.4935, time 30155.80ms, mfu 4.22%\n",
            "iter 756: loss 3.6147, time 30038.74ms, mfu 4.22%\n",
            "iter 757: loss 3.6398, time 30082.17ms, mfu 4.22%\n",
            "iter 758: loss 3.6729, time 30017.83ms, mfu 4.22%\n",
            "iter 759: loss 3.7217, time 30091.63ms, mfu 4.22%\n",
            "iter 760: loss 3.5645, time 30144.63ms, mfu 4.22%\n",
            "iter 761: loss 3.5944, time 30219.15ms, mfu 4.22%\n",
            "iter 762: loss 3.2574, time 30241.93ms, mfu 4.22%\n",
            "iter 763: loss 3.5980, time 30221.40ms, mfu 4.22%\n",
            "iter 764: loss 3.5829, time 30182.87ms, mfu 4.22%\n",
            "iter 765: loss 3.4316, time 30214.29ms, mfu 4.22%\n",
            "iter 766: loss 3.4527, time 30112.67ms, mfu 4.22%\n",
            "iter 767: loss 3.5815, time 30082.84ms, mfu 4.22%\n",
            "iter 768: loss 3.6075, time 30099.16ms, mfu 4.22%\n",
            "iter 769: loss 3.6742, time 30219.56ms, mfu 4.22%\n",
            "iter 770: loss 3.7640, time 30231.62ms, mfu 4.22%\n",
            "iter 771: loss 3.5468, time 30265.63ms, mfu 4.22%\n",
            "iter 772: loss 3.7209, time 30233.75ms, mfu 4.22%\n",
            "iter 773: loss 3.5628, time 30244.96ms, mfu 4.22%\n",
            "iter 774: loss 3.6967, time 30246.28ms, mfu 4.22%\n",
            "iter 775: loss 3.5719, time 30212.66ms, mfu 4.22%\n",
            "iter 776: loss 3.7044, time 30072.50ms, mfu 4.22%\n",
            "iter 777: loss 3.4542, time 30116.49ms, mfu 4.22%\n",
            "iter 778: loss 3.4851, time 30039.88ms, mfu 4.22%\n",
            "iter 779: loss 3.7705, time 30102.65ms, mfu 4.22%\n",
            "iter 780: loss 3.4574, time 30033.02ms, mfu 4.22%\n",
            "iter 781: loss 3.4732, time 30032.43ms, mfu 4.23%\n",
            "iter 782: loss 3.4227, time 30072.48ms, mfu 4.23%\n",
            "iter 783: loss 3.4598, time 30197.20ms, mfu 4.23%\n",
            "iter 784: loss 3.7051, time 30243.98ms, mfu 4.22%\n",
            "iter 785: loss 3.5461, time 30274.88ms, mfu 4.22%\n",
            "iter 786: loss 3.5281, time 30122.11ms, mfu 4.22%\n",
            "iter 787: loss 3.5933, time 30038.73ms, mfu 4.22%\n",
            "iter 788: loss 3.5622, time 30074.20ms, mfu 4.22%\n",
            "iter 789: loss 3.5680, time 30281.84ms, mfu 4.22%\n",
            "iter 790: loss 3.3359, time 30148.99ms, mfu 4.22%\n",
            "iter 791: loss 3.4486, time 30093.66ms, mfu 4.22%\n",
            "iter 792: loss 3.5760, time 29984.07ms, mfu 4.23%\n",
            "iter 793: loss 3.5726, time 30036.35ms, mfu 4.23%\n",
            "iter 794: loss 3.4401, time 30245.28ms, mfu 4.23%\n",
            "iter 795: loss 3.2388, time 30166.71ms, mfu 4.23%\n",
            "iter 796: loss 3.4450, time 29941.93ms, mfu 4.23%\n",
            "iter 797: loss 3.3826, time 30033.94ms, mfu 4.23%\n",
            "iter 798: loss 3.5652, time 30195.07ms, mfu 4.23%\n",
            "iter 799: loss 3.5439, time 30266.56ms, mfu 4.23%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqJisRILODzL",
        "outputId": "db2e3b56-c6c6-41f3-fe29-1ef2a25e7e1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 800: train loss 3.4667, val loss 3.5555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for module in model.modules():\n",
        "    if isinstance(module, PrunableLinear):\n",
        "      module.weight *= module.mask"
      ],
      "metadata": {
        "id": "epGk0Q8MaAlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7V-ouDTaD1z",
        "outputId": "b01e535a-0c24-43de-81e3-fd59bce65acc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 800: train loss 3.5245, val loss 3.5823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"saving checkpoint to {out_dir}\")\n",
        "torch.save({\n",
        "    'model': model.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "    'model_args': model_args,\n",
        "    'iter_num': iter_num,\n",
        "    'best_val_loss': best_val_loss,\n",
        "    'config': config,\n",
        "}, os.path.join(out_dir, 'ckpt_800_pruned_0.72.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vc3qL9_JaGGf",
        "outputId": "ae540b02-ddc0-47f4-a825-095c531327f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving checkpoint to out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.prune(m=0.135)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpKZU51LaOvy",
        "outputId": "eb8dc39f-ffff-4d54-b4b2-c36e005b058c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8087050581631369"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for module in model.modules():\n",
        "    if isinstance(module, PrunableLinear):\n",
        "      module.weight *= module.mask"
      ],
      "metadata": {
        "id": "HpF5CG97bCOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bALgfVASblUr",
        "outputId": "a388c7dc-36ae-425a-d33b-c905a2dfdc8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 800: train loss 8.3409, val loss 8.3226\n",
            "iter 800: loss 8.3951, time 127371.10ms, mfu -100.00%\n",
            "iter 801: loss 7.7727, time 29615.42ms, mfu -100.00%\n",
            "iter 802: loss 7.3765, time 29540.40ms, mfu -100.00%\n",
            "iter 803: loss 7.2748, time 29732.66ms, mfu -100.00%\n",
            "iter 804: loss 7.1498, time 29776.35ms, mfu -100.00%\n",
            "iter 805: loss 6.8713, time 29875.20ms, mfu 4.26%\n",
            "iter 806: loss 6.7723, time 29766.01ms, mfu 4.26%\n",
            "iter 807: loss 6.6045, time 29648.06ms, mfu 4.27%\n",
            "iter 808: loss 6.4459, time 29570.16ms, mfu 4.27%\n",
            "iter 809: loss 6.3131, time 29668.17ms, mfu 4.27%\n",
            "iter 810: loss 6.1586, time 29706.61ms, mfu 4.27%\n",
            "iter 811: loss 6.0910, time 29771.78ms, mfu 4.27%\n",
            "iter 812: loss 5.8455, time 29775.88ms, mfu 4.27%\n",
            "iter 813: loss 5.7877, time 29784.54ms, mfu 4.27%\n",
            "iter 814: loss 5.7291, time 29713.06ms, mfu 4.28%\n",
            "iter 815: loss 5.6286, time 29556.17ms, mfu 4.28%\n",
            "iter 816: loss 5.6160, time 29772.07ms, mfu 4.28%\n",
            "iter 817: loss 5.4840, time 29848.01ms, mfu 4.28%\n",
            "iter 818: loss 5.4755, time 29516.44ms, mfu 4.28%\n",
            "iter 819: loss 5.4259, time 29717.66ms, mfu 4.28%\n",
            "iter 820: loss 5.4378, time 29761.74ms, mfu 4.28%\n",
            "iter 821: loss 5.2033, time 29831.43ms, mfu 4.28%\n",
            "iter 822: loss 5.2351, time 29871.38ms, mfu 4.28%\n",
            "iter 823: loss 5.1039, time 29871.86ms, mfu 4.28%\n",
            "iter 824: loss 5.0396, time 29858.80ms, mfu 4.28%\n",
            "iter 825: loss 5.1114, time 29907.40ms, mfu 4.27%\n",
            "iter 826: loss 4.9774, time 29798.19ms, mfu 4.27%\n",
            "iter 827: loss 4.7452, time 29672.96ms, mfu 4.28%\n",
            "iter 828: loss 5.0211, time 29574.59ms, mfu 4.28%\n",
            "iter 829: loss 4.9913, time 29608.55ms, mfu 4.28%\n",
            "iter 830: loss 4.7651, time 29678.52ms, mfu 4.28%\n",
            "iter 831: loss 4.8771, time 29772.14ms, mfu 4.28%\n",
            "iter 832: loss 4.7522, time 29765.82ms, mfu 4.28%\n",
            "iter 833: loss 4.8005, time 29678.39ms, mfu 4.28%\n",
            "iter 834: loss 4.5941, time 29655.00ms, mfu 4.28%\n",
            "iter 835: loss 4.5823, time 29874.78ms, mfu 4.28%\n",
            "iter 836: loss 4.6437, time 29761.61ms, mfu 4.28%\n",
            "iter 837: loss 4.4836, time 29712.39ms, mfu 4.28%\n",
            "iter 838: loss 4.4188, time 29652.12ms, mfu 4.28%\n",
            "iter 839: loss 4.5091, time 29661.98ms, mfu 4.28%\n",
            "iter 840: loss 4.5353, time 29593.02ms, mfu 4.29%\n",
            "iter 841: loss 4.3435, time 29617.50ms, mfu 4.29%\n",
            "iter 842: loss 4.4043, time 29554.48ms, mfu 4.29%\n",
            "iter 843: loss 4.4235, time 29775.76ms, mfu 4.29%\n",
            "iter 844: loss 4.4380, time 29625.87ms, mfu 4.29%\n",
            "iter 845: loss 4.0797, time 29781.49ms, mfu 4.29%\n",
            "iter 846: loss 4.5567, time 29804.61ms, mfu 4.29%\n",
            "iter 847: loss 4.3666, time 29856.46ms, mfu 4.28%\n",
            "iter 848: loss 4.4118, time 29885.01ms, mfu 4.28%\n",
            "iter 849: loss 4.3845, time 29921.74ms, mfu 4.28%\n",
            "iter 850: loss 4.4334, time 29795.79ms, mfu 4.28%\n",
            "iter 851: loss 4.1839, time 29804.43ms, mfu 4.28%\n",
            "iter 852: loss 4.4312, time 29724.87ms, mfu 4.28%\n",
            "iter 853: loss 4.3031, time 29731.64ms, mfu 4.28%\n",
            "iter 854: loss 4.2162, time 29663.25ms, mfu 4.28%\n",
            "iter 855: loss 4.1634, time 29672.15ms, mfu 4.28%\n",
            "iter 856: loss 4.1352, time 29608.05ms, mfu 4.28%\n",
            "iter 857: loss 4.0977, time 29682.89ms, mfu 4.28%\n",
            "iter 858: loss 4.1871, time 29639.47ms, mfu 4.28%\n",
            "iter 859: loss 4.1677, time 29658.47ms, mfu 4.29%\n",
            "iter 860: loss 4.3389, time 29532.61ms, mfu 4.29%\n",
            "iter 861: loss 4.3546, time 29886.31ms, mfu 4.29%\n",
            "iter 862: loss 4.1863, time 29782.58ms, mfu 4.28%\n",
            "iter 863: loss 4.2262, time 29597.66ms, mfu 4.29%\n",
            "iter 864: loss 4.2168, time 29616.06ms, mfu 4.29%\n",
            "iter 865: loss 4.1635, time 29682.53ms, mfu 4.29%\n",
            "iter 866: loss 4.0153, time 29872.74ms, mfu 4.29%\n",
            "iter 867: loss 4.1398, time 29825.86ms, mfu 4.28%\n",
            "iter 868: loss 4.1005, time 29681.63ms, mfu 4.28%\n",
            "iter 869: loss 4.1316, time 29637.78ms, mfu 4.29%\n",
            "iter 870: loss 4.2424, time 29583.49ms, mfu 4.29%\n",
            "iter 871: loss 3.9675, time 29721.23ms, mfu 4.29%\n",
            "iter 872: loss 4.1519, time 29713.52ms, mfu 4.29%\n",
            "iter 873: loss 4.1252, time 29827.75ms, mfu 4.28%\n",
            "iter 874: loss 4.3076, time 29823.18ms, mfu 4.28%\n",
            "iter 875: loss 4.0594, time 29858.72ms, mfu 4.28%\n",
            "iter 876: loss 4.2581, time 29804.39ms, mfu 4.28%\n",
            "iter 877: loss 4.0320, time 29744.91ms, mfu 4.28%\n",
            "iter 878: loss 4.2424, time 29656.54ms, mfu 4.28%\n",
            "iter 879: loss 3.9639, time 29651.77ms, mfu 4.28%\n",
            "iter 880: loss 4.0511, time 29667.04ms, mfu 4.28%\n",
            "iter 881: loss 3.9477, time 29702.49ms, mfu 4.28%\n",
            "iter 882: loss 3.9684, time 29624.30ms, mfu 4.29%\n",
            "iter 883: loss 4.0713, time 29720.42ms, mfu 4.29%\n",
            "iter 884: loss 4.2210, time 29638.48ms, mfu 4.29%\n",
            "iter 885: loss 3.9300, time 29664.95ms, mfu 4.29%\n",
            "iter 886: loss 4.1810, time 29718.20ms, mfu 4.29%\n",
            "iter 887: loss 4.1048, time 29777.34ms, mfu 4.29%\n",
            "iter 888: loss 4.0724, time 29690.73ms, mfu 4.29%\n",
            "iter 889: loss 3.9290, time 29615.51ms, mfu 4.29%\n",
            "iter 890: loss 4.0502, time 29605.05ms, mfu 4.29%\n",
            "iter 891: loss 3.9024, time 29716.45ms, mfu 4.29%\n",
            "iter 892: loss 4.0526, time 29738.99ms, mfu 4.29%\n",
            "iter 893: loss 4.1110, time 29800.26ms, mfu 4.29%\n",
            "iter 894: loss 3.9763, time 29736.88ms, mfu 4.29%\n",
            "iter 895: loss 3.9516, time 29693.73ms, mfu 4.29%\n",
            "iter 896: loss 4.0198, time 29744.90ms, mfu 4.29%\n",
            "iter 897: loss 4.0216, time 29832.25ms, mfu 4.28%\n",
            "iter 898: loss 3.7831, time 29791.79ms, mfu 4.28%\n",
            "iter 899: loss 3.9825, time 29779.38ms, mfu 4.28%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ALx-TIwbqSN",
        "outputId": "47c7a24c-d961-4230-dc6f-224c2ac4140b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 900: train loss 3.9550, val loss 4.0399\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for module in model.modules():\n",
        "    if isinstance(module, PrunableLinear):\n",
        "      module.weight *= module.mask"
      ],
      "metadata": {
        "id": "6O5axbWCnG6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-SZMHiKnKHy",
        "outputId": "50e337b3-8ad6-427f-eb15-f2c893de3dad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 900: train loss 4.1152, val loss 4.2020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"saving checkpoint to {out_dir}\")\n",
        "torch.save({\n",
        "    'model': model.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "    'model_args': model_args,\n",
        "    'iter_num': iter_num,\n",
        "    'best_val_loss': best_val_loss,\n",
        "    'config': config,\n",
        "}, os.path.join(out_dir, 'ckpt_900_pruned_0.81.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xw86HqqOnOLI",
        "outputId": "26f57cc9-520e-4660-95b1-b7f73210da1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving checkpoint to out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.prune(m=0.175)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8msZj51nw0W",
        "outputId": "65b88a44-1e3b-42d4-b9ee-2f9f714e0399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9022883983709039"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for module in model.modules():\n",
        "    if isinstance(module, PrunableLinear):\n",
        "      module.weight *= module.mask"
      ],
      "metadata": {
        "id": "Eds7Fa-qoPVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNnhBWY1ohmy",
        "outputId": "b8bf1132-732b-4dad-e9e6-01df7a751b06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 900: train loss 9.6733, val loss 9.6650\n",
            "iter 900: loss 9.7478, time 124005.52ms, mfu -100.00%\n",
            "iter 901: loss 8.7884, time 29169.71ms, mfu -100.00%\n",
            "iter 902: loss 8.3018, time 28942.13ms, mfu -100.00%\n",
            "iter 903: loss 8.1438, time 29191.02ms, mfu -100.00%\n",
            "iter 904: loss 7.9312, time 29174.26ms, mfu -100.00%\n",
            "iter 905: loss 7.8321, time 29220.66ms, mfu 4.36%\n",
            "iter 906: loss 7.8898, time 29105.11ms, mfu 4.36%\n",
            "iter 907: loss 7.7316, time 29048.25ms, mfu 4.36%\n",
            "iter 908: loss 7.7245, time 28927.65ms, mfu 4.37%\n",
            "iter 909: loss 7.7123, time 28945.70ms, mfu 4.37%\n",
            "iter 910: loss 7.5970, time 29150.14ms, mfu 4.37%\n",
            "iter 911: loss 7.5676, time 29204.94ms, mfu 4.37%\n",
            "iter 912: loss 7.5733, time 29101.15ms, mfu 4.37%\n",
            "iter 913: loss 7.4346, time 29039.16ms, mfu 4.37%\n",
            "iter 914: loss 7.3628, time 28961.09ms, mfu 4.37%\n",
            "iter 915: loss 7.2901, time 29100.33ms, mfu 4.37%\n",
            "iter 916: loss 7.4284, time 29232.42ms, mfu 4.37%\n",
            "iter 917: loss 7.2996, time 29121.57ms, mfu 4.37%\n",
            "iter 918: loss 7.2370, time 28995.35ms, mfu 4.37%\n",
            "iter 919: loss 7.2218, time 28973.45ms, mfu 4.38%\n",
            "iter 920: loss 7.1434, time 29077.33ms, mfu 4.38%\n",
            "iter 921: loss 7.1569, time 29236.22ms, mfu 4.37%\n",
            "iter 922: loss 7.1187, time 29232.10ms, mfu 4.37%\n",
            "iter 923: loss 7.1595, time 29247.61ms, mfu 4.37%\n",
            "iter 924: loss 7.0119, time 29247.43ms, mfu 4.37%\n",
            "iter 925: loss 6.9630, time 29217.01ms, mfu 4.37%\n",
            "iter 926: loss 6.8706, time 29112.14ms, mfu 4.37%\n",
            "iter 927: loss 6.7358, time 29127.75ms, mfu 4.37%\n",
            "iter 928: loss 6.6816, time 29032.15ms, mfu 4.37%\n",
            "iter 929: loss 6.6911, time 29036.88ms, mfu 4.37%\n",
            "iter 930: loss 6.8314, time 29015.69ms, mfu 4.37%\n",
            "iter 931: loss 6.7116, time 29081.93ms, mfu 4.37%\n",
            "iter 932: loss 6.7155, time 29122.91ms, mfu 4.37%\n",
            "iter 933: loss 6.5776, time 29237.75ms, mfu 4.37%\n",
            "iter 934: loss 6.6866, time 29242.76ms, mfu 4.37%\n",
            "iter 935: loss 6.5427, time 29281.97ms, mfu 4.37%\n",
            "iter 936: loss 6.3833, time 29237.71ms, mfu 4.37%\n",
            "iter 937: loss 6.5911, time 29181.18ms, mfu 4.37%\n",
            "iter 938: loss 6.4492, time 29081.44ms, mfu 4.37%\n",
            "iter 939: loss 6.3843, time 29326.56ms, mfu 4.36%\n",
            "iter 940: loss 6.3480, time 29247.34ms, mfu 4.36%\n",
            "iter 941: loss 6.4691, time 29037.76ms, mfu 4.37%\n",
            "iter 942: loss 6.3427, time 29365.37ms, mfu 4.36%\n",
            "iter 943: loss 6.3216, time 29223.32ms, mfu 4.36%\n",
            "iter 944: loss 6.2793, time 29066.21ms, mfu 4.36%\n",
            "iter 945: loss 6.2897, time 29248.22ms, mfu 4.36%\n",
            "iter 946: loss 6.3555, time 29312.83ms, mfu 4.36%\n",
            "iter 947: loss 6.2952, time 29284.31ms, mfu 4.36%\n",
            "iter 948: loss 6.2077, time 29100.83ms, mfu 4.36%\n",
            "iter 949: loss 6.2679, time 29398.35ms, mfu 4.36%\n",
            "iter 950: loss 5.9925, time 29317.08ms, mfu 4.36%\n",
            "iter 951: loss 6.1292, time 29309.85ms, mfu 4.36%\n",
            "iter 952: loss 6.2254, time 29252.43ms, mfu 4.36%\n",
            "iter 953: loss 6.2023, time 29221.71ms, mfu 4.36%\n",
            "iter 954: loss 6.2073, time 29188.83ms, mfu 4.36%\n",
            "iter 955: loss 6.0621, time 29194.67ms, mfu 4.36%\n",
            "iter 956: loss 6.0228, time 29135.63ms, mfu 4.36%\n",
            "iter 957: loss 6.1185, time 29147.56ms, mfu 4.36%\n",
            "iter 958: loss 5.9870, time 29112.63ms, mfu 4.36%\n",
            "iter 959: loss 5.9882, time 29180.07ms, mfu 4.36%\n",
            "iter 960: loss 6.0086, time 29203.11ms, mfu 4.36%\n",
            "iter 961: loss 5.9183, time 29282.87ms, mfu 4.36%\n",
            "iter 962: loss 5.9376, time 29251.80ms, mfu 4.36%\n",
            "iter 963: loss 6.0338, time 29247.11ms, mfu 4.36%\n",
            "iter 964: loss 6.1048, time 29257.60ms, mfu 4.36%\n",
            "iter 965: loss 6.1142, time 29343.72ms, mfu 4.36%\n",
            "iter 966: loss 6.0345, time 29322.99ms, mfu 4.35%\n",
            "iter 967: loss 5.9461, time 29225.54ms, mfu 4.35%\n",
            "iter 968: loss 5.8602, time 29195.48ms, mfu 4.36%\n",
            "iter 969: loss 5.9719, time 29258.84ms, mfu 4.35%\n",
            "iter 970: loss 5.8356, time 29241.98ms, mfu 4.35%\n",
            "iter 971: loss 5.8735, time 29254.45ms, mfu 4.35%\n",
            "iter 972: loss 5.9718, time 29259.27ms, mfu 4.35%\n",
            "iter 973: loss 5.9749, time 29275.02ms, mfu 4.35%\n",
            "iter 974: loss 5.8392, time 29262.34ms, mfu 4.35%\n",
            "iter 975: loss 6.0215, time 29320.46ms, mfu 4.35%\n",
            "iter 976: loss 5.7417, time 29272.93ms, mfu 4.35%\n",
            "iter 977: loss 5.9316, time 29317.45ms, mfu 4.35%\n",
            "iter 978: loss 5.8308, time 29287.26ms, mfu 4.35%\n",
            "iter 979: loss 6.0395, time 29350.63ms, mfu 4.35%\n",
            "iter 980: loss 5.7648, time 29383.95ms, mfu 4.35%\n",
            "iter 981: loss 5.8522, time 29424.94ms, mfu 4.35%\n",
            "iter 982: loss 5.7298, time 29295.08ms, mfu 4.35%\n",
            "iter 983: loss 5.8814, time 29228.33ms, mfu 4.35%\n",
            "iter 984: loss 5.6721, time 29160.98ms, mfu 4.35%\n",
            "iter 985: loss 5.7519, time 29172.38ms, mfu 4.35%\n",
            "iter 986: loss 5.7966, time 29051.93ms, mfu 4.35%\n",
            "iter 987: loss 5.7009, time 29405.86ms, mfu 4.35%\n",
            "iter 988: loss 5.7632, time 29359.09ms, mfu 4.35%\n",
            "iter 989: loss 5.8555, time 29177.37ms, mfu 4.35%\n",
            "iter 990: loss 5.7135, time 29111.83ms, mfu 4.35%\n",
            "iter 991: loss 5.4932, time 29214.77ms, mfu 4.35%\n",
            "iter 992: loss 5.6632, time 29253.02ms, mfu 4.35%\n",
            "iter 993: loss 5.7606, time 29340.32ms, mfu 4.35%\n",
            "iter 994: loss 5.6687, time 29310.63ms, mfu 4.35%\n",
            "iter 995: loss 5.7114, time 29311.92ms, mfu 4.35%\n",
            "iter 996: loss 5.6860, time 29313.83ms, mfu 4.35%\n",
            "iter 997: loss 5.5944, time 29418.85ms, mfu 4.35%\n",
            "iter 998: loss 5.6534, time 29428.13ms, mfu 4.35%\n",
            "iter 999: loss 5.5356, time 29382.80ms, mfu 4.34%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRneellAolRT",
        "outputId": "53cb3c66-fa59-44d0-c675-78ca76a17e32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1000: train loss 5.6313, val loss 5.6840\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for module in model.modules():\n",
        "    if isinstance(module, PrunableLinear):\n",
        "      module.weight *= module.mask"
      ],
      "metadata": {
        "id": "wK81RNOAorub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJApwLn4ovSi",
        "outputId": "d833a279-e071-4030-e655-99fc9a894b4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1000: train loss 6.1090, val loss 6.1618\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"saving checkpoint to {out_dir}\")\n",
        "torch.save({\n",
        "    'model': model.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "    'model_args': model_args,\n",
        "    'iter_num': iter_num,\n",
        "    'best_val_loss': best_val_loss,\n",
        "    'config': config,\n",
        "}, os.path.join(out_dir, 'ckpt_1000_pruned_0.90.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzebuOF4oyJU",
        "outputId": "ff831d0b-c36e-4738-d68b-05692ea7e84a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving checkpoint to out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oEb8vHqYpMlU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}