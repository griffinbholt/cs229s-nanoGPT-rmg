{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SQ__4POy8Vo"
      },
      "source": [
        "To run this notebook: import GPT from model_quant.py in sample.py and train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cy75saA_KfAt",
        "outputId": "975b139d-98df-4c8d-f86a-b75be59205e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.16.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Collecting memory-profiler\n",
            "  Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
            "Collecting torcheval\n",
            "  Downloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.39.0-py2.py3-none-any.whl (254 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.0/254.0 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: torcheval, smmap, setproctitle, sentry-sdk, pyarrow-hotfix, memory-profiler, docker-pycreds, dill, tiktoken, multiprocess, gitdb, GitPython, wandb, datasets\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.40 datasets-2.15.0 dill-0.3.7 docker-pycreds-0.4.0 gitdb-4.0.11 memory-profiler-0.61.0 multiprocess-0.70.15 pyarrow-hotfix-0.6 sentry-sdk-1.39.0 setproctitle-1.3.3 smmap-5.0.1 tiktoken-0.5.2 torcheval-0.0.7 wandb-0.16.1\n"
          ]
        }
      ],
      "source": [
        "# install necessary libraries\n",
        "!pip install torch numpy transformers datasets tiktoken wandb tqdm memory-profiler torcheval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEhwPEfR8Bf2",
        "outputId": "fa27958a-5a8d-4e3a-9c65-da341a07e45b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Suzdrt38VI1",
        "outputId": "22daf161-855a-434d-e6d7-e1805697c724"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/CS229S/cs229s-nanoGPT-rmg\n"
          ]
        }
      ],
      "source": [
        "# replace this with the folder in Drive where your files are stored\n",
        "%cd /content/drive/MyDrive/CS229S/cs229s-nanoGPT-rmg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oB4RKhfbUBaB",
        "outputId": "51f77b04-4142-4735-e47d-82e6c98a18d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading builder script: 100% 8.48k/8.48k [00:00<00:00, 25.5MB/s]\n",
            "Downloading metadata: 100% 6.84k/6.84k [00:00<00:00, 27.9MB/s]\n",
            "Downloading readme: 100% 9.62k/9.62k [00:00<00:00, 22.5MB/s]\n",
            "Downloading data: 100% 190M/190M [00:13<00:00, 14.2MB/s]\n",
            "Setting num_proc from 8 back to 1 for the test split to disable multiprocessing as it only contains one shard.\n",
            "Generating test split: 100% 4358/4358 [00:00<00:00, 44842.14 examples/s]\n",
            "Setting num_proc from 8 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "Generating train split: 100% 1801350/1801350 [00:34<00:00, 52176.78 examples/s]\n",
            "Setting num_proc from 8 back to 1 for the validation split to disable multiprocessing as it only contains one shard.\n",
            "Generating validation split: 100% 3760/3760 [00:00<00:00, 51030.71 examples/s]\n",
            "tokenizing the splits (num_proc=8): 100% 1800449/1800449 [05:04<00:00, 5906.41 examples/s]\n",
            "tokenizing the splits (num_proc=8): 100% 901/901 [00:00<00:00, 1200.48 examples/s]\n",
            "writing /content/drive/MyDrive/CS229S/cs229s-nanoGPT-rmg/data/wikitext/train.bin: 100% 800/800 [00:15<00:00, 50.53it/s]\n",
            "writing /content/drive/MyDrive/CS229S/cs229s-nanoGPT-rmg/data/wikitext/val.bin: 100% 800/800 [00:01<00:00, 615.11it/s]\n"
          ]
        }
      ],
      "source": [
        "!python data/wikitext/prepare.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JQ4B-JnO7zn",
        "outputId": "1fcf88fd-3ea7-4563-88bf-8201c41cc2ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overriding: init_from = gpt2\n",
            "Overriding: start = Hello world!\n",
            "Overriding: num_samples = 1\n",
            "Overriding: max_new_tokens = 50\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "config.json: 100% 665/665 [00:00<00:00, 2.85MB/s]\n",
            "model.safetensors: 100% 548M/548M [00:02<00:00, 252MB/s]\n",
            "generation_config.json: 100% 124/124 [00:00<00:00, 632kB/s]\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "Hello world! Have any of you found a solution to the problem of so called 'crapture'? Yes, you do.\n",
            "\n",
            "Actually, here's an example of a problem created by the last time I encountered the concept.\n",
            "\n",
            "We found out\n",
            "MAX MEM:  1158434816\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "# Running this will save a copy of the quantized model to the 'out' directory\n",
        "# You may have to create the 'out' directory...\n",
        "!python sample.py \\\n",
        "    --init_from=gpt2 \\\n",
        "    --start=\"Hello world!\" \\\n",
        "    --num_samples=1 --max_new_tokens=50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-HeoSQgnRm4",
        "outputId": "462a9b00-9707-475f-aa00-f426bfe13caa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overriding: batch_size = 4\n",
            "Overriding: eval_only = True\n",
            "Overriding: init_from = gpt2-quantized\n",
            "tokens per iteration will be: 163,840\n",
            "Initializing from OpenAI GPT-2 weights: gpt2-quantized\n",
            "loading quantized weights from pretrained gpt: gpt2-quantized\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "MAX MEM:  175188992\n",
            "MAX MEM: 838740992\n",
            "num decayed parameter tensors: 0, with 0 parameters\n",
            "num non-decayed parameter tensors: 0, with 0 parameters\n",
            "using fused AdamW: True\n",
            "100% 200/200 [00:19<00:00, 10.12it/s]\n",
            "100% 200/200 [00:19<00:00, 10.24it/s]\n",
            "step 0: train loss 4.0608, val loss 4.0641\n",
            "MAX MEM:  2935009792\n"
          ]
        }
      ],
      "source": [
        "!python train.py --batch_size=4 --eval_only=True --init_from=gpt2-quantized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxI4rwlOJsqz",
        "outputId": "f3c432fc-c66a-462b-dff6-2bfe5eb586c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overriding: batch_size = 12\n",
            "Overriding: eval_only = True\n",
            "Overriding: init_from = gpt2-quantized\n",
            "tokens per iteration will be: 491,520\n",
            "Initializing from OpenAI GPT-2 weights: gpt2-quantized\n",
            "loading quantized weights from pretrained gpt: gpt2-quantized\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "MAX MEM: 838740992\n",
            "num decayed parameter tensors: 0, with 0 parameters\n",
            "num non-decayed parameter tensors: 0, with 0 parameters\n",
            "using fused AdamW: True\n",
            "100% 200/200 [00:57<00:00,  3.51it/s]\n",
            "100% 200/200 [00:57<00:00,  3.50it/s]\n",
            "step 0: train loss 4.0607, val loss 4.0611\n",
            "MAX MEM:  7102312960\n"
          ]
        }
      ],
      "source": [
        "!python train.py --batch_size=12 --eval_only=True --init_from=gpt2-quantized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8A7FM0rUIWIl",
        "outputId": "00718283-323f-4485-87eb-16056667ecad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overriding: batch_size = 4\n",
            "Overriding: eval_only = True\n",
            "tokens per iteration will be: 163,840\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "MAX MEM: 1215850496\n",
            "num decayed parameter tensors: 50, with 124,318,464 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n",
            "100% 200/200 [00:19<00:00, 10.10it/s]\n",
            "100% 200/200 [00:19<00:00, 10.20it/s]\n",
            "step 0: train loss 4.0313, val loss 4.0338\n",
            "MAX MEM:  3564008960\n"
          ]
        }
      ],
      "source": [
        "# manually set QUANTIZE = False in model_quant.py\n",
        "# just for comparison\n",
        "!python train.py --batch_size=4 --eval_only=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKDVK3mMZOZt",
        "outputId": "a6730866-0746-4665-c6d0-005bf22c8db3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overriding: batch_size = 12\n",
            "Overriding: eval_only = True\n",
            "tokens per iteration will be: 491,520\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "MAX MEM: 1215850496\n",
            "num decayed parameter tensors: 50, with 124,318,464 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n",
            "100% 200/200 [00:53<00:00,  3.76it/s]\n",
            "100% 200/200 [00:54<00:00,  3.69it/s]\n",
            "step 0: train loss 4.0353, val loss 4.0346\n",
            "MAX MEM:  7728379392\n"
          ]
        }
      ],
      "source": [
        "# manually set QUANTIZE = False\n",
        "# just for comparison\n",
        "!python train.py --batch_size=12 --eval_only=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZOzLpZnH1Qt",
        "outputId": "ef04aa50-24a7-49cf-e3fe-abd2527465a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "58.01490730373554\n",
            "58.03811790846851\n"
          ]
        }
      ],
      "source": [
        "# wikitext perplexities quantized, BS = 12\n",
        "import numpy as np\n",
        "print(np.exp(4.0607))\n",
        "print(np.exp(4.0611))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ue2ms_QfZfgH",
        "outputId": "be84c42a-0c4b-4fbb-856c-be59f3fb2f52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "58.020709084550155\n",
            "58.21249369509203\n"
          ]
        }
      ],
      "source": [
        "# wikitext perplexities quantized, BS = 4\n",
        "import numpy as np\n",
        "print(np.exp(4.0608))\n",
        "print(np.exp(4.0641))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-NagJiWIpHF",
        "outputId": "600e4f1c-7692-41a7-ae05-381b51ede744"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "56.33409799282667\n",
            "56.4751094286601\n"
          ]
        }
      ],
      "source": [
        "# wikitext perplexities unquantized, BS = 4\n",
        "import numpy as np\n",
        "print(np.exp(4.0313))\n",
        "print(np.exp(4.0338))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gix-qm_lj0zO",
        "outputId": "6a1fc3e6-5c67-46d4-e370-15fe6ac5d3ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "56.559885659080365\n",
            "56.52030759305821\n"
          ]
        }
      ],
      "source": [
        "# wikitext perplexities unquantized, BS = 12\n",
        "import numpy as np\n",
        "print(np.exp(4.0353))\n",
        "print(np.exp(4.0346))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python data/shakespeare/prepare.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python sample_spec.py --init_from=gpt2 --batch_size=1 --max_new_tokens=50 --prompt_length=1024 --num_samples=3 --num_warmup=1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
