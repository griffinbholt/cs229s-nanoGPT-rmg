{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqtKpwZz5hkI",
        "outputId": "0dbfb2d0-bf41-4348-83d5-d80290ea46a1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/University (M.S., Stanford)/CS 229S - Systems for ML/cs229s-nanoGPT-rmg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aNC46vF6V4m",
        "outputId": "673a2687-2295-4822-af16-b6eb82245f35"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/University (M.S., Stanford)/CS 229S - Systems for ML/cs229s-nanoGPT-rmg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch numpy transformers datasets tiktoken wandb tqdm memory-profiler torcheval"
      ],
      "metadata": {
        "id": "4i9iFCgX8nzI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python data/wikitext/prepare.py"
      ],
      "metadata": {
        "id": "LwC1_3PL8Apw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCjw_CAr0Jh3",
        "outputId": "615278b2-b3bc-41e4-a326-302dc4a05acf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens per iteration will be: 163,840\n",
            "Resuming training from out\n",
            "number of parameters: 353.77M\n",
            "num decayed parameter tensors: 98, with 354,501,632 parameters\n",
            "num non-decayed parameter tensors: 194, with 321,536 parameters\n",
            "using fused AdamW: True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from model import GPTConfig, GPT\n",
        "from pruning import convert_to_prunable, compress_layers, PrunableLinear\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
        "# I/O\n",
        "out_dir = 'out'\n",
        "eval_interval = 50  # TODO - 100\n",
        "log_interval = 1\n",
        "eval_iters = 200\n",
        "eval_only = False # if True, script exits right after the first eval\n",
        "always_save_checkpoint = False # if True, always save a checkpoint after each eval\n",
        "init_from = 'resume' # 'gpt2-medium' # 'scratch' or 'resume' or 'gpt2*'\n",
        "# wandb logging\n",
        "wandb_log = False # disabled by default\n",
        "wandb_project = 'cs229s'\n",
        "wandb_run_name = 'gpt2' # 'run' + str(time.time())\n",
        "# data\n",
        "dataset = 'wikitext'\n",
        "gradient_accumulation_steps = 5 * 8 # used to simulate larger batch sizes\n",
        "batch_size = 4 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
        "block_size = 1024\n",
        "# model\n",
        "n_layer = 12\n",
        "n_head = 12\n",
        "n_embd = 768\n",
        "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
        "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
        "# adamw optimizer\n",
        "learning_rate = 6e-4 # max learning rate\n",
        "max_iters = 50 # TODO - 100 # total number of training iterations\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
        "# learning rate decay settings\n",
        "decay_lr = True # whether to decay the learning rate\n",
        "warmup_iters = 2000 # how many steps to warm up for\n",
        "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
        "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
        "# DDP settings\n",
        "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
        "# system\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "# -----------------------------------------------------------------------------\n",
        "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
        "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# if not ddp, we are running on a single gpu, and one process\n",
        "seed_offset = 0\n",
        "tokens_per_iter = gradient_accumulation_steps * batch_size * block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "# poor man's data loader\n",
        "data_dir = os.path.join('data', dataset)\n",
        "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "\n",
        "# attempt to derive vocab_size from the dataset\n",
        "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
        "meta_vocab_size = None\n",
        "if os.path.exists(meta_path):\n",
        "    with open(meta_path, 'rb') as f:\n",
        "        meta = pickle.load(f)\n",
        "    meta_vocab_size = meta['vocab_size']\n",
        "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
        "\n",
        "# model init\n",
        "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
        "                  bias=bias, vocab_size=None, dropout=dropout) # start with model_args from command line\n",
        "if init_from == 'scratch':\n",
        "    # init a new model from scratch\n",
        "    print(\"Initializing a new model from scratch\")\n",
        "    # determine the vocab size we'll use for from-scratch training\n",
        "    if meta_vocab_size is None:\n",
        "        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
        "    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)\n",
        "elif init_from == 'resume':\n",
        "    print(f\"Resuming training from {out_dir}\")\n",
        "    # resume training from a checkpoint.\n",
        "    ckpt_path = os.path.join(out_dir, 'ckpt_150_pruned_0.05_direct.pt')\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    checkpoint_model_args = checkpoint['model_args']\n",
        "    # force these config attributes to be equal otherwise we can't even resume training\n",
        "    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
        "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
        "        model_args[k] = checkpoint_model_args[k]\n",
        "    # create the model\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)\n",
        "    convert_to_prunable(model, device=device_type)\n",
        "    state_dict = checkpoint['model']\n",
        "    # fix the keys of the state dictionary :(\n",
        "    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
        "    unwanted_prefix = '_orig_mod.'\n",
        "    for k,v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "    model.load_state_dict(state_dict)\n",
        "    iter_num = checkpoint['iter_num']\n",
        "    best_val_loss = checkpoint['best_val_loss']\n",
        "elif init_from.startswith('gpt2'):\n",
        "    print(f\"Initializing from OpenAI GPT-2 weights: {init_from}\")\n",
        "    # initialize from OpenAI GPT-2 weights\n",
        "    override_args = dict(dropout=dropout)\n",
        "    model = GPT.from_pretrained(init_from, override_args)\n",
        "    # read off the created config params, so we can store them into checkpoint correctly\n",
        "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
        "        model_args[k] = getattr(model.config, k)\n",
        "# crop down the model block size if desired, using model surgery\n",
        "# model = torch.load('test.pt')\n",
        "if block_size < model.config.block_size:\n",
        "    model.crop_block_size(block_size)\n",
        "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
        "model.to(device)\n",
        "\n",
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "\n",
        "# optimizer\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "if init_from == 'resume':\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "checkpoint = None # free up memory\n",
        "\n",
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "# logging\n",
        "if wandb_log:\n",
        "    import wandb\n",
        "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n",
        "\n",
        "del state_dict, checkpoint\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "fdXG4JNegVlo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def finetune(model, iter_num, best_val_loss, eval_only=False):\n",
        "  # training loop\n",
        "  X, Y = get_batch('train') # fetch the very first batch\n",
        "  t0 = time.time()\n",
        "  running_mfu = -1.0\n",
        "  for local_iter_num in range(max_iters):\n",
        "\n",
        "      # determine and set the learning rate for this iteration\n",
        "      lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "      for param_group in optimizer.param_groups:\n",
        "          param_group['lr'] = lr\n",
        "\n",
        "      # evaluate the loss on train/val sets and write checkpoints\n",
        "      # if iter_num % eval_interval == 0:\n",
        "      #     losses = estimate_loss()\n",
        "      #     print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "      #     if wandb_log:\n",
        "      #         wandb.log({\n",
        "      #             \"iter\": iter_num,\n",
        "      #             \"train/loss\": losses['train'],\n",
        "      #             \"val/loss\": losses['val'],\n",
        "      #             \"lr\": lr,\n",
        "      #             \"mfu\": running_mfu*100, # convert to percentage\n",
        "      #         })\n",
        "      #     if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "      #         best_val_loss = losses['val']\n",
        "      #         if iter_num > 0:\n",
        "      #             checkpoint = {\n",
        "      #                 'model': model.state_dict(),\n",
        "      #                 'optimizer': optimizer.state_dict(),\n",
        "      #                 'model_args': model_args,\n",
        "      #                 'iter_num': iter_num,\n",
        "      #                 'best_val_loss': best_val_loss,\n",
        "      #                 'config': config,\n",
        "      #             }\n",
        "      #             print(f\"saving checkpoint to {out_dir}\")\n",
        "      #             torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "      # if local_iter_num == 0 and eval_only:\n",
        "      #     break\n",
        "\n",
        "      # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "      # and using the GradScaler if data type is float16\n",
        "      for micro_step in range(gradient_accumulation_steps):\n",
        "          with ctx:\n",
        "              logits, loss = model(X, Y)\n",
        "              loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "          # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "          X, Y = get_batch('train')\n",
        "          # backward pass, with gradient scaling if training in fp16\n",
        "          scaler.scale(loss).backward()\n",
        "      # clip the gradient\n",
        "      if grad_clip != 0.0:\n",
        "          scaler.unscale_(optimizer)\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "      # step the optimizer and scaler if training in fp16\n",
        "      scaler.step(optimizer)\n",
        "      scaler.update()\n",
        "      # flush the gradients as soon as we can, no need for this memory anymore\n",
        "      optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "      # timing and logging\n",
        "      t1 = time.time()\n",
        "      dt = t1 - t0\n",
        "      t0 = t1\n",
        "      if iter_num % log_interval == 0:\n",
        "          # get loss as float. note: this is a CPU-GPU sync point\n",
        "          # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
        "          lossf = loss.item() * gradient_accumulation_steps\n",
        "          if local_iter_num >= 5: # let the training loop settle a bit\n",
        "              mfu = model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
        "              running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "          print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "\n",
        "      iter_num += 1\n",
        "  return iter_num, best_val_loss"
      ],
      "metadata": {
        "id": "OBVpMckTHJJ1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWr3jMFw0ZKN",
        "outputId": "29c30e5a-0fa2-4d2a-8745-f77f0e002fda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 3.7232, val loss 3.7326\n",
            "iter 0: loss 3.8476, time 138304.75ms, mfu -100.00%\n",
            "iter 1: loss 3.5523, time 31620.78ms, mfu -100.00%\n",
            "iter 2: loss 3.8951, time 31535.70ms, mfu -100.00%\n",
            "iter 3: loss 3.6432, time 31759.12ms, mfu -100.00%\n",
            "iter 4: loss 3.6798, time 31835.23ms, mfu -100.00%\n",
            "iter 5: loss 3.7793, time 31738.59ms, mfu 4.01%\n",
            "iter 6: loss 3.6765, time 31699.78ms, mfu 4.01%\n",
            "iter 7: loss 3.6175, time 31734.08ms, mfu 4.01%\n",
            "iter 8: loss 3.5956, time 31774.57ms, mfu 4.01%\n",
            "iter 9: loss 3.7610, time 31749.35ms, mfu 4.01%\n",
            "iter 10: loss 3.7035, time 31711.85ms, mfu 4.01%\n",
            "iter 11: loss 3.5853, time 31625.33ms, mfu 4.01%\n",
            "iter 12: loss 3.5244, time 31618.93ms, mfu 4.01%\n",
            "iter 13: loss 3.5588, time 31592.44ms, mfu 4.02%\n",
            "iter 14: loss 3.5339, time 31542.19ms, mfu 4.02%\n",
            "iter 15: loss 3.6042, time 31568.39ms, mfu 4.02%\n",
            "iter 16: loss 3.4638, time 31736.23ms, mfu 4.02%\n",
            "iter 17: loss 3.4432, time 31840.00ms, mfu 4.02%\n",
            "iter 18: loss 3.5116, time 31844.93ms, mfu 4.02%\n",
            "iter 19: loss 3.4349, time 31811.03ms, mfu 4.01%\n",
            "iter 20: loss 3.3000, time 31824.93ms, mfu 4.01%\n",
            "iter 21: loss 3.4941, time 31785.84ms, mfu 4.01%\n",
            "iter 22: loss 3.4543, time 31710.05ms, mfu 4.01%\n",
            "iter 23: loss 3.2301, time 31727.64ms, mfu 4.01%\n",
            "iter 24: loss 3.4035, time 31721.06ms, mfu 4.01%\n",
            "iter 25: loss 3.3403, time 31554.87ms, mfu 4.01%\n",
            "iter 26: loss 3.2395, time 31763.58ms, mfu 4.01%\n",
            "iter 27: loss 3.2169, time 31775.10ms, mfu 4.01%\n",
            "iter 28: loss 3.4475, time 31641.39ms, mfu 4.01%\n",
            "iter 29: loss 3.1738, time 31604.83ms, mfu 4.02%\n",
            "iter 30: loss 3.4122, time 31626.64ms, mfu 4.02%\n",
            "iter 31: loss 3.1282, time 31609.14ms, mfu 4.02%\n",
            "iter 32: loss 3.1820, time 31633.10ms, mfu 4.02%\n",
            "iter 33: loss 3.1628, time 31633.92ms, mfu 4.02%\n",
            "iter 34: loss 3.2325, time 31605.20ms, mfu 4.02%\n",
            "iter 35: loss 3.1175, time 31516.11ms, mfu 4.02%\n",
            "iter 36: loss 3.2081, time 31784.17ms, mfu 4.02%\n",
            "iter 37: loss 3.1680, time 31842.98ms, mfu 4.02%\n",
            "iter 38: loss 3.0822, time 31718.54ms, mfu 4.02%\n",
            "iter 39: loss 3.1180, time 31585.94ms, mfu 4.02%\n",
            "iter 40: loss 3.0479, time 31621.10ms, mfu 4.02%\n",
            "iter 41: loss 3.1208, time 31607.78ms, mfu 4.02%\n",
            "iter 42: loss 3.1571, time 31573.10ms, mfu 4.02%\n",
            "iter 43: loss 3.0869, time 31643.16ms, mfu 4.02%\n",
            "iter 44: loss 3.3685, time 31601.31ms, mfu 4.02%\n",
            "iter 45: loss 3.1092, time 31858.94ms, mfu 4.02%\n",
            "iter 46: loss 3.3292, time 31799.65ms, mfu 4.02%\n",
            "iter 47: loss 3.1233, time 31641.98ms, mfu 4.02%\n",
            "iter 48: loss 3.2363, time 31636.13ms, mfu 4.02%\n",
            "iter 49: loss 3.2049, time 31608.33ms, mfu 4.02%\n",
            "iter 50: loss 3.1725, time 31751.05ms, mfu 4.02%\n",
            "iter 51: loss 3.0872, time 31817.47ms, mfu 4.02%\n",
            "iter 52: loss 3.1518, time 31688.86ms, mfu 4.02%\n",
            "iter 53: loss 3.2431, time 31550.00ms, mfu 4.02%\n",
            "iter 54: loss 3.0024, time 31594.54ms, mfu 4.02%\n",
            "iter 55: loss 3.1252, time 31628.49ms, mfu 4.02%\n",
            "iter 56: loss 2.9412, time 31694.78ms, mfu 4.02%\n",
            "iter 57: loss 3.2762, time 31774.72ms, mfu 4.02%\n",
            "iter 58: loss 3.1127, time 31827.42ms, mfu 4.02%\n",
            "iter 59: loss 3.1000, time 31802.93ms, mfu 4.02%\n",
            "iter 60: loss 2.9952, time 31736.80ms, mfu 4.02%\n",
            "iter 61: loss 2.9477, time 31818.03ms, mfu 4.01%\n",
            "iter 62: loss 3.0035, time 31826.11ms, mfu 4.01%\n",
            "iter 63: loss 3.0878, time 31718.56ms, mfu 4.01%\n",
            "iter 64: loss 3.0713, time 31785.20ms, mfu 4.01%\n",
            "iter 65: loss 3.1025, time 31818.21ms, mfu 4.01%\n",
            "iter 66: loss 3.0213, time 31672.79ms, mfu 4.01%\n",
            "iter 67: loss 3.0777, time 31633.07ms, mfu 4.01%\n",
            "iter 68: loss 2.9883, time 31623.52ms, mfu 4.01%\n",
            "iter 69: loss 3.0485, time 31546.25ms, mfu 4.02%\n",
            "iter 70: loss 2.9809, time 31623.46ms, mfu 4.02%\n",
            "iter 71: loss 2.9744, time 31718.17ms, mfu 4.02%\n",
            "iter 72: loss 2.9271, time 31834.45ms, mfu 4.02%\n",
            "iter 73: loss 2.8855, time 31870.99ms, mfu 4.01%\n",
            "iter 74: loss 2.9897, time 31837.31ms, mfu 4.01%\n",
            "iter 75: loss 2.8242, time 31710.17ms, mfu 4.01%\n",
            "iter 76: loss 2.8594, time 31820.49ms, mfu 4.01%\n",
            "iter 77: loss 3.0080, time 31841.34ms, mfu 4.01%\n",
            "iter 78: loss 2.9570, time 31727.12ms, mfu 4.01%\n",
            "iter 79: loss 3.0005, time 31711.35ms, mfu 4.01%\n",
            "iter 80: loss 2.9068, time 31730.41ms, mfu 4.01%\n",
            "iter 81: loss 3.0260, time 31700.43ms, mfu 4.01%\n",
            "iter 82: loss 2.9559, time 31702.11ms, mfu 4.01%\n",
            "iter 83: loss 2.8301, time 31665.56ms, mfu 4.01%\n",
            "iter 84: loss 3.2671, time 31701.67ms, mfu 4.01%\n",
            "iter 85: loss 3.0839, time 31727.53ms, mfu 4.01%\n",
            "iter 86: loss 2.8721, time 31812.16ms, mfu 4.01%\n",
            "iter 87: loss 2.8994, time 31771.12ms, mfu 4.01%\n",
            "iter 88: loss 2.9675, time 31723.34ms, mfu 4.01%\n",
            "iter 89: loss 2.7085, time 31729.67ms, mfu 4.01%\n",
            "iter 90: loss 3.0200, time 31647.70ms, mfu 4.01%\n",
            "iter 91: loss 2.9229, time 31529.94ms, mfu 4.02%\n",
            "iter 92: loss 3.0344, time 31671.28ms, mfu 4.02%\n",
            "iter 93: loss 2.8267, time 31820.84ms, mfu 4.01%\n",
            "iter 94: loss 2.9279, time 31844.93ms, mfu 4.01%\n",
            "iter 95: loss 2.9031, time 31788.22ms, mfu 4.01%\n",
            "iter 96: loss 3.1034, time 31708.43ms, mfu 4.01%\n",
            "iter 97: loss 3.0125, time 31715.15ms, mfu 4.01%\n",
            "iter 98: loss 2.9442, time 31682.14ms, mfu 4.01%\n",
            "iter 99: loss 3.0288, time 31621.98ms, mfu 4.01%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(model.state_dict(), \"./out/backup_100_it.pt\")"
      ],
      "metadata": {
        "id": "2jWhNJ8xanUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPCvTmTjbCQl",
        "outputId": "29834e0d-09bd-4962-9c8b-30783440ce3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 100: train loss 2.9703, val loss 3.0115\n",
            "saving checkpoint to out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  pcnt_pruned = model.prune(m=0.0004475, rowwise=True)\n",
        "print(pcnt_pruned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6E5_txkue-Zv",
        "outputId": "e475a440-af91-4ec8-ba88-c1d9c2960d71"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.05014696193897384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for module in model.modules():\n",
        "    if isinstance(module, PrunableLinear):\n",
        "      module.weight *= module.mask"
      ],
      "metadata": {
        "id": "ufzp5YynqAJT"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iic1zmlkqP-e",
        "outputId": "198ac7c3-fa5d-4b81-dd69-cf7ce8f19c3b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 100: train loss 3.4429, val loss 3.4651\n",
            "iter 100: loss 3.4515, time 134053.14ms, mfu -100.00%\n",
            "iter 101: loss 3.3488, time 30309.06ms, mfu -100.00%\n",
            "iter 102: loss 3.4385, time 30230.92ms, mfu -100.00%\n",
            "iter 103: loss 3.2806, time 30344.04ms, mfu -100.00%\n",
            "iter 104: loss 3.2395, time 30415.68ms, mfu -100.00%\n",
            "iter 105: loss 3.2885, time 30490.47ms, mfu 4.18%\n",
            "iter 106: loss 3.3089, time 30471.76ms, mfu 4.18%\n",
            "iter 107: loss 3.2573, time 30379.12ms, mfu 4.18%\n",
            "iter 108: loss 3.2136, time 30355.45ms, mfu 4.18%\n",
            "iter 109: loss 3.3448, time 30293.81ms, mfu 4.18%\n",
            "iter 110: loss 3.2640, time 30309.61ms, mfu 4.18%\n",
            "iter 111: loss 3.2299, time 30361.85ms, mfu 4.18%\n",
            "iter 112: loss 3.2516, time 30295.37ms, mfu 4.19%\n",
            "iter 113: loss 3.3632, time 30268.49ms, mfu 4.19%\n",
            "iter 114: loss 3.2608, time 30279.79ms, mfu 4.19%\n",
            "iter 115: loss 3.2443, time 30227.85ms, mfu 4.19%\n",
            "iter 116: loss 3.1991, time 30247.65ms, mfu 4.19%\n",
            "iter 117: loss 3.2266, time 30359.05ms, mfu 4.19%\n",
            "iter 118: loss 3.2023, time 30415.08ms, mfu 4.19%\n",
            "iter 119: loss 3.0572, time 30370.82ms, mfu 4.19%\n",
            "iter 120: loss 3.2221, time 30266.74ms, mfu 4.19%\n",
            "iter 121: loss 3.3390, time 30313.55ms, mfu 4.20%\n",
            "iter 122: loss 3.2027, time 30404.46ms, mfu 4.19%\n",
            "iter 123: loss 3.2844, time 30242.13ms, mfu 4.20%\n",
            "iter 124: loss 3.1663, time 30237.52ms, mfu 4.20%\n",
            "iter 125: loss 3.0079, time 30223.23ms, mfu 4.20%\n",
            "iter 126: loss 3.1194, time 30234.12ms, mfu 4.20%\n",
            "iter 127: loss 3.2952, time 30336.56ms, mfu 4.20%\n",
            "iter 128: loss 3.1686, time 30378.50ms, mfu 4.20%\n",
            "iter 129: loss 3.2422, time 30297.14ms, mfu 4.20%\n",
            "iter 130: loss 2.9682, time 30295.14ms, mfu 4.20%\n",
            "iter 131: loss 3.2101, time 30263.08ms, mfu 4.20%\n",
            "iter 132: loss 3.0739, time 30240.14ms, mfu 4.20%\n",
            "iter 133: loss 3.2375, time 30242.38ms, mfu 4.20%\n",
            "iter 134: loss 3.1684, time 30204.98ms, mfu 4.20%\n",
            "iter 135: loss 3.1251, time 30320.39ms, mfu 4.20%\n",
            "iter 136: loss 3.1101, time 30403.03ms, mfu 4.20%\n",
            "iter 137: loss 3.0974, time 30393.40ms, mfu 4.20%\n",
            "iter 138: loss 3.1947, time 30403.88ms, mfu 4.20%\n",
            "iter 139: loss 3.1153, time 30411.99ms, mfu 4.20%\n",
            "iter 140: loss 3.1348, time 30430.85ms, mfu 4.20%\n",
            "iter 141: loss 3.0330, time 30463.90ms, mfu 4.19%\n",
            "iter 142: loss 3.3144, time 30350.94ms, mfu 4.19%\n",
            "iter 143: loss 3.0756, time 30229.41ms, mfu 4.20%\n",
            "iter 144: loss 3.1380, time 30282.12ms, mfu 4.20%\n",
            "iter 145: loss 3.0697, time 30355.23ms, mfu 4.20%\n",
            "iter 146: loss 2.9606, time 30399.07ms, mfu 4.20%\n",
            "iter 147: loss 3.0364, time 30292.74ms, mfu 4.20%\n",
            "iter 148: loss 3.0287, time 30273.72ms, mfu 4.20%\n",
            "iter 149: loss 3.1790, time 30235.47ms, mfu 4.20%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8rfe_ySqS5J",
        "outputId": "b9735bde-92f6-466d-d24a-3fd12083bd97"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 150: train loss 3.0887, val loss 3.1267\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for module in model.modules():\n",
        "    if isinstance(module, PrunableLinear):\n",
        "      module.weight *= module.mask"
      ],
      "metadata": {
        "id": "MV-dgcGs2HWF"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLahyPiW2IqF",
        "outputId": "6302ecbb-6754-4ddf-f532-ff8201da927a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 150: train loss 3.0904, val loss 3.1263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"saving checkpoint to {out_dir}\")\n",
        "torch.save({\n",
        "    'model': model.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "    'model_args': model_args,\n",
        "    'iter_num': iter_num,\n",
        "    'best_val_loss': best_val_loss,\n",
        "    'config': config,\n",
        "}, os.path.join(out_dir, 'ckpt_150_pruned_0.05_direct.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSJudBMW2KC9",
        "outputId": "559f75d5-7170-4ae2-9e2a-4eb85005c9aa"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving checkpoint to out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  pcnt_pruned = model.prune(m=0.000502, rowwise=True)\n",
        "print(pcnt_pruned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdCBhhzC2T6j",
        "outputId": "a463d0fd-bed6-4bd9-a92d-aa03d899e0d2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.10086216140700122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for module in model.modules():\n",
        "    if isinstance(module, PrunableLinear):\n",
        "      module.weight *= module.mask"
      ],
      "metadata": {
        "id": "7yZhe0VZTDck"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VU-2wk7_xJ9",
        "outputId": "91f1a1e5-1539-410f-9bae-6d9b40375bdd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 150: train loss 8.1182, val loss 8.1177\n",
            "iter 150: loss 8.1229, time 133027.23ms, mfu -100.00%\n",
            "iter 151: loss 6.6373, time 30258.04ms, mfu -100.00%\n",
            "iter 152: loss 5.4409, time 30234.30ms, mfu -100.00%\n",
            "iter 153: loss 4.9134, time 30240.91ms, mfu -100.00%\n",
            "iter 154: loss 4.5443, time 30438.89ms, mfu -100.00%\n",
            "iter 155: loss 4.5381, time 30264.00ms, mfu 4.21%\n",
            "iter 156: loss 4.4046, time 30239.01ms, mfu 4.21%\n",
            "iter 157: loss 4.3113, time 30274.13ms, mfu 4.21%\n",
            "iter 158: loss 4.2199, time 30290.47ms, mfu 4.21%\n",
            "iter 159: loss 4.2449, time 30257.71ms, mfu 4.21%\n",
            "iter 160: loss 4.1095, time 30416.80ms, mfu 4.20%\n",
            "iter 161: loss 4.0877, time 30441.79ms, mfu 4.20%\n",
            "iter 162: loss 3.9920, time 30482.72ms, mfu 4.20%\n",
            "iter 163: loss 4.0831, time 30350.17ms, mfu 4.20%\n",
            "iter 164: loss 3.9705, time 30256.57ms, mfu 4.20%\n",
            "iter 165: loss 3.9368, time 30257.02ms, mfu 4.20%\n",
            "iter 166: loss 3.8343, time 30396.94ms, mfu 4.20%\n",
            "iter 167: loss 3.8741, time 30433.75ms, mfu 4.20%\n",
            "iter 168: loss 3.8343, time 30341.58ms, mfu 4.20%\n",
            "iter 169: loss 3.6438, time 30337.58ms, mfu 4.20%\n",
            "iter 170: loss 3.8115, time 30439.78ms, mfu 4.20%\n",
            "iter 171: loss 3.8613, time 30403.95ms, mfu 4.20%\n",
            "iter 172: loss 3.7864, time 30286.81ms, mfu 4.20%\n",
            "iter 173: loss 3.8618, time 30317.14ms, mfu 4.20%\n",
            "iter 174: loss 3.7182, time 30438.63ms, mfu 4.20%\n",
            "iter 175: loss 3.5019, time 30347.86ms, mfu 4.20%\n",
            "iter 176: loss 3.6526, time 30397.68ms, mfu 4.19%\n",
            "iter 177: loss 3.7936, time 30308.32ms, mfu 4.20%\n",
            "iter 178: loss 3.6789, time 30257.40ms, mfu 4.20%\n",
            "iter 179: loss 3.7618, time 30364.02ms, mfu 4.20%\n",
            "iter 180: loss 3.4353, time 30367.60ms, mfu 4.20%\n",
            "iter 181: loss 3.7811, time 30402.98ms, mfu 4.20%\n",
            "iter 182: loss 3.4777, time 30245.46ms, mfu 4.20%\n",
            "iter 183: loss 3.6762, time 30218.77ms, mfu 4.20%\n",
            "iter 184: loss 3.5930, time 30391.43ms, mfu 4.20%\n",
            "iter 185: loss 3.5823, time 30441.01ms, mfu 4.20%\n",
            "iter 186: loss 3.5737, time 30397.07ms, mfu 4.20%\n",
            "iter 187: loss 3.5347, time 30325.54ms, mfu 4.20%\n",
            "iter 188: loss 3.6439, time 30275.47ms, mfu 4.20%\n",
            "iter 189: loss 3.5772, time 30278.14ms, mfu 4.20%\n",
            "iter 190: loss 3.5630, time 30266.16ms, mfu 4.20%\n",
            "iter 191: loss 3.4791, time 30236.82ms, mfu 4.20%\n",
            "iter 192: loss 3.6600, time 30312.08ms, mfu 4.20%\n",
            "iter 193: loss 3.4644, time 30364.70ms, mfu 4.20%\n",
            "iter 194: loss 3.5242, time 30410.37ms, mfu 4.20%\n",
            "iter 195: loss 3.4280, time 30247.00ms, mfu 4.20%\n",
            "iter 196: loss 3.3304, time 30187.41ms, mfu 4.20%\n",
            "iter 197: loss 3.3578, time 30235.45ms, mfu 4.20%\n",
            "iter 198: loss 3.3817, time 30239.33ms, mfu 4.20%\n",
            "iter 199: loss 3.5640, time 30319.90ms, mfu 4.20%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2eW3ArRAE99",
        "outputId": "9c2ee295-39b8-4c5a-8373-fb70ea97ded7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: train loss 3.4852, val loss 3.5335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for module in model.modules():\n",
        "    if isinstance(module, PrunableLinear):\n",
        "      module.weight *= module.mask"
      ],
      "metadata": {
        "id": "2S4IMAZ5GUXx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysea763oTVDO",
        "outputId": "277363ab-708f-4e5f-8bbc-4e8378e750cf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: train loss 3.5085, val loss 3.5571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"saving checkpoint to {out_dir}\")\n",
        "torch.save({\n",
        "    'model': model.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "    'model_args': model_args,\n",
        "    'iter_num': iter_num,\n",
        "    'best_val_loss': best_val_loss,\n",
        "    'config': config,\n",
        "}, os.path.join(out_dir, 'ckpt_200_pruned_0.10_indirect.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPzacdvuIDvr",
        "outputId": "7d20e6b1-bd25-478a-a8f7-073be9c51a56"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving checkpoint to out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  pcnt_pruned = model.prune(m=0.0014, rowwise=True)\n",
        "print(pcnt_pruned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Jb5nmOaGXch",
        "outputId": "2020b63d-b0bf-4587-fbff-c2877546aa02"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.15003297353813358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for module in model.modules():\n",
        "    if isinstance(module, PrunableLinear):\n",
        "      module.weight *= module.mask"
      ],
      "metadata": {
        "id": "TVQDgS4vJL-W"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h_1Zn6aISrl",
        "outputId": "c62254d6-8f28-46c2-97d6-0d5fdb5e9085"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: train loss 10.7548, val loss 10.7539\n",
            "iter 200: loss 10.7551, time 129212.54ms, mfu -100.00%\n",
            "iter 201: loss 10.5756, time 29528.97ms, mfu -100.00%\n",
            "iter 202: loss 10.3684, time 29294.47ms, mfu -100.00%\n",
            "iter 203: loss 9.9969, time 29516.62ms, mfu -100.00%\n",
            "iter 204: loss 9.9151, time 29553.52ms, mfu -100.00%\n",
            "iter 205: loss 10.0143, time 29425.08ms, mfu 4.33%\n",
            "iter 206: loss 9.9425, time 29320.66ms, mfu 4.33%\n",
            "iter 207: loss 9.8947, time 29451.89ms, mfu 4.33%\n",
            "iter 208: loss 9.8539, time 29470.36ms, mfu 4.33%\n",
            "iter 209: loss 9.9455, time 29413.90ms, mfu 4.33%\n",
            "iter 210: loss 9.9837, time 29352.33ms, mfu 4.33%\n",
            "iter 211: loss 9.7774, time 29384.75ms, mfu 4.33%\n",
            "iter 212: loss 9.8432, time 29434.38ms, mfu 4.33%\n",
            "iter 213: loss 9.8641, time 29405.52ms, mfu 4.33%\n",
            "iter 214: loss 9.9263, time 29366.07ms, mfu 4.33%\n",
            "iter 215: loss 9.8600, time 29238.63ms, mfu 4.33%\n",
            "iter 216: loss 9.7800, time 29300.09ms, mfu 4.33%\n",
            "iter 217: loss 9.8844, time 29409.48ms, mfu 4.33%\n",
            "iter 218: loss 9.8391, time 29446.57ms, mfu 4.33%\n",
            "iter 219: loss 9.8245, time 29364.71ms, mfu 4.33%\n",
            "iter 220: loss 9.8754, time 29237.94ms, mfu 4.33%\n",
            "iter 221: loss 9.8153, time 29234.44ms, mfu 4.34%\n",
            "iter 222: loss 9.8408, time 29338.56ms, mfu 4.34%\n",
            "iter 223: loss 9.9050, time 29403.96ms, mfu 4.34%\n",
            "iter 224: loss 9.8817, time 29430.10ms, mfu 4.34%\n",
            "iter 225: loss 10.0570, time 29404.12ms, mfu 4.33%\n",
            "iter 226: loss 10.1598, time 29402.92ms, mfu 4.33%\n",
            "iter 227: loss 9.6922, time 29425.04ms, mfu 4.33%\n",
            "iter 228: loss 9.7887, time 29407.72ms, mfu 4.33%\n",
            "iter 229: loss 9.6427, time 29426.08ms, mfu 4.33%\n",
            "iter 230: loss 9.7912, time 29411.41ms, mfu 4.33%\n",
            "iter 231: loss 9.8890, time 29438.30ms, mfu 4.33%\n",
            "iter 232: loss 9.7560, time 29423.55ms, mfu 4.33%\n",
            "iter 233: loss 9.5244, time 29536.15ms, mfu 4.33%\n",
            "iter 234: loss 9.6173, time 29610.09ms, mfu 4.33%\n",
            "iter 235: loss 9.7591, time 29621.29ms, mfu 4.32%\n",
            "iter 236: loss 9.6957, time 29541.19ms, mfu 4.32%\n",
            "iter 237: loss 9.6894, time 29519.87ms, mfu 4.32%\n",
            "iter 238: loss 9.7989, time 29636.81ms, mfu 4.32%\n",
            "iter 239: loss 9.6892, time 29599.05ms, mfu 4.32%\n",
            "iter 240: loss 9.5495, time 29443.81ms, mfu 4.32%\n",
            "iter 241: loss 9.8554, time 29460.52ms, mfu 4.32%\n",
            "iter 242: loss 9.4615, time 29493.06ms, mfu 4.32%\n",
            "iter 243: loss 9.6175, time 29568.62ms, mfu 4.32%\n",
            "iter 244: loss 9.4998, time 29658.37ms, mfu 4.31%\n",
            "iter 245: loss 9.5097, time 29510.14ms, mfu 4.31%\n",
            "iter 246: loss 9.5890, time 29441.35ms, mfu 4.32%\n",
            "iter 247: loss 9.4997, time 29536.25ms, mfu 4.32%\n",
            "iter 248: loss 9.5894, time 29602.72ms, mfu 4.31%\n",
            "iter 249: loss 9.4900, time 29530.27ms, mfu 4.31%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5M76LdtI17z",
        "outputId": "9ab8076d-c1ca-413c-c17a-dc5d6fcab0b4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 250: train loss 9.4812, val loss 9.4494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for module in model.modules():\n",
        "    if isinstance(module, PrunableLinear):\n",
        "      module.weight *= module.mask"
      ],
      "metadata": {
        "id": "nKt34vg0PMNh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUvagC2qPN-0",
        "outputId": "c90a3f3e-f56f-4bb8-e52b-d2b502b24ba2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 250: train loss 9.7192, val loss 9.6854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"saving checkpoint to {out_dir}\")\n",
        "torch.save({\n",
        "    'model': model.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "    'model_args': model_args,\n",
        "    'iter_num': iter_num,\n",
        "    'best_val_loss': best_val_loss,\n",
        "    'config': config,\n",
        "}, os.path.join(out_dir, 'ckpt_250_pruned_0.15.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuJ5-RUFPQKa",
        "outputId": "04757711-6ab6-492e-ddbb-9e98843a9a40"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving checkpoint to out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  pcnt_pruned = model.prune(m=0.0016176, rowwise=True)\n",
        "print(pcnt_pruned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lc2LKls7PWJO",
        "outputId": "0c2cbb6f-d382-46c0-cfa2-bc82a78f4821"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.20002408482743725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for module in model.modules():\n",
        "    if isinstance(module, PrunableLinear):\n",
        "      module.weight *= module.mask"
      ],
      "metadata": {
        "id": "p7SbmkWyPZJ4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFsR9lrzPapb",
        "outputId": "582a51c9-e676-4ccb-99e7-6393d83863a9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 250: train loss 10.8575, val loss 10.8572\n",
            "iter 250: loss 10.8579, time 127536.73ms, mfu -100.00%\n",
            "iter 251: loss 10.8251, time 28493.48ms, mfu -100.00%\n",
            "iter 252: loss 10.8238, time 28655.35ms, mfu -100.00%\n",
            "iter 253: loss 10.8246, time 28691.28ms, mfu -100.00%\n",
            "iter 254: loss 10.8303, time 28491.23ms, mfu -100.00%\n",
            "iter 255: loss 10.8268, time 28407.13ms, mfu 4.48%\n",
            "iter 256: loss 10.8249, time 28289.82ms, mfu 4.48%\n",
            "iter 257: loss 10.8250, time 28383.71ms, mfu 4.48%\n",
            "iter 258: loss 10.8242, time 28450.57ms, mfu 4.48%\n",
            "iter 259: loss 10.8236, time 28466.07ms, mfu 4.48%\n",
            "iter 260: loss 10.8238, time 28373.42ms, mfu 4.48%\n",
            "iter 261: loss 10.8261, time 28279.93ms, mfu 4.48%\n",
            "iter 262: loss 10.8259, time 28317.83ms, mfu 4.49%\n",
            "iter 263: loss 10.8237, time 28319.90ms, mfu 4.49%\n",
            "iter 264: loss 10.8238, time 28239.58ms, mfu 4.49%\n",
            "iter 265: loss 10.8242, time 28177.92ms, mfu 4.49%\n",
            "iter 266: loss 10.8236, time 28155.33ms, mfu 4.50%\n",
            "iter 267: loss 10.8235, time 28130.14ms, mfu 4.50%\n",
            "iter 268: loss 10.8229, time 28167.96ms, mfu 4.50%\n",
            "iter 269: loss 10.8208, time 28217.39ms, mfu 4.50%\n",
            "iter 270: loss 10.8221, time 28358.65ms, mfu 4.50%\n",
            "iter 271: loss 10.8213, time 28349.53ms, mfu 4.50%\n",
            "iter 272: loss 10.8210, time 28352.09ms, mfu 4.50%\n",
            "iter 273: loss 10.8204, time 28370.52ms, mfu 4.50%\n",
            "iter 274: loss 10.8197, time 28413.86ms, mfu 4.50%\n",
            "iter 275: loss 10.8170, time 28445.92ms, mfu 4.49%\n",
            "iter 276: loss 10.8179, time 28326.94ms, mfu 4.49%\n",
            "iter 277: loss 10.8155, time 28399.06ms, mfu 4.49%\n",
            "iter 278: loss 10.8116, time 28536.65ms, mfu 4.49%\n",
            "iter 279: loss 10.8067, time 28634.84ms, mfu 4.49%\n",
            "iter 280: loss 10.8042, time 28716.47ms, mfu 4.48%\n",
            "iter 281: loss 10.8226, time 28788.48ms, mfu 4.47%\n",
            "iter 282: loss 10.7857, time 28760.56ms, mfu 4.47%\n",
            "iter 283: loss 10.8746, time 28842.89ms, mfu 4.46%\n",
            "iter 284: loss 10.8707, time 28960.11ms, mfu 4.46%\n",
            "iter 285: loss 10.7497, time 29003.08ms, mfu 4.45%\n",
            "iter 286: loss 10.9144, time 28910.38ms, mfu 4.45%\n",
            "iter 287: loss 10.9367, time 28961.15ms, mfu 4.44%\n",
            "iter 288: loss 10.8540, time 28830.32ms, mfu 4.44%\n",
            "iter 289: loss 10.7817, time 28712.84ms, mfu 4.44%\n",
            "iter 290: loss 10.7953, time 28631.39ms, mfu 4.44%\n",
            "iter 291: loss 10.8252, time 28619.41ms, mfu 4.44%\n",
            "iter 292: loss 10.8057, time 28528.21ms, mfu 4.44%\n",
            "iter 293: loss 10.7850, time 28558.18ms, mfu 4.44%\n",
            "iter 294: loss 10.7809, time 28706.58ms, mfu 4.44%\n",
            "iter 295: loss 10.7940, time 28789.76ms, mfu 4.44%\n",
            "iter 296: loss 10.7856, time 28666.17ms, mfu 4.44%\n",
            "iter 297: loss 10.7691, time 28559.76ms, mfu 4.44%\n",
            "iter 298: loss 10.7678, time 28555.09ms, mfu 4.44%\n",
            "iter 299: loss 10.7611, time 28699.02ms, mfu 4.44%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpGiD2N_SPjE",
        "outputId": "55a6dc66-5ee8-4459-f85a-1200efc02703"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 300: train loss 10.7435, val loss 10.7443\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for module in model.modules():\n",
        "    if isinstance(module, PrunableLinear):\n",
        "      module.weight *= module.mask"
      ],
      "metadata": {
        "id": "_J8mZnplsqSI"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-pU7LL2srx0",
        "outputId": "4d879f14-cae7-463d-d72e-429346ff5549"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 300: train loss 10.8285, val loss 10.8281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"saving checkpoint to {out_dir}\")\n",
        "torch.save({\n",
        "    'model': model.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "    'model_args': model_args,\n",
        "    'iter_num': iter_num,\n",
        "    'best_val_loss': best_val_loss,\n",
        "    'config': config,\n",
        "}, os.path.join(out_dir, 'ckpt_300_pruned_0.20.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zboczy3EstXS",
        "outputId": "3dfa1c71-6947-4654-b365-da3ccad247db"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving checkpoint to out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  pcnt_pruned = model.prune(m=0.0076, rowwise=False)\n",
        "print(pcnt_pruned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cLhj7dYswlk",
        "outputId": "a4eedbab-b719-4b49-aa84-90889c52528f"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2501452252399177\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for module in model.modules():\n",
        "    if isinstance(module, PrunableLinear):\n",
        "      module.weight *= module.mask"
      ],
      "metadata": {
        "id": "lrkr5jcOzETH"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jE6DTeiKzl3Z",
        "outputId": "7f19b1a2-6dcf-4d46-fe16-c7d6074c36eb"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 300: train loss 10.8288, val loss 10.8284\n",
            "iter 300: loss 10.8280, time 125962.33ms, mfu -100.00%\n",
            "iter 301: loss 10.8191, time 28281.72ms, mfu -100.00%\n",
            "iter 302: loss 10.8145, time 28294.12ms, mfu -100.00%\n",
            "iter 303: loss 10.8107, time 28526.59ms, mfu -100.00%\n",
            "iter 304: loss 10.7951, time 28505.07ms, mfu -100.00%\n",
            "iter 305: loss 10.7849, time 28582.73ms, mfu 4.45%\n",
            "iter 306: loss 10.7583, time 28655.99ms, mfu 4.45%\n",
            "iter 307: loss 10.7331, time 28759.91ms, mfu 4.45%\n",
            "iter 308: loss 10.6993, time 28825.21ms, mfu 4.45%\n",
            "iter 309: loss 10.6734, time 28968.24ms, mfu 4.44%\n",
            "iter 310: loss 10.6556, time 29154.32ms, mfu 4.43%\n",
            "iter 311: loss 10.6779, time 29203.73ms, mfu 4.43%\n",
            "iter 312: loss 10.6679, time 29310.14ms, mfu 4.42%\n",
            "iter 313: loss 10.6331, time 29101.28ms, mfu 4.41%\n",
            "iter 314: loss 10.6713, time 28990.01ms, mfu 4.41%\n",
            "iter 315: loss 10.6455, time 29074.23ms, mfu 4.41%\n",
            "iter 316: loss 10.6510, time 29103.85ms, mfu 4.41%\n",
            "iter 317: loss 10.6647, time 29185.71ms, mfu 4.40%\n",
            "iter 318: loss 10.6295, time 29265.37ms, mfu 4.40%\n",
            "iter 319: loss 10.6137, time 29065.65ms, mfu 4.39%\n",
            "iter 320: loss 10.6271, time 28977.28ms, mfu 4.39%\n",
            "iter 321: loss 10.6395, time 28993.19ms, mfu 4.39%\n",
            "iter 322: loss 10.6680, time 29152.65ms, mfu 4.39%\n",
            "iter 323: loss 10.6772, time 29211.30ms, mfu 4.39%\n",
            "iter 324: loss 10.6077, time 29067.27ms, mfu 4.39%\n",
            "iter 325: loss 10.6121, time 29211.28ms, mfu 4.38%\n",
            "iter 326: loss 10.6057, time 29341.70ms, mfu 4.38%\n",
            "iter 327: loss 10.5812, time 29174.21ms, mfu 4.38%\n",
            "iter 328: loss 10.6400, time 28995.93ms, mfu 4.38%\n",
            "iter 329: loss 10.6228, time 28899.01ms, mfu 4.38%\n",
            "iter 330: loss 10.6323, time 29062.79ms, mfu 4.38%\n",
            "iter 331: loss 10.6742, time 29385.64ms, mfu 4.38%\n",
            "iter 332: loss 10.6782, time 29337.74ms, mfu 4.37%\n",
            "iter 333: loss 10.6300, time 29203.81ms, mfu 4.37%\n",
            "iter 334: loss 10.6274, time 29150.54ms, mfu 4.37%\n",
            "iter 335: loss 10.6236, time 29100.60ms, mfu 4.37%\n",
            "iter 336: loss 10.6236, time 29033.17ms, mfu 4.37%\n",
            "iter 337: loss 10.6101, time 29108.36ms, mfu 4.37%\n",
            "iter 338: loss 10.6323, time 29221.96ms, mfu 4.37%\n",
            "iter 339: loss 10.5824, time 29217.54ms, mfu 4.37%\n",
            "iter 340: loss 10.5886, time 29240.80ms, mfu 4.37%\n",
            "iter 341: loss 10.5836, time 29082.80ms, mfu 4.37%\n",
            "iter 342: loss 10.5794, time 29026.00ms, mfu 4.37%\n",
            "iter 343: loss 10.5782, time 29060.82ms, mfu 4.37%\n",
            "iter 344: loss 10.5586, time 29120.48ms, mfu 4.37%\n",
            "iter 345: loss 10.6058, time 29228.93ms, mfu 4.37%\n",
            "iter 346: loss 10.5937, time 29280.50ms, mfu 4.37%\n",
            "iter 347: loss 10.5663, time 29205.72ms, mfu 4.37%\n",
            "iter 348: loss 10.5478, time 29107.74ms, mfu 4.37%\n",
            "iter 349: loss 10.6010, time 29026.45ms, mfu 4.37%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fHmRE1qzo6F",
        "outputId": "c65d6a9d-a69e-4b09-a7f5-5836963fe956"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 350: train loss 10.5773, val loss 10.5797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for module in model.modules():\n",
        "    if isinstance(module, PrunableLinear):\n",
        "      module.weight *= module.mask"
      ],
      "metadata": {
        "id": "CaeZQ9ExzuyP"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwoM0gEJzwdP",
        "outputId": "4b6b9165-ffe7-4c24-fdc5-6c2e01a2ebfe"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 350: train loss 10.8228, val loss 10.8234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"saving checkpoint to {out_dir}\")\n",
        "torch.save({\n",
        "    'model': model.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "    'model_args': model_args,\n",
        "    'iter_num': iter_num,\n",
        "    'best_val_loss': best_val_loss,\n",
        "    'config': config,\n",
        "}, os.path.join(out_dir, 'ckpt_350_pruned_0.25.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqW8yhftzxwD",
        "outputId": "0db137f5-a559-4ce2-e5fd-eea96a0f3264"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving checkpoint to out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  pcnt_pruned = model.prune(m=0.0155, rowwise=False)\n",
        "print(pcnt_pruned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCYCYWXnz2ff",
        "outputId": "9f9c03fa-e664-4ff7-c604-30fec4d3c49c"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.30162594039540297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for module in model.modules():\n",
        "    if isinstance(module, PrunableLinear):\n",
        "      module.weight *= module.mask"
      ],
      "metadata": {
        "id": "HDh1YJpa6UCN"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MRwzRn_6mTs",
        "outputId": "0cdc5b5a-04ff-421c-aca0-4a88e8acfb42"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 350: train loss 10.8228, val loss 10.8236\n",
            "iter 350: loss 10.8227, time 126484.99ms, mfu -100.00%\n",
            "iter 351: loss 10.8241, time 28681.25ms, mfu -100.00%\n",
            "iter 352: loss 10.8136, time 28540.77ms, mfu -100.00%\n",
            "iter 353: loss 10.7949, time 28549.83ms, mfu -100.00%\n",
            "iter 354: loss 10.7921, time 28625.11ms, mfu -100.00%\n",
            "iter 355: loss 10.7650, time 28681.81ms, mfu 4.44%\n",
            "iter 356: loss 10.7464, time 28663.46ms, mfu 4.44%\n",
            "iter 357: loss 10.7325, time 28790.82ms, mfu 4.44%\n",
            "iter 358: loss 10.6689, time 28926.73ms, mfu 4.43%\n",
            "iter 359: loss 10.6428, time 29058.49ms, mfu 4.43%\n",
            "iter 360: loss 10.6708, time 29201.52ms, mfu 4.42%\n",
            "iter 361: loss 10.6539, time 29322.24ms, mfu 4.41%\n",
            "iter 362: loss 10.6985, time 29414.35ms, mfu 4.41%\n",
            "iter 363: loss 10.6260, time 29199.23ms, mfu 4.40%\n",
            "iter 364: loss 10.6780, time 29029.11ms, mfu 4.40%\n",
            "iter 365: loss 10.6373, time 29137.85ms, mfu 4.40%\n",
            "iter 366: loss 10.5978, time 29243.21ms, mfu 4.39%\n",
            "iter 367: loss 10.6331, time 29191.38ms, mfu 4.39%\n",
            "iter 368: loss 10.6048, time 29034.17ms, mfu 4.39%\n",
            "iter 369: loss 10.5896, time 28966.57ms, mfu 4.39%\n",
            "iter 370: loss 10.6060, time 29098.35ms, mfu 4.39%\n",
            "iter 371: loss 10.6162, time 29177.40ms, mfu 4.39%\n",
            "iter 372: loss 10.5809, time 29195.26ms, mfu 4.38%\n",
            "iter 373: loss 10.5454, time 29150.78ms, mfu 4.38%\n",
            "iter 374: loss 10.5706, time 29135.23ms, mfu 4.38%\n",
            "iter 375: loss 10.5543, time 29176.78ms, mfu 4.38%\n",
            "iter 376: loss 10.5617, time 29191.96ms, mfu 4.38%\n",
            "iter 377: loss 10.5628, time 29314.82ms, mfu 4.37%\n",
            "iter 378: loss 10.5909, time 29283.07ms, mfu 4.37%\n",
            "iter 379: loss 10.5863, time 29250.57ms, mfu 4.37%\n",
            "iter 380: loss 10.5728, time 29166.99ms, mfu 4.37%\n",
            "iter 381: loss 10.6089, time 29169.11ms, mfu 4.37%\n",
            "iter 382: loss 10.5795, time 29107.48ms, mfu 4.37%\n",
            "iter 383: loss 10.5929, time 29167.35ms, mfu 4.37%\n",
            "iter 384: loss 10.5901, time 29077.85ms, mfu 4.37%\n",
            "iter 385: loss 10.5869, time 29058.70ms, mfu 4.37%\n",
            "iter 386: loss 10.5716, time 29037.24ms, mfu 4.37%\n",
            "iter 387: loss 10.5726, time 29108.53ms, mfu 4.37%\n",
            "iter 388: loss 10.5581, time 29145.63ms, mfu 4.37%\n",
            "iter 389: loss 10.5609, time 29202.80ms, mfu 4.37%\n",
            "iter 390: loss 10.5515, time 29094.72ms, mfu 4.37%\n",
            "iter 391: loss 10.5801, time 29003.68ms, mfu 4.37%\n",
            "iter 392: loss 10.5488, time 29001.27ms, mfu 4.37%\n",
            "iter 393: loss 10.5402, time 29047.27ms, mfu 4.38%\n",
            "iter 394: loss 10.5882, time 29106.11ms, mfu 4.38%\n",
            "iter 395: loss 10.5578, time 29167.62ms, mfu 4.37%\n",
            "iter 396: loss 10.5327, time 29218.97ms, mfu 4.37%\n",
            "iter 397: loss 10.5824, time 29068.61ms, mfu 4.37%\n",
            "iter 398: loss 10.5106, time 29002.46ms, mfu 4.38%\n",
            "iter 399: loss 10.5408, time 29153.01ms, mfu 4.37%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2wHdffy6nz8",
        "outputId": "98a60dfc-2627-4e9c-ddd2-8d204a49a779"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: train loss 10.5507, val loss 10.5547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for module in model.modules():\n",
        "    if isinstance(module, PrunableLinear):\n",
        "      module.weight *= module.mask"
      ],
      "metadata": {
        "id": "kbE4bOu9ATTx"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJzc3h-lAVAm",
        "outputId": "b551a25e-7fc2-4d2a-e694-90de7e249b16"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: train loss 10.8535, val loss 10.8558\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"saving checkpoint to {out_dir}\")\n",
        "torch.save({\n",
        "    'model': model.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "    'model_args': model_args,\n",
        "    'iter_num': iter_num,\n",
        "    'best_val_loss': best_val_loss,\n",
        "    'config': config,\n",
        "}, os.path.join(out_dir, 'ckpt_400_pruned_0.30.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhThdwxBAWk2",
        "outputId": "a5c95391-61eb-4db7-d9a7-5d0eec4aca02"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving checkpoint to out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  pcnt_pruned = model.prune(m=0.0235, rowwise=False)\n",
        "print(pcnt_pruned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bw7jNlb_Aabe",
        "outputId": "cf850874-356e-4a73-e1da-ac109a1faccb"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3528153288068805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for module in model.modules():\n",
        "    if isinstance(module, PrunableLinear):\n",
        "      module.weight *= module.mask"
      ],
      "metadata": {
        "id": "u4Eqc07xAdO3"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzMgKV3TB1w8",
        "outputId": "49d07c52-453a-4927-869d-32a588ee0f61"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: train loss 10.8857, val loss 10.8884\n",
            "iter 400: loss 10.8839, time 126673.64ms, mfu -100.00%\n",
            "iter 401: loss 10.8244, time 28614.73ms, mfu -100.00%\n",
            "iter 402: loss 10.8067, time 28438.76ms, mfu -100.00%\n",
            "iter 403: loss 10.7972, time 28673.97ms, mfu -100.00%\n",
            "iter 404: loss 10.7793, time 28749.23ms, mfu -100.00%\n",
            "iter 405: loss 10.7555, time 28774.50ms, mfu 4.42%\n",
            "iter 406: loss 10.7335, time 28790.37ms, mfu 4.42%\n",
            "iter 407: loss 10.6860, time 28884.22ms, mfu 4.42%\n",
            "iter 408: loss 10.6607, time 28843.65ms, mfu 4.42%\n",
            "iter 409: loss 10.6170, time 28922.03ms, mfu 4.42%\n",
            "iter 410: loss 10.6314, time 28964.27ms, mfu 4.42%\n",
            "iter 411: loss 10.6249, time 28990.36ms, mfu 4.42%\n",
            "iter 412: loss 10.5904, time 28957.18ms, mfu 4.41%\n",
            "iter 413: loss 10.6219, time 28950.46ms, mfu 4.41%\n",
            "iter 414: loss 10.5691, time 28974.24ms, mfu 4.41%\n",
            "iter 415: loss 10.5573, time 29030.50ms, mfu 4.41%\n",
            "iter 416: loss 10.5602, time 29048.54ms, mfu 4.41%\n",
            "iter 417: loss 10.5578, time 29040.70ms, mfu 4.40%\n",
            "iter 418: loss 10.5553, time 29019.11ms, mfu 4.40%\n",
            "iter 419: loss 10.5403, time 28973.35ms, mfu 4.40%\n",
            "iter 420: loss 10.5556, time 29035.18ms, mfu 4.40%\n",
            "iter 421: loss 10.5589, time 29122.58ms, mfu 4.40%\n",
            "iter 422: loss 10.5870, time 29144.85ms, mfu 4.39%\n",
            "iter 423: loss 10.5859, time 28950.21ms, mfu 4.39%\n",
            "iter 424: loss 10.5654, time 28841.11ms, mfu 4.40%\n",
            "iter 425: loss 10.5810, time 28918.87ms, mfu 4.40%\n",
            "iter 426: loss 10.5353, time 29096.84ms, mfu 4.39%\n",
            "iter 427: loss 10.5501, time 28987.28ms, mfu 4.39%\n",
            "iter 428: loss 10.5581, time 28865.94ms, mfu 4.40%\n",
            "iter 429: loss 10.5351, time 28839.93ms, mfu 4.40%\n",
            "iter 430: loss 10.5774, time 29008.35ms, mfu 4.40%\n",
            "iter 431: loss 10.5734, time 29177.38ms, mfu 4.39%\n",
            "iter 432: loss 10.5817, time 29153.75ms, mfu 4.39%\n",
            "iter 433: loss 10.5555, time 28964.45ms, mfu 4.39%\n",
            "iter 434: loss 10.5746, time 28857.30ms, mfu 4.39%\n",
            "iter 435: loss 10.5453, time 28977.92ms, mfu 4.39%\n",
            "iter 436: loss 10.5188, time 29104.94ms, mfu 4.39%\n",
            "iter 437: loss 10.5783, time 29118.44ms, mfu 4.39%\n",
            "iter 438: loss 10.5490, time 29066.19ms, mfu 4.39%\n",
            "iter 439: loss 10.5399, time 29035.61ms, mfu 4.39%\n",
            "iter 440: loss 10.5228, time 29055.22ms, mfu 4.39%\n",
            "iter 441: loss 10.5820, time 29019.36ms, mfu 4.39%\n",
            "iter 442: loss 10.4913, time 29145.26ms, mfu 4.39%\n",
            "iter 443: loss 10.5270, time 29098.87ms, mfu 4.38%\n",
            "iter 444: loss 10.5076, time 29037.31ms, mfu 4.38%\n",
            "iter 445: loss 10.5472, time 29015.79ms, mfu 4.39%\n",
            "iter 446: loss 10.5056, time 28930.15ms, mfu 4.39%\n",
            "iter 447: loss 10.5293, time 28933.96ms, mfu 4.39%\n",
            "iter 448: loss 10.4885, time 28912.60ms, mfu 4.39%\n",
            "iter 449: loss 10.5360, time 28929.24ms, mfu 4.39%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Rel5HPdB4OD",
        "outputId": "4e3fbd8c-4833-4ec7-a307-68f324d3ad54"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 450: train loss 10.5238, val loss 10.5269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for module in model.modules():\n",
        "    if isinstance(module, PrunableLinear):\n",
        "      module.weight *= module.mask"
      ],
      "metadata": {
        "id": "PB2vNrL3H2jL"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwWXLhynH4xz",
        "outputId": "0dbb6192-4d01-4062-c74c-f5a554ed1be7"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 450: train loss 10.9864, val loss 10.9887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"saving checkpoint to {out_dir}\")\n",
        "torch.save({\n",
        "    'model': model.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "    'model_args': model_args,\n",
        "    'iter_num': iter_num,\n",
        "    'best_val_loss': best_val_loss,\n",
        "    'config': config,\n",
        "}, os.path.join(out_dir, 'ckpt_450_pruned_0.35.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfknv-pEH6gQ",
        "outputId": "9404e71d-5872-432d-9895-10ce1530cf49"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving checkpoint to out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  pcnt_pruned = model.prune(m=0.165, rowwise=False)\n",
        "print(pcnt_pruned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iO0buQeRH_q2",
        "outputId": "f88c849e-729b-43ba-ade7-5aac99140e67"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9079216491612829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for module in model.modules():\n",
        "    if isinstance(module, PrunableLinear):\n",
        "      module.weight *= module.mask"
      ],
      "metadata": {
        "id": "rCUrWhs8IvV_"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vG3wfiKAJDv9",
        "outputId": "085a3d5c-0204-4f72-d031-cb8e87611dea"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 450: train loss 10.8343, val loss 10.8335\n",
            "iter 450: loss 10.8342, time 115888.02ms, mfu -100.00%\n",
            "iter 451: loss 10.8303, time 26731.39ms, mfu -100.00%\n",
            "iter 452: loss 10.8269, time 26477.96ms, mfu -100.00%\n",
            "iter 453: loss 10.8224, time 26738.61ms, mfu -100.00%\n",
            "iter 454: loss 10.8164, time 26810.90ms, mfu -100.00%\n",
            "iter 455: loss 10.8132, time 26749.35ms, mfu 4.76%\n",
            "iter 456: loss 10.8090, time 26686.05ms, mfu 4.76%\n",
            "iter 457: loss 10.8165, time 26652.46ms, mfu 4.76%\n",
            "iter 458: loss 10.8158, time 26678.87ms, mfu 4.76%\n",
            "iter 459: loss 10.8254, time 26745.39ms, mfu 4.76%\n",
            "iter 460: loss 10.8079, time 26800.73ms, mfu 4.76%\n",
            "iter 461: loss 10.8194, time 26826.99ms, mfu 4.76%\n",
            "iter 462: loss 10.8139, time 26819.42ms, mfu 4.76%\n",
            "iter 463: loss 10.8221, time 26702.82ms, mfu 4.76%\n",
            "iter 464: loss 10.8134, time 26613.85ms, mfu 4.76%\n",
            "iter 465: loss 10.8108, time 26531.13ms, mfu 4.77%\n",
            "iter 466: loss 10.8041, time 26509.79ms, mfu 4.77%\n",
            "iter 467: loss 10.8164, time 26606.92ms, mfu 4.77%\n",
            "iter 468: loss 10.8063, time 26581.45ms, mfu 4.77%\n",
            "iter 469: loss 10.8115, time 26577.86ms, mfu 4.77%\n",
            "iter 470: loss 10.8170, time 26546.98ms, mfu 4.78%\n",
            "iter 471: loss 10.8151, time 26517.85ms, mfu 4.78%\n",
            "iter 472: loss 10.8126, time 26502.92ms, mfu 4.78%\n",
            "iter 473: loss 10.8112, time 26501.05ms, mfu 4.78%\n",
            "iter 474: loss 10.8173, time 26538.95ms, mfu 4.79%\n",
            "iter 475: loss 10.8095, time 26548.13ms, mfu 4.79%\n",
            "iter 476: loss 10.8076, time 26511.56ms, mfu 4.79%\n",
            "iter 477: loss 10.8146, time 26546.86ms, mfu 4.79%\n",
            "iter 478: loss 10.8105, time 26592.12ms, mfu 4.79%\n",
            "iter 479: loss 10.8116, time 26555.67ms, mfu 4.79%\n",
            "iter 480: loss 10.8105, time 26574.42ms, mfu 4.79%\n",
            "iter 481: loss 10.8164, time 26573.37ms, mfu 4.79%\n",
            "iter 482: loss 10.8123, time 26557.11ms, mfu 4.79%\n",
            "iter 483: loss 10.8123, time 26613.98ms, mfu 4.79%\n",
            "iter 484: loss 10.8102, time 26545.22ms, mfu 4.79%\n",
            "iter 485: loss 10.8184, time 26520.45ms, mfu 4.79%\n",
            "iter 486: loss 10.8045, time 26543.97ms, mfu 4.79%\n",
            "iter 487: loss 10.8097, time 26543.27ms, mfu 4.79%\n",
            "iter 488: loss 10.7990, time 26543.12ms, mfu 4.79%\n",
            "iter 489: loss 10.8074, time 26592.38ms, mfu 4.79%\n",
            "iter 490: loss 10.8079, time 26620.16ms, mfu 4.79%\n",
            "iter 491: loss 10.8159, time 26518.56ms, mfu 4.79%\n",
            "iter 492: loss 10.8079, time 26488.72ms, mfu 4.79%\n",
            "iter 493: loss 10.8190, time 26477.15ms, mfu 4.80%\n",
            "iter 494: loss 10.8049, time 26518.46ms, mfu 4.80%\n",
            "iter 495: loss 10.8087, time 26526.87ms, mfu 4.80%\n",
            "iter 496: loss 10.8180, time 26457.35ms, mfu 4.80%\n",
            "iter 497: loss 10.8031, time 26495.03ms, mfu 4.80%\n",
            "iter 498: loss 10.8126, time 26595.24ms, mfu 4.80%\n",
            "iter 499: loss 10.8023, time 26621.22ms, mfu 4.80%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ez2d7CViJFY_",
        "outputId": "3a3652d4-19c8-4bb9-e87d-fcba1cc0c137"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 500: train loss 10.8073, val loss 10.8089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for module in model.modules():\n",
        "    if isinstance(module, PrunableLinear):\n",
        "      module.weight *= module.mask"
      ],
      "metadata": {
        "id": "ScVDhTovPVGx"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss, eval_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jl6hevA4QqdC",
        "outputId": "3de4a272-f645-417e-b6fa-df7ccb426833"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 500: train loss 10.8088, val loss 10.8105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"saving checkpoint to {out_dir}\")\n",
        "torch.save({\n",
        "    'model': model.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "    'model_args': model_args,\n",
        "    'iter_num': iter_num,\n",
        "    'best_val_loss': best_val_loss,\n",
        "    'config': config,\n",
        "}, os.path.join(out_dir, 'ckpt_500_pruned_0.91.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwmsFWghQr9c",
        "outputId": "f4a274c0-ad84-430b-eec9-9841df1f01ea"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving checkpoint to out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pruning import compress_layers, compress"
      ],
      "metadata": {
        "id": "30lq38EdQvyv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compress_layers(model, device=device_type)\n",
        "torch.cuda.empty_cache()\n",
        "import gc; gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrxxAEGuVWCo",
        "outputId": "80e39061-0f82-4b00-e7c4-f0cd525478dd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "94"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRTt9vVJfZU_",
        "outputId": "36f35deb-317f-446f-8619-547539a48dc9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50257, 1024)\n",
              "    (wpe): Embedding(1024, 1024)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-23): 24 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): CompressedLinear(in_features=1024, out_features=3072, bias=True)\n",
              "          (c_proj): CompressedLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): CompressedLinear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): CompressedLinear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): CompressedLinear(in_features=1024, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'test.pt')"
      ],
      "metadata": {
        "id": "Y8hceoftfOvY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMDsCs03gZ-I",
        "outputId": "4398a0d3-3543-4ee9-e0c8-df9789e600d6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50257, 1024)\n",
              "    (wpe): Embedding(1024, 1024)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-23): 24 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): CompressedLinear(in_features=1024, out_features=3072, bias=True)\n",
              "          (c_proj): CompressedLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): CompressedLinear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): CompressedLinear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): CompressedLinear(in_features=1024, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num, best_val_loss = finetune(model, iter_num, best_val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "5c5aD4tHWvjK",
        "outputId": "87fbae14-457f-463f-f166-2ab8300a086e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-e6075eb330fa>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0miter_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinetune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_val_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-5bc49db8b7c5>\u001b[0m in \u001b[0;36mfinetune\u001b[0;34m(model, iter_num, best_val_loss, eval_only)\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mmicro_step\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m               \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m               \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mgradient_accumulation_steps\u001b[0m \u001b[0;31m# scale the loss to account for gradient accumulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m           \u001b[0;31m# immediately async prefetch next batch while model is doing the forward pass on the GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/University (M.S., Stanford)/CS 229S - Systems for ML/cs229s-nanoGPT-rmg/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok_emb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpos_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/University (M.S., Stanford)/CS 229S - Systems for ML/cs229s-nanoGPT-rmg/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/University (M.S., Stanford)/CS 229S - Systems for ML/cs229s-nanoGPT-rmg/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# calculate query, key, values for all heads in batch and move head forward to be the batch dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_head\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, nh, T, hs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_head\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, nh, T, hs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'split'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pruning import CompressedLinear"
      ],
      "metadata": {
        "id": "4rBJs0YQiZmr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in model.named_children():\n",
        "    if len(list(module.children())) > 0:\n",
        "        compress_layers(module, device)  # compound module: recurse\n",
        "    if isinstance(module, CompressedLinear):\n",
        "        print(name)\n",
        "        print(module.rows_not_pruned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdLppbVviXu-",
        "outputId": "74bff05f-711b-4d22-d6c7-5101d421953b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lm_head\n",
            "tensor([True, True, True,  ..., True, True, True], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXLnkALBaJrR",
        "outputId": "7d64d58f-7812-406e-bd74-5881a1b99cc4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50257, 1024)\n",
              "    (wpe): Embedding(1024, 1024)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-23): 24 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): CompressedLinear(in_features=1024, out_features=3072, bias=True)\n",
              "          (c_proj): CompressedLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): CompressedLinear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): CompressedLinear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): CompressedLinear(in_features=1024, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-foMGXMQbNV0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}